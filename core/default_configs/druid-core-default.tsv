druid.extensions.directory	extensions (This is a relative path to Druid's working directory)	The root extension directory where user can put extensions related files. Druid will load extensions stored under this directory.
druid.extensions.hadoopDependenciesDir	hadoop-dependencies (This is a relative path to Druid's working directory	The root hadoop dependencies directory where user can put hadoop related dependencies files. Druid will load the dependencies based on the hadoop coordinate specified in the hadoop index task.
druid.extensions.loadList	null	A JSON array of extensions to load from extension directories by Druid. If it is not specified, its value will be null and Druid will load all the extensions under druid.extensions.directory. If its value is empty list [], then no extensions will be loaded at all. It is also allowed to specify absolute path of other custom extensions not stored in the common extensions directory.
druid.extensions.searchCurrentClassloader	true	This is a boolean flag that determines if Druid will search the main classloader for extensions.  It defaults to true but can be turned off if you have reason to not automatically add all modules on the classpath.
druid.extensions.useExtensionClassloaderFirst	false	This is a boolean flag that determines if Druid extensions should prefer loading classes from their own jars rather than jars bundled with Druid. If false, extensions must be compatible with classes provided by any jars bundled with Druid. If true, extensions may depend on conflicting versions.
druid.extensions.hadoopContainerDruidClasspath	null	Hadoop Indexing launches hadoop jobs and this configuration provides way to explicitly set the user classpath for the hadoop job. By default this is computed automatically by druid based on the druid process classpath and set of extensions. However, sometimes you might want to be explicit to resolve dependency conflicts between druid and hadoop.
druid.extensions.addExtensionsToHadoopContainer	false	Only applicable if druid.extensions.hadoopContainerDruidClasspath is provided. If set to true, then extensions specified in the loadList are added to hadoop container classpath. Note that when druid.extensions.hadoopContainerDruidClasspath is not provided then extensions are always added to hadoop container classpath.
druid.modules.excludeList	[]	A JSON array of canonical class names (e.g., "org.apache.druid.somepackage.SomeModule") of module classes which shouldn't be loaded, even if they are found in extensions specified by druid.extensions.loadList, or in the list of core modules specified to be loaded on a particular Druid process type. Useful when some useful extension contains some module, which shouldn't be loaded on some Druid process type because some dependencies of that module couldn't be satisfied.
druid.zk.paths.base	/druid	Base ZooKeeper path.
druid.zk.service.host	none	The ZooKeeper hosts to connect to. This is a REQUIRED property and therefore a host address must be supplied.
druid.zk.service.user	none	The username to authenticate with ZooKeeper. This is an optional property.
druid.zk.service.pwd	none	The [Password Provider](../operations/password-provider.md) or the string password to authenticate with ZooKeeper. This is an optional property.
druid.zk.service.authScheme	digest	digest is the only authentication scheme supported. 
druid.zk.service.sessionTimeoutMs	30000	ZooKeeper session timeout, in milliseconds.
druid.zk.service.connectionTimeoutMs	15000	ZooKeeper connection timeout, in milliseconds.
druid.zk.service.compress	true	Boolean flag for whether or not created Znodes should be compressed.
druid.zk.service.acl	false	Boolean flag for whether or not to enable ACL security for ZooKeeper. If ACL is enabled, zNode creators will have all permissions.
druid.zk.paths.base	/druid	Base ZooKeeper path.
druid.zk.paths.propertiesPath	${druid.zk.paths.base}/properties	ZooKeeper properties path.
druid.zk.paths.announcementsPath	${druid.zk.paths.base}/announcements	Druid process announcement path.
druid.zk.paths.liveSegmentsPath	${druid.zk.paths.base}/segments	Current path for where Druid processes announce their segments.
druid.zk.paths.loadQueuePath	${druid.zk.paths.base}/loadQueue	Entries here cause Historical processes to load and drop segments.
druid.zk.paths.coordinatorPath	${druid.zk.paths.base}/coordinator	Used by the Coordinator for leader election.
druid.zk.paths.servedSegmentsPath	${druid.zk.paths.base}/servedSegments	Deprecated. Legacy path for where Druid processes announce their segments.
druid.zk.paths.indexer.base	${druid.zk.paths.base}/indexer	Base ZooKeeper path for 
druid.zk.paths.indexer.announcementsPath	${druid.zk.paths.indexer.base}/announcements	Middle managers announce themselves here.
druid.zk.paths.indexer.tasksPath	${druid.zk.paths.indexer.base}/tasks	Used to assign tasks to MiddleManagers.
druid.zk.paths.indexer.statusPath	${druid.zk.paths.indexer.base}/status	Parent path for announcement of task statuses.
druid.discovery.curator.path	/druid/discovery	Services announce themselves under this ZooKeeper path.
druid.enablePlaintextPort	true	Enable/Disable HTTP connector.
druid.enableTlsPort	false	Enable/Disable HTTPS connector.
druid.server.https.keyStorePath	none	The file path or URL of the TLS/SSL Key store.
druid.server.https.keyStoreType	none	The type of the key store.
druid.server.https.certAlias	none	Alias of TLS/SSL certificate for the connector.
druid.server.https.keyStorePassword	none	The [Password Provider](../operations/password-provider.md) or String password for the Key Store.
druid.server.https.keyManagerFactoryAlgorithm	javax.net.ssl.KeyManagerFactory.getDefaultAlgorithm()	Algorithm to use for creating KeyManager, more details [here](https://docs.oracle.com/javase/7/docs/technotes/guides/security/jsse/JSSERefGuide.html#KeyManager).
druid.server.https.keyManagerPassword	none	The [Password Provider](../operations/password-provider.md) or String password for the Key Manager.
druid.server.https.includeCipherSuites	Jetty's default include cipher list	List of cipher suite names to include. You can either use the exact cipher suite name or a regular expression.
druid.server.https.excludeCipherSuites	Jetty's default exclude cipher list	List of cipher suite names to exclude. You can either use the exact cipher suite name or a regular expression.
druid.server.https.includeProtocols	Jetty's default include protocol list	List of exact protocols names to include.
druid.server.https.excludeProtocols	Jetty's default exclude protocol list	List of exact protocols names to exclude.
druid.client.https.protocol	TLSv1.2	SSL protocol to use.
druid.client.https.trustStoreType	java.security.KeyStore.getDefaultType()	The type of the key store where trusted root certificates are stored.
druid.client.https.trustStorePath	none	The file path or URL of the TLS/SSL Key store where trusted root certificates are stored.
druid.client.https.trustStoreAlgorithm	javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()	Algorithm to be used by TrustManager to validate certificate chains
druid.client.https.trustStorePassword	none	The [Password Provider](../operations/password-provider.md) or String password for the Trust Store.
druid.auth.authenticatorChain	List of Authenticator type names	JSON List of Strings
druid.escalator.type	Type of the Escalator that should be used for internal Druid communications. This Escalator must use an authentication scheme that is supported by an Authenticator in druid.auth.authenticatorChain.	String
druid.auth.authorizers	List of Authorizer type names 	JSON List of Strings
druid.auth.unsecuredPaths	List of paths for which security checks will not be performed. All requests to these paths will be allowed.	 List of Strings
druid.auth.allowUnauthenticatedHttpOptions	If true, skip authentication checks for HTTP OPTIONS requests. This is needed for certain use cases, such as supporting CORS pre-flight requests. Note that disabling authentication checks for OPTIONS requests will allow unauthenticated users to determine what Druid endpoints are valid (by checking if the OPTIONS request returns a 200 instead of 404), so enabling this option may reveal information about server configuration, including information about what extensions are loaded (if those extensions add endpoints).	Boolean
druid.startup.logging.logProperties	false	Log all properties on startup (from common.runtime.properties, runtime.properties, and the JVM command line).
druid.startup.logging.maskProperties	["password"]	Masks sensitive properties (passwords, for example) containing theses words.
druid.request.logging.type	noop (request logging disabled by default)	How to log every query request. Choices: noop, [file](#file-request-logging), [emitter](#emitter-request-logging), [slf4j](#slf4j-request-logging), [filtered](#filtered-request-logging), [composing](#composing-request-logging), [switching](#switching-request-logging)
druid.request.logging.dir	none	Historical, Realtime and Broker processes maintain request logs of all of the requests they get (interaction is via POST, so normal request logs donâ€™t generally capture information about the actual query), this specifies the directory to store the request logs in
druid.request.logging.filePattern	"yyyy-MM-dd'.log'"	[Joda datetime format](http://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat.html) for each file
druid.request.logging.feed	none	Feed name for requests.
druid.request.logging.setMDC	false	If you want to set MDC entries within the log entry, set this value to true. Your logging system must be configured to support MDC in order to format this data.
druid.request.logging.setContextMDC	false	Set to "true" to add  the Druid query context to the MDC entries. Only applies when setMDC is true.
druid.request.logging.queryTimeThresholdMs	0, i.e., no filtering	Threshold value for the query/time metric in milliseconds.
druid.request.logging.sqlQueryTimeThresholdMs	0, i.e., no filtering	Threshold value for the sqlQuery/time metric in milliseconds.
druid.request.logging.mutedQueryTypes 	 []	 Query requests of these types are not logged. Query types are defined as string objects corresponding to the "queryType" value for the specified query in the Druid's [native JSON query API](http://druid.apache.org/docs/latest/querying/querying). Misspelled query types will be ignored. Example to ignore scan and timeBoundary queries: ["scan", "timeBoundary"]
druid.request.logging.delegate.type	none	Type of delegate request logger to log requests.
druid.request.logging.loggerProviders	none	List of request loggers for emitting request logs.
druid.request.logging.nativeQueryLogger	none	Request logger for emitting native query request logs.
druid.request.logging.sqlQueryLogger	none	Request logger for emitting SQL query request logs.
druid.audit.manager.auditHistoryMillis	1 week	Default duration for querying audit history.
druid.audit.manager.includePayloadAsDimensionInMetric	false	Boolean flag on whether to add payload column in service metric.
druid.audit.manager.maxPayloadSizeBytes	-1	The maximum size of audit payload to store in Druid's metadata store audit table. If the size of audit payload exceeds this value, the audit log would be stored with a message indicating that the payload was omitted instead. Setting maxPayloadSizeBytes to -1 (default value) disables this check, meaning Druid will always store audit payload regardless of it's size. Setting to any negative number other than -1 is invalid. Human-readable format is supported, see [here](human-readable-byte.md).  
druid.audit.manager.skipNullField	false	If true, the audit payload stored in metadata store will exclude any field with null value. 
druid.monitoring.emissionPeriod	PT1M	 Frequency that Druid emits metrics.
druid.emitter.logging.loggerClass	LoggingEmitter	Choices: HttpPostEmitter, LoggingEmitter, NoopServiceEmitter, ServiceEmitter. The class used for logging.
druid.emitter.logging.logLevel	info	Choices: debug, info, warn, error. The log level at which message are logged.
druid.emitter.http.flushMillis	60000	How often the internal message buffer is flushed (data is sent).
druid.emitter.http.flushCount	500	How many messages the internal message buffer can hold before flushing (sending).
druid.emitter.http.basicAuthentication	not specified = no authentication	[Password Provider](../operations/password-provider.md) for providing Login and password for authentication in "login:password" form, e.g., druid.emitter.http.basicAuthentication=admin:adminpassword uses Default Password Provider which allows plain text passwords.
druid.emitter.http.flushTimeOut	not specified = no timeout	The timeout after which an event should be sent to the endpoint, even if internal buffers are not filled, in milliseconds.
druid.emitter.http.batchingStrategy	ARRAY	The strategy of how the batch is formatted. "ARRAY" means [event1,event2], "NEWLINES" means event1\nevent2, ONLY_EVENTS means event1event2.
druid.emitter.http.maxBatchSize	the minimum of (10% of JVM heap size divided by 2) or (5242880 (i. e. 5 MiB))	The maximum batch size, in bytes.
druid.emitter.http.batchQueueSizeLimit	the maximum of (2) or (10% of the JVM heap size divided by 5MiB)	The maximum number of batches in emitter queue, if there are problems with emitting.
druid.emitter.http.minHttpTimeoutMillis	0	If the speed of filling batches imposes timeout smaller than that, not even trying to send batch to endpoint, because it will likely fail, not being able to send the data that fast. Configure this depending based on emitter/successfulSending/minTimeMs metric. Reasonable values are 10ms..100ms.
druid.emitter.http.recipientBaseUrl	none, required config	The base URL to emit messages to. Druid will POST JSON to be consumed at the HTTP endpoint specified by this property.
druid.emitter.http.ssl.useDefaultJavaContext	false	If set to true, the HttpEmitter will use SSLContext.getDefault(), the default Java SSLContext, and all other properties below are ignored.
druid.emitter.http.ssl.trustStorePath	null	The file path or URL of the TLS/SSL Key store where trusted root certificates are stored. If this is unspecified, the HTTP Emitter will use the same SSLContext as Druid's internal HTTP client, as described in the beginning of this section, and all other properties below are ignored.
druid.emitter.http.ssl.trustStoreType	java.security.KeyStore.getDefaultType()	The type of the key store where trusted root certificates are stored.
druid.emitter.http.ssl.trustStoreAlgorithm	javax.net.ssl.TrustManagerFactory.getDefaultAlgorithm()	Algorithm to be used by TrustManager to validate certificate chains
druid.emitter.http.ssl.trustStorePassword	none	The [Password Provider](../operations/password-provider.md) or String password for the Trust Store.
druid.emitter.http.ssl.protocol	"TLSv1.2"	TLS protocol to use.
druid.emitter.parametrized.recipientBaseUrlPattern	none, required config	The URL pattern to send an event to, based on the event's feed. E.g., http://foo.bar/{feed}, that will send event to http://foo.bar/metrics if the event's feed is "metrics".
druid.emitter.composing.emitters	[]	List of emitter modules to load, e.g., ["logging","http"].
druid.metadata.storage.type	derby	The type of metadata storage to use. Choose from "mysql", "postgresql", or "derby".
druid.metadata.storage.connector.connectURI	none	The JDBC URI for the database to connect to
druid.metadata.storage.connector.user	none	The username to connect with.
druid.metadata.storage.connector.password	none	The [Password Provider](../operations/password-provider.md) or String password used to connect with.
druid.metadata.storage.connector.createTables	true	If Druid requires a table and it doesn't exist, create it?
druid.metadata.storage.tables.base	druid	The base name for tables.
druid.metadata.storage.tables.dataSource	druid_dataSource	The table to use to look for dataSources which created by [Kafka Indexing Service](../development/extensions-core/kafka-ingestion.md).
druid.metadata.storage.tables.pendingSegments	druid_pendingSegments	The table to use to look for pending segments.
druid.metadata.storage.tables.segments	druid_segments	The table to use to look for segments.
druid.metadata.storage.tables.rules	druid_rules	The table to use to look for segment load/drop rules.
druid.metadata.storage.tables.config	druid_config	The table to use to look for configs.
druid.metadata.storage.tables.tasks	druid_tasks	Used by the indexing service to store tasks.
druid.metadata.storage.tables.taskLog	druid_taskLog	Used by the indexing service to store task logs.
druid.metadata.storage.tables.taskLock	druid_taskLock	Used by the indexing service to store task locks.
druid.metadata.storage.tables.supervisors	druid_supervisors	Used by the indexing service to store supervisor configurations.
druid.metadata.storage.tables.audit	druid_audit	The table to use for audit history of configuration changes, e.g., Coordinator rules.
druid.storage.type	local	Choices:local, noop, s3, hdfs, c*. The type of deep storage to use.
druid.storage.storageDirectory	/tmp/druid/localStorage	Directory on disk to use as deep storage.
druid.storage.bucket	none	S3 bucket name.
druid.storage.baseKey	none	S3 object key prefix for storage.
druid.storage.disableAcl	false	Boolean flag for ACL. If this is set to false, the full control would be granted to the bucket owner. This may require to set additional permissions. See [S3 permissions settings](../development/extensions-core/s3.md#s3-permissions-settings).
druid.storage.archiveBucket	none	S3 bucket name for archiving when running the *archive task*.
druid.storage.archiveBaseKey	none	S3 object key prefix for archiving.
druid.storage.sse.type	None	Server-side encryption type. Should be one of s3, kms, and custom. See the below [Server-side encryption section](../development/extensions-core/s3.md#server-side-encryption) for more details.
druid.storage.sse.kms.keyId	None	AWS KMS key ID. This is used only when druid.storage.sse.type is kms and can be empty to use the default key ID.
druid.storage.sse.custom.base64EncodedKey	None	Base64-encoded key. Should be specified if druid.storage.sse.type is custom.
druid.storage.useS3aSchema	false	If true, use the "s3a" filesystem when using Hadoop-based ingestion. If false, the "s3n" filesystem will be used. Only affects Hadoop-based ingestion.
druid.storage.storageDirectory	none	HDFS directory to use as deep storage.
druid.storage.host	none	Cassandra host.
druid.storage.keyspace	none	Cassandra key space.
druid.ingestion.hdfs.allowedProtocols	Allowed protocols for the HDFS input source and HDFS firehose.	List of protocols
druid.ingestion.http.allowedProtocols	Allowed protocols for the HTTP input source and HTTP firehose.	List of protocols
druid.access.jdbc.enforceAllowedProperties	When true, Druid applies druid.access.jdbc.allowedProperties to JDBC connections starting with jdbc:postgresql:, jdbc:mysql:, or jdbc:mariadb:. When false, Druid allows any kind of JDBC connections without JDBC property validation. This config is for backward compatibility especially during upgrades since enforcing allow list can break existing ingestion jobs or lookups based on JDBC. This config is deprecated and will be removed in a future release.	Boolean
druid.access.jdbc.allowedProperties	Defines a list of allowed JDBC properties. Druid always enforces the list for all JDBC connections starting with jdbc:postgresql:, jdbc:mysql:, and jdbc:mariadb: if druid.access.jdbc.enforceAllowedProperties is set to true.<br/><br/>This option is tested against MySQL connector 5.1.49, MariaDB connector 2.7.4, and PostgreSQL connector 42.2.14. Other connector versions might not work.	List of JDBC properties
druid.access.jdbc.allowUnknownJdbcUrlFormat	When false, Druid only accepts JDBC connections starting with jdbc:postgresql: or jdbc:mysql:. When true, Druid allows JDBC connections to any kind of database, but only enforces druid.access.jdbc.allowedProperties for PostgreSQL and MySQL/MariaDB.	Boolean
druid.indexer.logs.type	file	Where to store task logs.  noop, [s3](#s3-task-logs), [azure](#azure-blob-store-task-logs), [google](#google-cloud-storage-task-logs), [hdfs](#hdfs-task-logs), [file](#file-task-logs) 
druid.indexer.logs.directory	log	Local filesystem path.
druid.indexer.logs.s3Bucket	none	S3 bucket name.
druid.indexer.logs.s3Prefix	none	S3 key prefix.
druid.indexer.logs.disableAcl	false	Boolean flag for ACL. If this is set to false, the full control would be granted to the bucket owner. If the task logs bucket is the same as the deep storage (S3) bucket, then the value of this property will need to be set to true if druid.storage.disableAcl has been set to true.
druid.indexer.logs.container	none	The Azure Blob Store container to write logs to
druid.indexer.logs.prefix	none	The path to prepend to logs
druid.indexer.logs.bucket	none	The Google Cloud Storage bucket to write logs to
druid.indexer.logs.prefix	none	The path to prepend to logs
druid.indexer.logs.directory	none	The directory to store logs.
druid.indexer.logs.kill.enabled	false	Boolean value for whether to enable deletion of old task logs. If set to true, Overlord will submit kill tasks periodically based on druid.indexer.logs.kill.delay specified, which will delete task logs from the log directory as well as tasks and tasklogs table entries in metadata storage except for tasks created in the last druid.indexer.logs.kill.durationToRetain period. 
druid.indexer.logs.kill.durationToRetain	None	 Required if kill is enabled. In milliseconds, task logs and entries in task-related metadata storage tables to be retained created in last x milliseconds. 
druid.indexer.logs.kill.initialDelay	random value less than 300000 (5 mins)	 Optional. Number of milliseconds after Overlord start when first auto kill is run. 
druid.indexer.logs.kill.delay	21600000 (6 hours)	Optional. Number of milliseconds of delay between successive executions of auto kill run. 
druid.server.http.showDetailedJettyErrors	true	When set to true, any error from the Jetty layer / Jetty filter includes the following fields  in the JSON response: servlet, message, url, status, and cause, if it exists. When set to false, the JSON response only includes message, url, and status. The field values remain unchanged.
druid.server.http.errorResponseTransform.strategy	none	Error response transform strategy. The strategy controls how Druid transforms error responses from Druid services. When unset or set to none, Druid leaves error responses unchanged.
druid.server.http.errorResponseTransform.allowedRegex	[]	The list of regular expressions Druid uses to validate error messages. If the error message matches any of the regular expressions, then Druid includes it in the response unchanged. If the error message does not match any of the regular expressions, Druid replaces the error message with null or with a default message depending on the type of underlying Exception. 
druid.selectors.indexing.serviceName	druid/overlord	The druid.service name of the Overlord process. To start the Overlord with a different name, set it with this property. 
druid.selectors.coordinator.serviceName	druid/coordinator	The druid.service name of the Coordinator process. To start the Coordinator with a different name, set it with this property. 
druid.announcer.segmentsPerNode	50	Each Znode contains info for up to this many segments.
druid.announcer.maxBytesPerNode	524288	Max byte size for Znode.
druid.announcer.skipDimensionsAndMetrics	false	Skip Dimensions and Metrics list from segment announcements. NOTE: Enabling this will also remove the dimensions and metrics list from Coordinator and Broker endpoints.
druid.announcer.skipLoadSpec	false	Skip segment LoadSpec from segment announcements. NOTE: Enabling this will also remove the loadspec from Coordinator and Broker endpoints.
druid.announcer.skipSegmentAnnouncementOnZk	false	Skip announcing segments to zookeeper. Note that the batch server view will not work if this is set to true.
druid.javascript.enabled	false	Set to "true" to enable JavaScript functionality. This affects the JavaScript parser, filter, extractionFn, aggregator, post-aggregator, router strategy, and worker selection strategy.
druid.indexing.doubleStorage	double	Set to "float" to use 32-bit double representation for double columns.
druid.generic.useDefaultValueForNull	true	When set to true, null values will be stored as '' for string columns and 0 for numeric columns. Set to false to store and query data in SQL compatible mode.
druid.generic.ignoreNullsForStringCardinality	false	When set to true, null values will be ignored for the built-in cardinality aggregator over string columns. Set to false to include null values while estimating cardinality of only string columns using the built-in cardinality aggregator. This setting takes effect only when druid.generic.useDefaultValueForNull is set to true and is ignored in SQL compatibility mode. Additionally, empty strings (equivalent to null) are not counted when this is set to true. 
druid.global.http.numConnections	20	Size of connection pool per destination URL. If there are more HTTP requests than this number that all need to speak to the same URL, then they will queue up.
druid.global.http.eagerInitialization	true	Indicates that http connections should be eagerly initialized. If set to true, numConnections connections are created upon initialization
druid.global.http.compressionCodec	gzip	Compression codec to communicate with others. May be "gzip" or "identity".
druid.global.http.readTimeout	PT15M	The timeout for data reads.
druid.global.http.unusedConnectionTimeout	PT4M	The timeout for idle connections in connection pool. The connection in the pool will be closed after this timeout and a new one will be established. This timeout should be less than druid.global.http.readTimeout. Set this timeout = ~90% of druid.global.http.readTimeout
druid.global.http.numMaxThreads	max(10, ((number of cores * 17) / 16 + 2) + 30)	Maximum number of I/O worker threads
druid.server.hiddenProperties	 ["druid.s3.accessKey","druid.s3.secretKey","druid.metadata.storage.connector.password", "password", "key", "token", "pwd"] 	 If property names or substring of property names (case insensitive) is in this list, responses of the /status/properties endpoint do not show these properties 
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8081	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	8281	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/coordinator	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.coordinator.period	PT60S	The run period for the Coordinator. The Coordinator operates by maintaining the current state of the world in memory and periodically looking at the set of "used" segments and segments being served to make decisions about whether any changes need to be made to the data topology. This property sets the delay between each of these runs.
druid.coordinator.period.indexingPeriod	PT1800S (30 mins)	How often to send compact/merge/conversion tasks to the indexing service. It's recommended to be longer than druid.manager.segments.pollDuration
druid.coordinator.startDelay	PT300S	The operation of the Coordinator works on the assumption that it has an up-to-date view of the state of the world when it runs, the current ZK interaction code, however, is written in a way that doesnâ€™t allow the Coordinator to know for a fact that itâ€™s done loading the current state of the world. This delay is a hack to give it enough time to believe that it has all the data.
druid.coordinator.load.timeout	PT15M	The timeout duration for when the Coordinator assigns a segment to a Historical process.
druid.coordinator.kill.pendingSegments.on	true	Boolean flag for whether or not the Coordinator clean up old entries in the pendingSegments table of metadata store. If set to true, Coordinator will check the created time of most recently complete task. If it doesn't exist, it finds the created time of the earliest running/pending/waiting tasks. Once the created time is found, then for all dataSources not in the killPendingSegmentsSkipList (see [Dynamic configuration](#dynamic-configuration)), Coordinator will ask the Overlord to clean up the entries 1 day or more older than the found created time in the pendingSegments table. This will be done periodically based on druid.coordinator.period.indexingPeriod specified.
druid.coordinator.kill.on	false	Boolean flag for whether or not the Coordinator should submit kill task for unused segments, that is, permanently delete them from metadata store and deep storage. If set to true, then for all whitelisted dataSources (or optionally all), Coordinator will submit tasks periodically based on period specified. A whitelist can be set via dynamic configuration killDataSourceWhitelist described later.<br /><br />When druid.coordinator.kill.on is true, segments are eligible for permanent deletion once their data intervals are older than druid.coordinator.kill.durationToRetain relative to the current time. If a segment's data interval is older than this threshold at the time it is marked unused, it is eligible for permanent deletion immediately after being marked unused.
druid.coordinator.kill.period	P1D (1 Day)	How often to send kill tasks to the indexing service. Value must be greater than druid.coordinator.period.indexingPeriod. Only applies if kill is turned on.
druid.coordinator.kill.durationToRetain	P90D	Only applies if you set druid.coordinator.kill.on to true. This value is ignored if druid.coordinator.kill.ignoreDurationToRetain is true. Valid configurations must be a ISO8601 period. Druid will not kill unused segments whose interval end date is beyond now - durationToRetain. durationToRetain can be a negative ISO8601 period, which would result in now - durationToRetain to be in the future.<br /><br />Note that the durationToRetain parameter applies to the segment interval, not the time that the segment was last marked unused. For example, if durationToRetain is set to P90D, then a segment for a time chunk 90 days in the past is eligible for permanent deletion immediately after being marked unused.
druid.coordinator.kill.ignoreDurationToRetain	false	A way to override druid.coordinator.kill.durationToRetain and tell the coordinator that you do not care about the end date of unused segment intervals when it comes to killing them. If true, the coordinator considers all unused segments as eligible to be killed.
druid.coordinator.kill.maxSegments	100	Kill at most n unused segments per kill task submission, must be greater than 0. Only applies and MUST be specified if kill is turned on.
druid.coordinator.balancer.strategy	cost	Specify the type of balancing strategy for the coordinator to use to distribute segments among the historicals. cachingCost is logically equivalent to cost but is more CPU-efficient on large clusters. diskNormalized weights the costs according to the servers' disk usage ratios - there are known issues with this strategy distributing segments unevenly across the cluster. random distributes segments among services randomly.
druid.coordinator.balancer.cachingCost.awaitInitialization	false	Whether to wait for segment view initialization before creating the cachingCost balancing strategy. This property is enabled only when druid.coordinator.balancer.strategy is cachingCost. If set to 'true', the Coordinator will not start to assign segments, until the segment view is initialized. If set to 'false', the Coordinator will fallback to use the cost balancing strategy only if the segment view is not initialized yet. Notes, it may take much time to wait for the initialization since the cachingCost balancing strategy involves much computing to build itself.
druid.coordinator.loadqueuepeon.repeatDelay	PT0.050S (50 ms)	The start and repeat delay for the loadqueuepeon, which manages the load and drop of segments.
druid.coordinator.asOverlord.enabled	false	Boolean value for whether this Coordinator process should act like an Overlord as well. This configuration allows users to simplify a druid cluster by not having to deploy any standalone Overlord processes. If set to true, then Overlord console is available at http://coordinator-host:port/console.html and be sure to set druid.coordinator.asOverlord.overlordService also. See next.
druid.coordinator.asOverlord.overlordService	NULL	 Required, if druid.coordinator.asOverlord.enabled is true. This must be same value as druid.service on standalone Overlord processes and druid.selectors.indexing.serviceName on Middle Managers.
druid.coordinator.period.metadataStoreManagementPeriod	No 	How often to run metadata management tasks in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. 
druid.coordinator.kill.supervisor.on	 No 	 Boolean value for whether to enable automatic deletion of terminated supervisors. If set to true, Coordinator will periodically remove terminated supervisors from the supervisor table in metadata storage.
druid.coordinator.kill.supervisor.period	 No	 How often to do automatic deletion of terminated supervisor in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Value must be equal to or greater than  druid.coordinator.period.metadataStoreManagementPeriod. Only applies if druid.coordinator.kill.supervisor.on is set to "True".
druid.coordinator.kill.supervisor.durationToRetain	 Yes if druid.coordinator.kill.supervisor.on is set to "True".	 Duration of terminated supervisor to be retained from created time in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Only applies if druid.coordinator.kill.supervisor.on is set to "True".
druid.coordinator.kill.audit.on	 No 	 Boolean value for whether to enable automatic deletion of audit logs. If set to true, Coordinator will periodically remove audit logs from the audit table entries in metadata storage.
druid.coordinator.kill.audit.period	 No	 How often to do automatic deletion of audit logs in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Value must be equal to or greater than  druid.coordinator.period.metadataStoreManagementPeriod. Only applies if druid.coordinator.kill.audit.on is set to "True".
druid.coordinator.kill.audit.durationToRetain	 Yes if druid.coordinator.kill.audit.on is set to "True".	 Duration of audit logs to be retained from created time in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Only applies if druid.coordinator.kill.audit.on is set to "True".
druid.coordinator.kill.compaction.on	 No 	 Boolean value for whether to enable automatic deletion of compaction configurations. If set to true, Coordinator will periodically remove compaction configuration of inactive datasource (datasource with no used and unused segments) from the config table in metadata storage.  
druid.coordinator.kill.compaction.period	 No	 How often to do automatic deletion of compaction configurations in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Value must be equal to or greater than  druid.coordinator.period.metadataStoreManagementPeriod. Only applies if druid.coordinator.kill.compaction.on is set to "True".
druid.coordinator.kill.rule.on	 No 	 Boolean value for whether to enable automatic deletion of rules. If set to true, Coordinator will periodically remove rules of inactive datasource (datasource with no used and unused segments) from the rule table in metadata storage.
druid.coordinator.kill.rule.period	 No	 How often to do automatic deletion of rules in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Value must be equal to or greater than  druid.coordinator.period.metadataStoreManagementPeriod. Only applies if druid.coordinator.kill.rule.on is set to "True".
druid.coordinator.kill.rule.durationToRetain	 Yes if druid.coordinator.kill.rule.on is set to "True".	 Duration of rules to be retained from created time in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Only applies if druid.coordinator.kill.rule.on is set to "True".
druid.coordinator.kill.datasource.on	 No 	 Boolean value for whether to enable automatic deletion of datasource metadata (Note: datasource metadata only exists for datasource created from supervisor). If set to true, Coordinator will periodically remove datasource metadata of terminated supervisor from the datasource table in metadata storage.  
druid.coordinator.kill.datasource.period	 No	 How often to do automatic deletion of datasource metadata in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Value must be equal to or greater than  druid.coordinator.period.metadataStoreManagementPeriod. Only applies if druid.coordinator.kill.datasource.on is set to "True".
druid.coordinator.kill.datasource.durationToRetain	 Yes if druid.coordinator.kill.datasource.on is set to "True".	 Duration of datasource metadata to be retained from created time in [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601) duration format. Only applies if druid.coordinator.kill.datasource.on is set to "True".
druid.serverview.type	Segment discovery method to use. "http" enables discovering segments using HTTP instead of ZooKeeper.	batch or http
druid.coordinator.loadqueuepeon.type	Whether to use "http" or "curator" implementation to assign segment loads/drops to historical	curator or http
druid.coordinator.segment.awaitInitializationOnStart	Whether the Coordinator will wait for its view of segments to fully initialize before starting up. If set to 'true', the Coordinator's HTTP server will not start up, and the Coordinator will not announce itself as available, until the server view is initialized.	true or false
druid.coordinator.loadqueuepeon.http.batchSize	1	Number of segment load/drop requests to batch in one HTTP request. Note that it must be smaller than druid.segmentCache.numLoadingThreads config on Historical process.
druid.manager.config.pollDuration	PT1M	How often the manager polls the config table for updates.
druid.manager.segments.pollDuration	PT1M	The duration between polls the Coordinator does for updates to the set of active segments. Generally defines the amount of lag time it can take for the Coordinator to notice new segments.
druid.manager.rules.pollDuration	PT1M	The duration between polls the Coordinator does for updates to the set of active rules. Generally defines the amount of lag time it can take for the Coordinator to notice rules.
druid.manager.rules.defaultRule	_default	The default rule for the cluster
druid.manager.rules.alertThreshold	PT10M	The duration after a failed poll upon which an alert should be emitted.
druid.manager.lookups.hostDeleteTimeout	PT1S	How long to wait for a DELETE request to a particular process before considering the DELETE a failure
druid.manager.lookups.hostUpdateTimeout	PT10S	How long to wait for a POST request to a particular process before considering the POST a failure
druid.manager.lookups.deleteAllTimeout	PT10S	How long to wait for all DELETE requests to finish before considering the delete attempt a failure
druid.manager.lookups.updateAllTimeout	PT60S	How long to wait for all POST requests to finish before considering the attempt a failure
druid.manager.lookups.threadPoolSize	10	How many processes can be managed concurrently (concurrent POST and DELETE requests). Requests this limit will wait in a queue until a slot becomes available.
druid.manager.lookups.period	30_000	How many milliseconds between checks for configuration changes
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8090	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	8290	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/overlord	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.indexer.runner.type	local	Choices "local" or "remote". Indicates whether tasks should be run locally or in a distributed environment. Experimental task runner "httpRemote" is also available which is same as "remote" but uses HTTP to interact with Middle Managers instead of ZooKeeper.
druid.indexer.storage.type	local	Choices are "local" or "metadata". Indicates whether incoming tasks should be stored locally (in heap) or in metadata storage. "local" is mainly for internal testing while "metadata" is recommended in production because storing incoming tasks in metadata storage allows for tasks to be resumed if the Overlord should fail.
druid.indexer.storage.recentlyFinishedThreshold	PT24H	Duration of time to store task results. Default is 24 hours. If you have hundreds of tasks running in a day, consider increasing this threshold.
druid.indexer.tasklock.forceTimeChunkLock	true	_**Setting this to false is still experimental**_<br/> If set, all tasks are enforced to use time chunk lock. If not set, each task automatically chooses a lock type to use. This configuration can be overwritten by setting forceTimeChunkLock in the [task context](../ingestion/tasks.md#context). See [Task Locking & Priority](../ingestion/tasks.md#context) for more details about locking in tasks.
druid.indexer.task.default.context	empty context	Default task context that is applied to all tasks submitted to the Overlord. Any default in this config does not override neither the context values the user provides nor druid.indexer.tasklock.forceTimeChunkLock.
druid.indexer.queue.maxSize	Integer.MAX_VALUE	Maximum number of active tasks at one time.
druid.indexer.queue.startDelay	PT1M	Sleep this long before starting Overlord queue management. This can be useful to give a cluster time to re-orient itself after e.g. a widespread network issue.
druid.indexer.queue.restartDelay	PT30S	Sleep this long when Overlord queue management throws an exception before trying again.
druid.indexer.queue.storageSyncRate	PT1M	Sync Overlord state this often with an underlying task persistence mechanism.
druid.indexer.runner.taskAssignmentTimeout	PT5M	How long to wait after a task as been assigned to a MiddleManager before throwing an error.
druid.indexer.runner.minWorkerVersion	"0"	The minimum MiddleManager version to send tasks to. The version number is a string. This affects the expected behavior during certain operations like comparison against druid.worker.version. Specifically, the version comparison follows dictionary order. Use ISO8601 date format for the version to accommodate date comparisons. 
druid.indexer.runner.compressZnodes	true	Indicates whether or not the Overlord should expect MiddleManagers to compress Znodes.
druid.indexer.runner.maxZnodeBytes	 512 KiB 	The maximum size Znode in bytes that can be created in ZooKeeper, should be in the range of [10KiB, 2GiB). [Human-readable format](human-readable-byte.md) is supported.
druid.indexer.runner.taskCleanupTimeout	PT15M	How long to wait before failing a task after a MiddleManager is disconnected from ZooKeeper.
druid.indexer.runner.taskShutdownLinkTimeout	PT1M	How long to wait on a shutdown request to a MiddleManager before timing out
druid.indexer.runner.pendingTasksRunnerNumThreads	1	Number of threads to allocate pending-tasks to workers, must be at least 1.
druid.indexer.runner.maxRetriesBeforeBlacklist	5	Number of consecutive times the MiddleManager can fail tasks,  before the worker is blacklisted, must be at least 1
druid.indexer.runner.workerBlackListBackoffTime	PT15M	How long to wait before a task is whitelisted again. This value should be greater that the value set for taskBlackListCleanupPeriod.
druid.indexer.runner.workerBlackListCleanupPeriod	PT5M	A duration after which the cleanup thread will startup to clean blacklisted workers.
druid.indexer.runner.maxPercentageBlacklistWorkers	20	The maximum percentage of workers to blacklist, this must be between 0 and 100.
druid.indexer.autoscale.strategy	noop	Choices are "noop", "ec2" or "gce". Sets the strategy to run when autoscaling is required.
druid.indexer.autoscale.doAutoscale	false	If set to "true" autoscaling will be enabled.
druid.indexer.autoscale.provisionPeriod	PT1M	How often to check whether or not new MiddleManagers should be added.
druid.indexer.autoscale.terminatePeriod	PT5M	How often to check when MiddleManagers should be removed.
druid.indexer.autoscale.originTime	2012-01-01T00:55:00.000Z	The starting reference timestamp that the terminate period increments upon.
druid.indexer.autoscale.workerIdleTimeout	PT90M	How long can a worker be idle (not a run task) before it can be considered for termination.
druid.indexer.autoscale.maxScalingDuration	PT15M	How long the Overlord will wait around for a MiddleManager to show up before giving up.
druid.indexer.autoscale.numEventsToTrack	10	The number of autoscaling related events (node creation and termination) to track.
druid.indexer.autoscale.pendingTaskTimeout	PT30S	How long a task can be in "pending" state before the Overlord tries to scale up.
druid.indexer.autoscale.workerVersion	null	If set, will only create nodes of set version during autoscaling. Overrides dynamic configuration. 
druid.indexer.autoscale.workerPort	8080	The port that MiddleManagers will run on.
druid.indexer.autoscale.workerCapacityHint	-1	 An estimation of the number of task slots available for each worker launched by the auto scaler when there are no workers running. The auto scaler uses the worker capacity hint to launch workers with an adequate capacity to handle pending tasks. When unset or set to a value less than or equal to 0, the auto scaler scales workers equal to the value for minNumWorkers in autoScaler config instead. The auto scaler assumes that each worker, either a middleManager or indexer, has the same amount of task slots. Therefore, when all your workers have the same capacity (homogeneous capacity), set the value for autoscale.workerCapacityHint equal to druid.worker.capacity. If your workers have different capacities (heterogeneous capacity), set the value to the average of druid.worker.capacity across the workers. For example, if two workers have druid.worker.capacity=10, and one has druid.worker.capacity=4, set autoscale.workerCapacityHint=8. Only applies to pendingTaskBased provisioning strategy.
druid.supervisor.healthinessThreshold	3	The number of successful runs before an unhealthy supervisor is again considered healthy.
druid.supervisor.unhealthinessThreshold	3	The number of failed runs before the supervisor is considered unhealthy.
druid.supervisor.taskHealthinessThreshold	3	The number of consecutive task successes before an unhealthy supervisor is again considered healthy.
druid.supervisor.taskUnhealthinessThreshold	3	The number of consecutive task failures before the supervisor is considered unhealthy.
druid.supervisor.storeStackTrace	false	Whether full stack traces of supervisor exceptions should be stored and returned by the supervisor /status endpoint.
druid.supervisor.maxStoredExceptionEvents	max(healthinessThreshold, unhealthinessThreshold)	The maximum number of exception events that can be returned through the supervisor /status endpoint.
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8091	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	8291	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/middlemanager	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.indexer.runner.allowedPrefixes	"com.metamx", "druid", "org.apache.druid", "user.timezone", "file.encoding", "java.io.tmpdir", "hadoop"	Whitelist of prefixes for configs that can be passed down to child peons.
druid.indexer.runner.compressZnodes	true	Indicates whether or not the MiddleManagers should compress Znodes.
druid.indexer.runner.classpath	System.getProperty("java.class.path")	Java classpath for the peon.
druid.indexer.runner.javaCommand	java	Command required to execute java.
druid.indexer.runner.javaOpts	""	*DEPRECATED* A string of -X Java options to pass to the peon's JVM. Quotable parameters or parameters with spaces are encouraged to use javaOptsArray
druid.indexer.runner.javaOptsArray	[]	A JSON array of strings to be passed in as options to the peon's JVM. This is additive to druid.indexer.runner.javaOpts and is recommended for properly handling arguments which contain quotes or spaces like ["-XX:OnOutOfMemoryError=kill -9 %p"]
druid.indexer.runner.maxZnodeBytes	512KiB	The maximum size Znode in bytes that can be created in ZooKeeper, should be in the range of [10KiB, 2GiB). [Human-readable format](human-readable-byte.md) is supported.
druid.indexer.runner.startPort	8100	Starting port used for peon processes, should be greater than 1023 and less than 65536.
druid.indexer.runner.endPort	65535	Ending port used for peon processes, should be greater than or equal to druid.indexer.runner.startPort and less than 65536.
druid.indexer.runner.ports	[]	A JSON array of integers to specify ports that used for peon processes. If provided and non-empty, ports for peon processes will be chosen from these ports. And druid.indexer.runner.startPort/druid.indexer.runner.endPort will be completely ignored.
druid.worker.ip	localhost	The IP of the worker.
druid.worker.version	0	Version identifier for the MiddleManager. The version number is a string. This affects the expected behavior during certain operations like comparison against druid.indexer.runner.minWorkerVersion. Specifically, the version comparison follows dictionary order. Use ISO8601 date format for the version to accommodate date comparisons.
druid.worker.capacity	Number of CPUs on the machine - 1	Maximum number of tasks the MiddleManager can accept.
druid.worker.category	_default_worker_category	A string to name the category that the MiddleManager node belongs to.
druid.processing.buffer.sizeBytes	auto (max 1 GiB)	This specifies a buffer size (less than 2GiB) for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. [Human-readable format](human-readable-byte.md) is supported.
druid.processing.buffer.poolCacheMaxCount	Integer.MAX_VALUE	processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.
druid.processing.formatString	processing-%s	Realtime and Historical processes use this format string to name their processing threads.
druid.processing.numMergeBuffers	max(2, druid.processing.numThreads / 4)	The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.
druid.processing.numThreads	Number of cores - 1 (or 1)	The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1.
druid.processing.columnCache.sizeBytes	0 (disabled)	Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.
druid.processing.fifo	true	If the processing queue should treat tasks of equal priority in a FIFO manner
druid.processing.tmpDir	path represented by java.io.tmpdir	Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path.
druid.processing.intermediaryData.storage.type	local	Storage type for storing intermediary segments of data shuffle between native parallel index tasks. Current choices are "local" which stores segment files in local storage of Middle Managers (or Indexer) or "deepstore" which uses configured deep storage. Note - With "deepstore" type data is stored in shuffle-data directory under the configured deep storage path, auto clean up for this directory is not supported yet. One can setup cloud storage lifecycle rules for auto clean up of data at shuffle-data prefix location.
druid.realtime.cache.useCache	Enable the cache on the realtime.	true, false
druid.realtime.cache.populateCache	Populate the cache on the realtime.	true, false
druid.realtime.cache.unCacheable	All query types to not cache.	All druid query types
druid.realtime.cache.maxEntrySize	Maximum cache entry size in bytes.	positive integer
druid.peon.mode	remote	Choices are "local" and "remote". Setting this to local means you intend to run the peon as a standalone process (Not recommended).
druid.indexer.task.baseDir	System.getProperty("java.io.tmpdir")	Base temporary working directory.
druid.indexer.task.baseTaskDir	${druid.indexer.task.baseDir}/persistent/task	Base temporary working directory for tasks.
druid.indexer.task.batchProcessingMode	CLOSED_SEGMENTS	 Batch ingestion tasks have three operating modes to control construction and tracking for intermediary segments: OPEN_SEGMENTS, CLOSED_SEGMENTS, and CLOSED_SEGMENT_SINKS. OPEN_SEGMENTS uses the streaming ingestion code path and performs a mmap on intermediary segments to build a timeline to make these segments available to realtime queries. Batch ingestion doesn't require intermediary segments, so the default mode, CLOSED_SEGMENTS, eliminates mmap of intermediary segments. CLOSED_SEGMENTS mode still tracks the entire set of segments in heap. The CLOSED_SEGMENTS_SINKS mode is the most aggressive configuration and should have the smallest memory footprint. It eliminates in-memory tracking and mmap of intermediary segments produced during segment creation. CLOSED_SEGMENTS_SINKS mode isn't as well tested as other modes so is currently considered experimental. You can use OPEN_SEGMENTS mode if problems occur with the 2 newer modes. 
druid.indexer.task.defaultHadoopCoordinates	org.apache.hadoop:hadoop-client:2.8.5	Hadoop version to use with HadoopIndexTasks that do not request a particular version.
druid.indexer.task.defaultRowFlushBoundary	75000	Highest row count before persisting to disk. Used for indexing generating tasks.
druid.indexer.task.directoryLockTimeout	PT10M	Wait this long for zombie peons to exit before giving up on their replacements.
druid.indexer.task.gracefulShutdownTimeout	PT5M	Wait this long on middleManager restart for restorable tasks to gracefully exit.
druid.indexer.task.hadoopWorkingPath	/tmp/druid-indexing	Temporary working directory for Hadoop tasks.
druid.indexer.task.restoreTasksOnRestart	false	If true, MiddleManagers will attempt to stop tasks gracefully on shutdown and restore them on restart.
druid.indexer.task.ignoreTimestampSpecForDruidInputSource	false	If true, tasks using the [Druid input source](../ingestion/native-batch-input-source.md) will ignore the provided timestampSpec, and will use the __time column of the input datasource. This option is provided for compatibility with ingestion specs written before Druid 0.22.0.
druid.indexer.task.storeEmptyColumns	true	Boolean value for whether or not to store empty columns during ingestion. When set to true, Druid stores every column specified in the [dimensionsSpec](../ingestion/ingestion-spec.md#dimensionsspec). If you use schemaless ingestion and don't specify any dimensions to ingest, you must also set [includeAllDimensions](../ingestion/ingestion-spec.md#dimensionsspec) for Druid to store empty columns.<br/><br/>If you set storeEmptyColumns to false, Druid SQL queries referencing empty columns will fail. If you intend to leave storeEmptyColumns disabled, you should either ingest dummy data for empty columns or else not query on empty columns.<br/><br/>This configuration can be overwritten by setting storeEmptyColumns in the [task context](../ingestion/tasks.md#context-parameters).
druid.indexer.server.maxChatRequests	0	Maximum number of concurrent requests served by a task's chat handler. Set to 0 to disable limiting.
druid.peon.taskActionClient.retry.minWait	PT5S	The minimum retry time to communicate with Overlord.
druid.peon.taskActionClient.retry.maxWait	PT1M	The maximum retry time to communicate with Overlord.
druid.peon.taskActionClient.retry.maxRetryCount	60	The maximum number of retries to communicate with Overlord.
druid.peon.defaultSegmentWriteOutMediumFactory.type	tmpFile	tmpFile, offHeapMemory, or onHeapMemory, see explanation above
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8091	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	8283	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/indexer	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.worker.version	0	Version identifier for the Indexer.
druid.worker.capacity	Number of available processors - 1	Maximum number of tasks the Indexer can accept.
druid.worker.globalIngestionHeapLimitBytes	60% of configured JVM heap	Total amount of heap available for ingestion processing. This is applied by automatically setting the maxBytesInMemory property on tasks.
druid.worker.numConcurrentMerges	druid.worker.capacity / 2, rounded down	Maximum number of segment persist or merge operations that can run concurrently across all tasks.
druid.indexer.task.baseDir	System.getProperty("java.io.tmpdir")	Base temporary working directory.
druid.indexer.task.baseTaskDir	${druid.indexer.task.baseDir}/persistent/tasks	Base temporary working directory for tasks.
druid.indexer.task.defaultHadoopCoordinates	org.apache.hadoop:hadoop-client:2.8.5	Hadoop version to use with HadoopIndexTasks that do not request a particular version.
druid.indexer.task.gracefulShutdownTimeout	PT5M	Wait this long on Indexer restart for restorable tasks to gracefully exit.
druid.indexer.task.hadoopWorkingPath	/tmp/druid-indexing	Temporary working directory for Hadoop tasks.
druid.indexer.task.restoreTasksOnRestart	false	If true, the Indexer will attempt to stop tasks gracefully on shutdown and restore them on restart.
druid.indexer.task.ignoreTimestampSpecForDruidInputSource	false	If true, tasks using the [Druid input source](../ingestion/native-batch-input-source.md) will ignore the provided timestampSpec, and will use the __time column of the input datasource. This option is provided for compatibility with ingestion specs written before Druid 0.22.0.
druid.indexer.task.storeEmptyColumns	true	Boolean value for whether or not to store empty columns during ingestion. When set to true, Druid stores every column specified in the [dimensionsSpec](../ingestion/ingestion-spec.md#dimensionsspec). If you use schemaless ingestion and don't specify any dimensions to ingest, you must also set [includeAllDimensions](../ingestion/ingestion-spec.md#dimensionsspec) for Druid to store empty columns.<br/><br/>If you set storeEmptyColumns to false, Druid SQL queries referencing empty columns will fail. If you intend to leave storeEmptyColumns disabled, you should either ingest dummy data for empty columns or else not query on empty columns.<br/><br/>This configuration can be overwritten by setting storeEmptyColumns in the [task context](../ingestion/tasks.md#context-parameters).
druid.peon.taskActionClient.retry.minWait	PT5S	The minimum retry time to communicate with Overlord.
druid.peon.taskActionClient.retry.maxWait	PT1M	The maximum retry time to communicate with Overlord.
druid.peon.taskActionClient.retry.maxRetryCount	60	The maximum number of retries to communicate with Overlord.
druid.server.http.numThreads	max(10, (Number of cores * 17) / 16 + 2) + 30	Number of threads for HTTP requests. Please see the [Indexer Server HTTP threads](../design/indexer.md#server-http-threads) documentation for more details on how the Indexer uses this configuration.
druid.server.http.queueSize	Unbounded	Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server.
druid.server.http.maxIdleTime	PT5M	The Jetty max idle time for a connection.
druid.server.http.enableRequestLimit	false	If enabled, no requests would be queued in jetty queue and "HTTP 429 Too Many Requests" error response would be sent. 
druid.server.http.defaultQueryTimeout	300000	Query timeout in millis, beyond which unfinished queries will be cancelled
druid.server.http.gracefulShutdownTimeout	PT30S	The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete(Only values greater than zero are valid).
druid.server.http.unannouncePropagationDelay	PT0S (do not wait)	How long to wait for ZooKeeper unannouncements to propagate before shutting down Jetty. This is a minimum and druid.server.http.gracefulShutdownTimeout does not start counting down until after this period elapses.
druid.server.http.maxQueryTimeout	Long.MAX_VALUE	Maximum allowed value (in milliseconds) for timeout parameter. See [query-context](../querying/query-context.md) to know more about timeout. Query is rejected if the query context timeout is greater than this value. 
druid.server.http.maxRequestHeaderSize	8 * 1024	Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks.
druid.server.http.enableForwardedRequestCustomizer	false	If enabled, adds Jetty ForwardedRequestCustomizer which reads X-Forwarded-* request headers to manipulate servlet request object when Druid is used behind a proxy.
druid.server.http.allowedHttpMethods	[]	List of HTTP methods that should be allowed in addition to the ones required by Druid APIs. Druid APIs require GET, PUT, POST, and DELETE, which are always allowed. This option is not useful unless you have installed an extension that needs these additional HTTP methods or that adds functionality related to CORS. None of Druid's bundled extensions require these methods.
druid.server.http.contentSecurityPolicy	frame-ancestors 'none'	Content-Security-Policy header value to set on each non-POST response. Setting this property to an empty string, or omitting it, both result in the default frame-ancestors: none being set.
druid.processing.buffer.sizeBytes	auto (max 1GiB)	This specifies a buffer size (less than 2GiB) for the storage of intermediate results. The computation engine in the Indexer processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. [Human-readable format](human-readable-byte.md) is supported.
druid.processing.buffer.poolCacheMaxCount	Integer.MAX_VALUE	processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.
druid.processing.formatString	processing-%s	Indexer processes use this format string to name their processing threads.
druid.processing.numMergeBuffers	max(2, druid.processing.numThreads / 4)	The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.
druid.processing.numThreads	Number of cores - 1 (or 1)	The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1.
druid.processing.columnCache.sizeBytes	0 (disabled)	Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.
druid.processing.fifo	true	If the processing queue should treat tasks of equal priority in a FIFO manner
druid.processing.tmpDir	path represented by java.io.tmpdir	Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path.
druid.realtime.cache.useCache	Enable the cache on the realtime.	true, false
druid.realtime.cache.populateCache	Populate the cache on the realtime.	true, false
druid.realtime.cache.unCacheable	All query types to not cache.	All druid query types
druid.realtime.cache.maxEntrySize	Maximum cache entry size in bytes.	positive integer
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8083	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	8283	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/historical	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.server.maxSize	Sum of maxSize values defined within druid.segmentCache.locations	The maximum number of bytes-worth of segments that the process wants assigned to it. The Coordinator process will attempt to assign segments to a Historical process only if this property is greater than the total size of segments served by it. Since this property defines the upper limit on the total segment size that can be assigned to a Historical, it is defaulted to the sum of all maxSize values specified within druid.segmentCache.locations property. Human-readable format is supported, see [here](human-readable-byte.md). 
druid.server.tier	  _default_tier 	 A string to name the distribution tier that the storage process belongs to. Many of the [rules Coordinator processes use](../operations/rule-configuration.md) to manage segments can be keyed on tiers. 
druid.server.priority	 0 	In a tiered architecture, the priority of the tier, thus allowing control over which processes are queried. Higher numbers mean higher priority. The default (no priority) works for architecture with no cross replication (tiers that have no data-storage overlap). Data centers typically have equal priority. 
druid.segmentCache.locations	 none 	Segments assigned to a Historical process are first stored on the local file system (in a disk cache) and then served by the Historical process. These locations define where that local cache resides. This value cannot be NULL or EMPTY. Here is an example druid.segmentCache.locations=[{"path": "/mnt/druidSegments", "maxSize": "10k", "freeSpacePercent": 1.0}]. "freeSpacePercent" is optional, if provided then enforces that much of free disk partition space while storing segments. But, it depends on File.getTotalSpace() and File.getFreeSpace() methods, so enable if only if they work for your File System.
druid.segmentCache.locationSelector.strategy	leastBytesUsed	The strategy used to select a location from the configured druid.segmentCache.locations for segment distribution. Possible values are leastBytesUsed, roundRobin, random, or mostAvailableSize. 
druid.segmentCache.deleteOnRemove	true	Delete segment files from cache once a process is no longer serving a segment.
druid.segmentCache.dropSegmentDelayMillis	30000 (30 seconds)	How long a process delays before completely dropping segment.
druid.segmentCache.infoDir	${first_location}/info_dir	Historical processes keep track of the segments they are serving so that when the process is restarted they can reload the same segments without waiting for the Coordinator to reassign. This path defines where this metadata is kept. Directory will be created if needed.
druid.segmentCache.announceIntervalMillis	5000 (5 seconds)	How frequently to announce segments while segments are loading from cache. Set this value to zero to wait for all segments to be loaded before announcing.
druid.segmentCache.numLoadingThreads	max(1,Number of cores / 6)	How many segments to drop or load concurrently from deep storage. Note that the work of loading segments involves downloading segments from deep storage, decompressing them and loading them to a memory mapped location. So the work is not all I/O Bound. Depending on CPU and network load, one could possibly increase this config to a higher value.
druid.segmentCache.numBootstrapThreads	druid.segmentCache.numLoadingThreads	How many segments to load concurrently during historical startup.
druid.segmentCache.lazyLoadOnStart	false	Whether or not to load segment columns metadata lazily during historical startup. When set to true, Historical startup time will be dramatically improved by deferring segment loading until the first time that segment takes part in a query, which will incur this cost instead.
druid.coordinator.loadqueuepeon.curator.numCallbackThreads	2	Number of threads for executing callback actions associated with loading or dropping of segments. One might want to increase this number when noticing clusters are lagging behind w.r.t. balancing segments across historical nodes.
druid.segmentCache.numThreadsToLoadSegmentsIntoPageCacheOnDownload	0	Number of threads to asynchronously read segment index files into null output stream on each new segment download after the historical process finishes bootstrapping. Recommended to set to 1 or 2 or leave unspecified to disable. See also druid.segmentCache.numThreadsToLoadSegmentsIntoPageCacheOnBootstrap
druid.segmentCache.numThreadsToLoadSegmentsIntoPageCacheOnBootstrap	druid.segmentCache.numThreadsToLoadSegmentsIntoPageCacheOnDownload	Number of threads to asynchronously read segment index files into null output stream during historical process bootstrap. This thread pool is terminated after historical process finishes bootstrapping. Recommended to set to half of available cores. If left unspecified, druid.segmentCache.numThreadsToLoadSegmentsIntoPageCacheOnDownload will be used. If both configs are unspecified, this feature is disabled. Preemptively loading segments into page cache helps in the sense that later when a segment is queried, it's already in page cache and only a minor page fault needs to be triggered instead of a more costly major page fault to make the query latency more consistent. Note that loading segment into page cache just does a blind loading of segment index files and will evict any existing segments from page cache at the discretion of operating system when the total segment size on local disk is larger than the page cache usable in the RAM, which roughly equals to total available RAM in the host - druid process memory including both heap and direct memory allocated - memory used by other non druid processes on the host, so it is the user's responsibility to ensure the host has enough RAM to host all the segments to avoid random evictions to fully leverage this feature.
druid.server.http.numThreads	max(10, (Number of cores * 17) / 16 + 2) + 30	Number of threads for HTTP requests.
druid.server.http.queueSize	Unbounded	Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server.
druid.server.http.maxIdleTime	PT5M	The Jetty max idle time for a connection.
druid.server.http.enableRequestLimit	false	If enabled, no requests would be queued in jetty queue and "HTTP 429 Too Many Requests" error response would be sent. 
druid.server.http.defaultQueryTimeout	300000	Query timeout in millis, beyond which unfinished queries will be cancelled
druid.server.http.gracefulShutdownTimeout	PT30S	The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete(Only values greater than zero are valid).
druid.server.http.unannouncePropagationDelay	PT0S (do not wait)	How long to wait for ZooKeeper unannouncements to propagate before shutting down Jetty. This is a minimum and druid.server.http.gracefulShutdownTimeout does not start counting down until after this period elapses.
druid.server.http.maxQueryTimeout	Long.MAX_VALUE	Maximum allowed value (in milliseconds) for timeout parameter. See [query-context](../querying/query-context.md) to know more about timeout. Query is rejected if the query context timeout is greater than this value. 
druid.server.http.maxRequestHeaderSize	8 * 1024	Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks.
druid.server.http.contentSecurityPolicy	frame-ancestors 'none'	Content-Security-Policy header value to set on each non-POST response. Setting this property to an empty string, or omitting it, both result in the default frame-ancestors: none being set.
druid.processing.buffer.sizeBytes	auto (max 1GiB)	This specifies a buffer size (less than 2GiB), for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed.  [Human-readable format](human-readable-byte.md) is supported.
druid.processing.buffer.poolCacheMaxCount	Integer.MAX_VALUE	processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.
druid.processing.formatString	processing-%s	Realtime and Historical processes use this format string to name their processing threads.
druid.processing.numMergeBuffers	max(2, druid.processing.numThreads / 4)	The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.
druid.processing.numThreads	Number of cores - 1 (or 1)	The number of processing threads to have available for parallel processing of segments. Our rule of thumb is num_cores - 1, which means that even under heavy load there will still be one core available to do background tasks like talking with ZooKeeper and pulling down segments. If only one core is available, this property defaults to the value 1.
druid.processing.columnCache.sizeBytes	0 (disabled)	Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.
druid.processing.fifo	true	If the processing queue should treat tasks of equal priority in a FIFO manner
druid.processing.tmpDir	path represented by java.io.tmpdir	Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path.
druid.historical.cache.useCache	Enable the cache on the Historical.	true, false
druid.historical.cache.populateCache	Populate the cache on the Historical.	true, false
druid.historical.cache.unCacheable	All query types to not cache.	All druid query types
druid.historical.cache.maxEntrySize	Maximum cache entry size in bytes.	positive integer
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8082	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	8282	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/broker	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.broker.balancer.type	Determines how the broker balances connections to Historical processes. random choose randomly, connectionCount picks the process with the fewest number of active connections to	random, connectionCount
druid.broker.select.tier	If segments are cross-replicated across tiers in a cluster, you can tell the broker to prefer to select segments in a tier with a certain priority.	highestPriority, lowestPriority, custom
druid.broker.select.tier.custom.priorities	Select servers in tiers with a custom priority list.	An array of integer priorities. E.g., [-1, 0, 1, 2]
druid.query.scheduler.numThreads	Unbounded	Maximum number of HTTP threads to dedicate to query processing. To save HTTP thread capacity, this should be lower than druid.server.http.numThreads, but it is worth noting that like druid.server.http.enableRequestLimit is set that query requests over this limit will be denied instead of waiting in the Jetty HTTP request queue.
druid.query.scheduler.laning.strategy	none	Query laning strategy to use to assign queries to a lane in order to control capacities for certain classes of queries.
druid.query.scheduler.prioritization.strategy	manual	Query prioritization strategy to automatically assign priorities.
druid.query.scheduler.prioritization.periodThreshold	None	ISO duration threshold for how old data can be queried before automatically adjusting query priority.
druid.query.scheduler.prioritization.durationThreshold	None	ISO duration threshold for maximum duration a queries interval can span before the priority is automatically adjusted.
druid.query.scheduler.prioritization.segmentCountThreshold	None	Number threshold for maximum number of segments that can take part in a query before its priority is automatically adjusted.
druid.query.scheduler.prioritization.adjustment	None	Amount to reduce the priority of queries which cross any threshold.
druid.query.scheduler.laning.maxLowPercent	No default, must be set if using this mode	Maximum percent of the smaller number of druid.server.http.numThreads or druid.query.scheduler.numThreads, defining the number of HTTP threads that can be used by queries with a priority lower than 0. Value must be an integer in the range 1 to 100, and will be rounded up
druid.query.scheduler.laning.lanes.{name}	No default, must define at least one lane with a limit above 0. If druid.query.scheduler.laning.isLimitPercent is set to true, values must be integers in the range of 1 to 100.	Maximum percent or exact limit of queries that can concurrently run in the defined lanes. Any number of lanes may be defined like this. The lane names 'total' and 'default' are reserved for internal use.
druid.query.scheduler.laning.isLimitPercent	false	If set to true, the values set for druid.query.scheduler.laning.lanes will be treated as a percent of the smaller number of druid.server.http.numThreads or druid.query.scheduler.numThreads. Note that in this mode, these lane values across lanes are _not_ required to add up to, and can exceed, 100%.
druid.server.http.numThreads	max(10, (Number of cores * 17) / 16 + 2) + 30	Number of threads for HTTP requests.
druid.server.http.queueSize	Unbounded	Size of the worker queue used by Jetty server to temporarily store incoming client connections. If this value is set and a request is rejected by jetty because queue is full then client would observe request failure with TCP connection being closed immediately with a completely empty response from server.
druid.server.http.maxIdleTime	PT5M	The Jetty max idle time for a connection.
druid.server.http.enableRequestLimit	false	If enabled, no requests would be queued in jetty queue and "HTTP 429 Too Many Requests" error response would be sent. 
druid.server.http.defaultQueryTimeout	300000	Query timeout in millis, beyond which unfinished queries will be cancelled
druid.server.http.maxScatterGatherBytes	Long.MAX_VALUE	Maximum number of bytes gathered from data processes such as Historicals and realtime processes to execute a query. Queries that exceed this limit will fail. This is an advance configuration that allows to protect in case Broker is under heavy load and not utilizing the data gathered in memory fast enough and leading to OOMs. This limit can be further reduced at query time using maxScatterGatherBytes in the context. Note that having large limit is not necessarily bad if broker is never under heavy concurrent load in which case data gathered is processed quickly and freeing up the memory used. Human-readable format is supported, see [here](human-readable-byte.md). 
druid.server.http.maxSubqueryRows	100000	Maximum number of rows from all subqueries per query. Druid stores the subquery rows in temporary tables that live in the Java heap. druid.server.http.maxSubqueryRows is a guardrail to prevent the system from exhausting available heap. When a subquery exceeds the row limit, Druid throws a resource limit exceeded exception: "Subquery generated results beyond maximum."<br /><br />It is a good practice to avoid large subqueries in Druid. However, if you choose to raise the subquery row limit, you must also increase the heap size of all Brokers, Historicals, and task Peons that process data for the subqueries to accommodate the subquery results.<br /><br />There is no formula to calculate the correct value. Trial and error is the best approach.
druid.server.http.gracefulShutdownTimeout	PT30S	The maximum amount of time Jetty waits after receiving shutdown signal. After this timeout the threads will be forcefully shutdown. This allows any queries that are executing to complete(Only values greater than zero are valid).
druid.server.http.unannouncePropagationDelay	PT0S (do not wait)	How long to wait for ZooKeeper unannouncements to propagate before shutting down Jetty. This is a minimum and druid.server.http.gracefulShutdownTimeout does not start counting down until after this period elapses.
druid.server.http.maxQueryTimeout	Long.MAX_VALUE	Maximum allowed value (in milliseconds) for timeout parameter. See [query-context](../querying/query-context.md) to know more about timeout. Query is rejected if the query context timeout is greater than this value. 
druid.server.http.maxRequestHeaderSize	8 * 1024	Maximum size of a request header in bytes. Larger headers consume more memory and can make a server more vulnerable to denial of service attacks. 
druid.server.http.contentSecurityPolicy	frame-ancestors 'none'	Content-Security-Policy header value to set on each non-POST response. Setting this property to an empty string, or omitting it, both result in the default frame-ancestors: none being set.
druid.broker.http.numConnections	20	Size of connection pool for the Broker to connect to Historical and real-time processes. If there are more queries than this number that all need to speak to the same process, then they will queue up.
druid.broker.http.eagerInitialization	true	Indicates that http connections from Broker to Historical and Real-time processes should be eagerly initialized. If set to true, numConnections connections are created upon initialization
druid.broker.http.compressionCodec	gzip	Compression codec the Broker uses to communicate with Historical and real-time processes. May be "gzip" or "identity".
druid.broker.http.readTimeout	PT15M	The timeout for data reads from Historical servers and real-time tasks.
druid.broker.http.unusedConnectionTimeout	PT4M	The timeout for idle connections in connection pool. The connection in the pool will be closed after this timeout and a new one will be established. This timeout should be less than druid.broker.http.readTimeout. Set this timeout = ~90% of druid.broker.http.readTimeout
druid.broker.http.maxQueuedBytes	25MB or 2% of maximum Broker heap size, whichever is greater	Maximum number of bytes queued per query before exerting backpressure on channels to the data servers.<br /><br />Similar to druid.server.http.maxScatterGatherBytes, except unlike that configuration, this one will trigger backpressure rather than query failure. Zero means disabled. Can be overridden by the ["maxQueuedBytes" query context parameter](../querying/query-context.md). Human-readable format is supported, see [here](human-readable-byte.md). 
druid.broker.http.numMaxThreads	max(10, ((number of cores * 17) / 16 + 2) + 30)	Maximum number of I/O worker threads
druid.broker.retryPolicy.numTries	1	Number of tries.
druid.processing.buffer.sizeBytes	auto (max 1GiB)	This specifies a buffer size (less than 2GiB) for the storage of intermediate results. The computation engine in both the Historical and Realtime processes will use a scratch buffer of this size to do all of their intermediate computations off-heap. Larger values allow for more aggregations in a single pass over the data while smaller values can require more passes depending on the query that is being executed. [Human-readable format](human-readable-byte.md) is supported.
druid.processing.buffer.poolCacheInitialCount	0	initializes the number of buffers allocated on the intermediate results pool. Note that pool can create more buffers if necessary.
druid.processing.buffer.poolCacheMaxCount	Integer.MAX_VALUE	processing buffer pool caches the buffers for later use, this is the maximum count cache will grow to. note that pool can create more buffers than it can cache if necessary.
druid.processing.numMergeBuffers	max(2, druid.processing.numThreads / 4)	The number of direct memory buffers available for merging query results. The buffers are sized by druid.processing.buffer.sizeBytes. This property is effectively a concurrency limit for queries that require merging buffers. If you are using any queries that require merge buffers (currently, just groupBy v2) then you should have at least two of these.
druid.processing.columnCache.sizeBytes	0 (disabled)	Maximum size in bytes for the dimension value lookup cache. Any value greater than 0 enables the cache. It is currently disabled by default. Enabling the lookup cache can significantly improve the performance of aggregators operating on dimension values, such as the JavaScript aggregator, or cardinality aggregator, but can slow things down if the cache hit rate is low (i.e. dimensions with few repeating values). Enabling it may also require additional garbage collection tuning to avoid long GC pauses.
druid.processing.fifo	true	If the processing queue should treat tasks of equal priority in a FIFO manner
druid.processing.tmpDir	path represented by java.io.tmpdir	Path where temporary files created while processing a query should be stored. If specified, this configuration takes priority over the default java.io.tmpdir path.
druid.processing.merge.useParallelMergePool	true	Enable automatic parallel merging for Brokers on a dedicated async ForkJoinPool. If false, instead merges will be done serially on the HTTP thread pool.
druid.processing.merge.pool.parallelism	Runtime.getRuntime().availableProcessors() * 0.75 (rounded up)	Size of ForkJoinPool. Note that the default configuration assumes that the value returned by Runtime.getRuntime().availableProcessors() represents 2 hyper-threads per physical core, and multiplies this value by 0.75 in attempt to size 1.5 times the number of _physical_ cores.
druid.processing.merge.pool.defaultMaxQueryParallelism	Runtime.getRuntime().availableProcessors() * 0.5 (rounded up)	Default maximum number of parallel merge tasks per query. Note that the default configuration assumes that the value returned by Runtime.getRuntime().availableProcessors() represents 2 hyper-threads per physical core, and multiplies this value by 0.5 in attempt to size to the number of _physical_ cores.
druid.processing.merge.pool.awaitShutdownMillis	60_000	Time to wait for merge ForkJoinPool tasks to complete before ungracefully stopping on process shutdown in milliseconds.
druid.processing.merge.task.targetRunTimeMillis	100	Ideal run-time of each ForkJoinPool merge task, before forking off a new task to continue merging sequences.
druid.processing.merge.task.initialYieldNumRows	16384	Number of rows to yield per ForkJoinPool merge task, before forking off a new task to continue merging sequences.
druid.processing.merge.task.smallBatchNumRows	4096	Size of result batches to operate on in ForkJoinPool merge tasks.
druid.broker.internal.query.config.context	null	A string formatted key:value map of a query context to add to internally generated broker queries.
druid.sql.enable	true	Whether to enable SQL at all, including background metadata fetching. If false, this overrides all other SQL-related properties and disables SQL metadata, serving, and planning completely.
druid.sql.avatica.enable	true	Whether to enable JDBC querying at /druid/v2/sql/avatica/.
druid.sql.avatica.maxConnections	25	Maximum number of open connections for the Avatica server. These are not HTTP connections, but are logical client connections that may span multiple HTTP connections.
druid.sql.avatica.maxRowsPerFrame	5,000	Maximum acceptable value for the JDBC client Statement.setFetchSize method. This setting determines the maximum number of rows that Druid will populate in a single 'fetch' for a JDBC ResultSet. Set this property to -1 to enforce no row limit on the server-side and potentially return the entire set of rows on the initial statement execution. If the JDBC client calls Statement.setFetchSize with a value other than -1, Druid uses the lesser value of the client-provided limit and maxRowsPerFrame. If maxRowsPerFrame is smaller than minRowsPerFrame, then the ResultSet size will be fixed. To handle queries that produce results with a large number of rows, you can increase value of druid.sql.avatica.maxRowsPerFrame to reduce the number of fetches required to completely transfer the result set.
druid.sql.avatica.minRowsPerFrame	100	Minimum acceptable value for the JDBC client Statement.setFetchSize method. The value for this property must greater than 0. If the JDBC client calls Statement.setFetchSize with a lesser value, Druid uses minRowsPerFrame instead. If maxRowsPerFrame is less than minRowsPerFrame, Druid uses the minimum value of the two. For handling queries which produce results with a large number of rows, you can increase this value to reduce the number of fetches required to completely transfer the result set.
druid.sql.avatica.maxStatementsPerConnection	4	Maximum number of simultaneous open statements per Avatica client connection.
druid.sql.avatica.connectionIdleTimeout	PT5M	Avatica client connection idle timeout.
druid.sql.http.enable	true	Whether to enable JSON over HTTP querying at /druid/v2/sql/.
druid.sql.planner.maxTopNLimit	100000	Maximum threshold for a [TopN query](../querying/topnquery.md). Higher limits will be planned as [GroupBy queries](../querying/groupbyquery.md) instead.
druid.sql.planner.metadataRefreshPeriod	PT1M	Throttle for metadata refreshes.
druid.sql.planner.useApproximateCountDistinct	true	Whether to use an approximate cardinality algorithm for COUNT(DISTINCT foo).
druid.sql.planner.useGroupingSetForExactDistinct	false	Only relevant when useApproximateCountDistinct is disabled. If set to true, exact distinct queries are re-written using grouping sets. Otherwise, exact distinct queries are re-written using joins. This should be set to true for group by query with multiple exact distinct aggregations. This flag can be overridden per query.
druid.sql.planner.useApproximateTopN	true	Whether to use approximate [TopN queries](../querying/topnquery.md) when a SQL query could be expressed as such. If false, exact [GroupBy queries](../querying/groupbyquery.md) will be used instead.
druid.sql.planner.requireTimeCondition	false	Whether to require SQL to have filter conditions on __time column so that all generated native queries will have user specified intervals. If true, all queries without filter condition on __time column will fail
druid.sql.planner.sqlTimeZone	UTC	Sets the default time zone for the server, which will affect how time functions and timestamp literals behave. Should be a time zone name like "America/Los_Angeles" or offset like "-08:00".
druid.sql.planner.metadataSegmentCacheEnable	false	Whether to keep a cache of published segments in broker. If true, broker polls coordinator in background to get segments from metadata store and maintains a local cache. If false, coordinator's REST API will be invoked when broker needs published segments info.
druid.sql.planner.metadataSegmentPollPeriod	60000	How often to poll coordinator for published segments list if druid.sql.planner.metadataSegmentCacheEnable is set to true. Poll period is in milliseconds. 
druid.sql.planner.authorizeSystemTablesDirectly	false	If true, Druid authorizes queries against any of the system schema tables (sys in SQL) as SYSTEM_TABLE resources which require READ access, in addition to permissions based content filtering.
druid.sql.planner.useNativeQueryExplain	true	If true, EXPLAIN PLAN FOR will return the explain plan as a JSON representation of equivalent native query(s), else it will return the original version of explain plan generated by Calcite. It can be overridden per query with useNativeQueryExplain context key.
druid.sql.planner.maxNumericInFilters	-1 (disabled)	Max limit for the amount of numeric values that can be compared for a string type dimension when the entire SQL WHERE clause of a query translates to an [OR](../querying/filters.md#or) of [Bound filter](../querying/filters.md#bound-filter). By default, Druid does not restrict the amount of numeric Bound Filters on String columns, although this situation may block other queries from running. Set this property to a smaller value to prevent Druid from running queries that have prohibitively long segment processing times. The optimal limit requires some trial and error; we recommend starting with 100.  Users who submit a query that exceeds the limit of maxNumericInFilters should instead rewrite their queries to use strings in the WHERE clause instead of numbers. For example, WHERE someString IN (â€˜123â€™, â€˜456â€™). If this value is disabled, maxNumericInFilters set through query context is ignored.
druid.sql.approxCountDistinct.function	APPROX_COUNT_DISTINCT_BUILTIN	Implementation to use for the [APPROX_COUNT_DISTINCT function](../querying/sql-aggregations.md). Without extensions loaded, the only valid value is APPROX_COUNT_DISTINCT_BUILTIN (a HyperLogLog, or HLL, based implementation). If the [DataSketches extension](../development/extensions-core/datasketches-extension.md) is loaded, this can also be APPROX_COUNT_DISTINCT_DS_HLL (alternative HLL implementation) or APPROX_COUNT_DISTINCT_DS_THETA.<br /><br />Theta sketches use significantly more memory than HLL sketches, so you should prefer one of the two HLL implementations.
druid.broker.cache.useCache	Enable the cache on the Broker.	true, false
druid.broker.cache.populateCache	Populate the cache on the Broker.	true, false
druid.broker.cache.useResultLevelCache	Enable result level caching on the Broker.	true, false
druid.broker.cache.populateResultLevelCache	Populate the result level cache on the Broker.	true, false
druid.broker.cache.resultLevelCacheLimit	Maximum size of query response that can be cached.	positive integer
druid.broker.cache.unCacheable	All query types to not cache.	All druid query types
druid.broker.cache.cacheBulkMergeLimit	Queries with more segments than this number will not attempt to fetch from cache at the broker level, leaving potential caching fetches (and cache result merging) to the Historicals	positive integer or 0
druid.broker.cache.maxEntrySize	Maximum cache entry size in bytes.	positive integer
druid.serverview.type	Segment discovery method to use. "http" enables discovering segments using HTTP instead of ZooKeeper.	batch or http
druid.broker.segment.watchedTiers	The Broker watches segment announcements from processes that serve segments to build a cache to relate each process to the segments it serves. This configuration allows the Broker to only consider segments being served from a list of tiers. By default, Broker considers all tiers. This can be used to partition your dataSources in specific Historical tiers and configure brokers in partitions so that they are only queryable for specific dataSources. This config is mutually exclusive from druid.broker.segment.ignoredTiers and at most one of these can be configured on a Broker.	List of strings
druid.broker.segment.ignoredTiers	The Broker watches segment announcements from processes that serve segments to build a cache to relate each process to the segments it serves. This configuration allows the Broker to ignore the segments being served from a list of tiers. By default, Broker considers all tiers. This config is mutually exclusive from druid.broker.segment.watchedTiers and at most one of these can be configured on a Broker.	List of strings
druid.broker.segment.watchedDataSources	Broker watches the segment announcements from processes serving segments to build cache of which process is serving which segments, this configuration allows to only consider segments being served from a whitelist of dataSources. By default, Broker would consider all datasources. This can be used to configure brokers in partitions so that they are only queryable for specific dataSources.	List of strings
druid.broker.segment.watchRealtimeTasks	The Broker watches segment announcements from processes that serve segments to build a cache to relate each process to the segments it serves.  When watchRealtimeTasks is true, the Broker watches for segment announcements from both Historicals and realtime processes. To configure a broker to exclude segments served by realtime processes, set watchRealtimeTasks to false. 	Boolean
druid.broker.segment.awaitInitializationOnStart	Whether the Broker will wait for its view of segments to fully initialize before starting up. If set to 'true', the Broker's HTTP server will not start up, and the Broker will not announce itself as available, until the server view is initialized. See also druid.sql.planner.awaitInitializationOnStart, a related setting.	Boolean
druid.cache.type	The type of cache to use for queries. See below of the configuration options for each cache type	local, memcached, hybrid, caffeine
druid.cache.sizeInBytes	0	Maximum cache size in bytes. Zero disables caching.
druid.cache.initialSize	500000	Initial size of the hashtable backing the cache.
druid.cache.logEvictionCount	0	If non-zero, log cache eviction every logEvictionCount items.
druid.cache.type	caffeine	 Set this to caffeine or leave out parameter
druid.cache.sizeInBytes	min(1GiB, Runtime.maxMemory / 10)	The maximum size of the cache in bytes on heap. It can be configured as described in [here](human-readable-byte.md). 
druid.cache.expireAfter	None (no time limit)	The time (in ms) after an access for which a cache entry may be expired
druid.cache.cacheExecutorFactory	ForkJoinPool common pool (COMMON_FJP)	The executor factory to use for Caffeine maintenance. One of COMMON_FJP, SINGLE_THREAD, or SAME_THREAD
druid.cache.evictOnClose	false	If a close of a namespace (ex: removing a segment from a process) should cause an eager eviction of associated cache values
druid.cache.expiration	2592000 (30 days)	Memcached [expiration time](https://code.google.com/p/memcached/wiki/NewCommands#Standard_Protocol).
druid.cache.timeout	500	Maximum time in milliseconds to wait for a response from Memcached.
druid.cache.hosts	none	Comma separated list of Memcached hosts <host:port>.
druid.cache.maxObjectSize	52428800 (50 MiB)	Maximum object size in bytes for a Memcached object.
druid.cache.memcachedPrefix	druid	Key prefix for all keys in Memcached.
druid.cache.numConnections	1	Number of memcached connections to use.
druid.cache.protocol	binary	Memcached communication protocol. Can be binary or text.
druid.cache.locator	consistent	Memcached locator. Can be consistent or array_mod.
druid.cache.l1.type	caffeine	type of cache to use for L1 cache. See druid.cache.type configuration for valid types.
druid.cache.l2.type	caffeine	type of cache to use for L2 cache. See druid.cache.type configuration for valid types.
druid.cache.l1.*	defaults are the same as for the given cache type.	Any property valid for the given type of L1 cache can be set using this prefix. For instance, if you are using a caffeine L1 cache, specify druid.cache.l1.sizeInBytes to set its size.
druid.cache.l2.*	defaults are the same as for the given cache type.	Prefix for L2 cache settings, see description for L1.
druid.cache.useL2	true	A boolean indicating whether to query L2 cache, if it's a miss in L1. It makes sense to configure this to false on Historical processes, if L2 is a remote cache like memcached, and this cache also used on brokers, because in this case if a query reached Historical it means that a broker didn't find corresponding results in the same remote cache, so a query to the remote cache from Historical is guaranteed to be a miss.
druid.cache.populateL2	true	A boolean indicating whether to put results into L2 cache.
druid.query.topN.minTopNThreshold	1000	See [TopN Aliasing](../querying/topnquery.md#aliasing) for details.
druid.query.search.maxSearchLimit	1000	Maximum number of search results to return.
druid.query.search.searchStrategy	useIndexes	Default search query strategy.
druid.query.segmentMetadata.defaultHistory	P1W	When no interval is specified in the query, use a default interval of defaultHistory before the end time of the most recent segment, specified in ISO8601 format. This property also controls the duration of the default interval used by GET /druid/v2/datasources/{dataSourceName} interactions for retrieving datasource dimensions/metrics.
druid.query.segmentMetadata.defaultAnalysisTypes	["cardinality", "interval", "minmax"]	This can be used to set the Default Analysis Types for all segment metadata queries, this can be overridden when making the query
druid.query.groupBy.maxSelectorDictionarySize	100000000	Maximum amount of heap space (approximately) to use for per-segment string dictionaries. See [groupBy memory tuning and resource limits](../querying/groupbyquery.md#memory-tuning-and-resource-limits) for details.
druid.query.groupBy.maxMergingDictionarySize	100000000	Maximum amount of heap space (approximately) to use for per-query string dictionaries. When the dictionary exceeds this size, a spill to disk will be triggered. See [groupBy memory tuning and resource limits](../querying/groupbyquery.md#memory-tuning-and-resource-limits) for details.
druid.query.groupBy.maxOnDiskStorage	0 (disabled)	Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling.
druid.query.groupBy.defaultOnDiskStorage	druid.query.groupBy.maxOnDiskStorage	Default amount of disk space to use, per-query, for spilling the result sets to disk when either the merging buffer or the dictionary fills up. Set to zero to disable disk spilling for queries which don't override maxOnDiskStorage in their context.
druid.query.groupBy.defaultStrategy	v2	Default groupBy query strategy.
druid.query.groupBy.singleThreaded	false	Merge results using a single thread.
druid.query.groupBy.bufferGrouperInitialBuckets	0	Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024).
druid.query.groupBy.bufferGrouperMaxLoadFactor	0	Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7).
druid.query.groupBy.forceHashAggregation	false	Force to use hash-based aggregation.
druid.query.groupBy.intermediateCombineDegree	8	Number of intermediate processes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful CPU cores.
druid.query.groupBy.numParallelCombineThreads	1 (disabled)	Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(druid.query.groupBy.numParallelCombineThreads, druid.processing.numThreads).
druid.query.groupBy.maxIntermediateRows	50000	Maximum number of intermediate rows for the per-segment grouping engine. This is a tuning parameter that does not impose a hard limit; rather, it potentially shifts merging work from the per-segment engine to the overall merging index. Queries that exceed this limit will not fail.
druid.query.groupBy.maxResults	500000	Maximum number of results. Queries that exceed this limit will fail.
druid.expressions.useStrictBooleans	false	Controls the behavior of Druid boolean operators and functions, if set to true all boolean values will be either a 1 or 0. See [expression documentation](../misc/math-expr.md#logical-operator-modes)
druid.expressions.allowNestedArrays	false	If enabled, Druid array expressions can create nested arrays. This is experimental and should be used with caution.
druid.host	InetAddress.getLocalHost().getCanonicalHostName()	The host for the current process. This is used to advertise the current processes location as reachable from another process and should generally be specified such that http://${druid.host}/ could actually talk to this process
druid.bindOnHost	false	Indicating whether the process's internal jetty server bind on druid.host. Default is false, which means binding to all interfaces.
druid.plaintextPort	8888	This is the port to actually listen on; unless port mapping is used, this will be the same port as is on druid.host
druid.tlsPort	9088	TLS port for HTTPS connector, if [druid.enableTlsPort](../operations/tls-support.md) is set then this config will be used. If druid.host contains port then that port will be ignored. This should be a non-negative Integer.
druid.service	druid/router	The name of the service. This is used as a dimension when emitting metrics and alerts to differentiate between the various services
druid.router.defaultBrokerServiceName	druid/broker	The default Broker to connect to in case service discovery fails.
druid.router.tierToBrokerMap	{"_default_tier": "<defaultBrokerServiceName>"}	Queries for a certain tier of data are routed to their appropriate Broker. This value should be an ordered JSON map of tiers to Broker names. The priority of Brokers is based on the ordering.
druid.router.defaultRule	"_default"	The default rule for all datasources.
druid.router.pollPeriod	PT1M	How often to poll for new rules.
druid.router.sql.enable	false	Enable routing of SQL queries using strategies. Whentrue, the Router uses the  strategies defined in druid.router.strategies to determine the broker service for a given SQL query. When false, the Router uses the defaultBrokerServiceName.
druid.router.strategies	[{"type":"timeBoundary"},{"type":"priority"}]	Please see [Router Strategies](../design/router.md#router-strategies) for details.
druid.router.avatica.balancer.type	rendezvousHash	Class to use for balancing Avatica queries across Brokers. Please see [Avatica Query Balancing](../design/router.md#avatica-query-balancing).
druid.router.managementProxy.enabled	false	Enables the Router's [management proxy](../design/router.md#router-as-management-proxy) functionality.
druid.router.http.numConnections	20	Size of connection pool for the Router to connect to Broker processes. If there are more queries than this number that all need to speak to the same process, then they will queue up.
druid.router.http.eagerInitialization	true	Indicates that http connections from Router to Broker should be eagerly initialized. If set to true, numConnections connections are created upon initialization
druid.router.http.readTimeout	PT15M	The timeout for data reads from Broker processes.
druid.router.http.numMaxThreads	max(10, ((number of cores * 17) / 16 + 2) + 30)	Maximum number of worker threads to handle HTTP requests and responses
druid.router.http.numRequestsQueued	1024	Maximum number of requests that may be queued to a destination
druid.router.http.requestBuffersize	8 * 1024	Size of the content buffer for receiving requests. These buffers are only used for active connections that have requests with bodies that will not fit within the header buffer