execution.allow-client-job-configurations	true	Determines whether configurations in the user program are allowed. Depending on your deployment mode failing the job might have different affects. Either your client that is trying to submit the job to an external cluster (session cluster deployment) throws the exception or the Job manager (application mode deployment).
execution.attached	false	Specifies if the pipeline is submitted in attached or detached mode.
execution.job-listeners	(none)	Custom JobListeners to be registered with the execution environment. The registered listeners cannot have constructors with arguments.
execution.shutdown-on-application-finish	true	Whether a Flink Application cluster should shut down automatically after its application finishes (either successfully or as result of a failure). Has no effect for other deployment modes.
execution.shutdown-on-attached-exit	false	If the job is submitted in attached mode, perform a best-effort cluster shutdown when the CLI is terminated abruptly, e.g., in response to a user interrupt, such as typing Ctrl + C.
execution.submit-failed-job-on-application-error	false	If a failed job should be submitted (in the application mode) when there is an error in the application driver before an actual job submission. This is intended for providing a clean way of reporting failures back to the user and is especially useful in combination with 'execution.shutdown-on-application-finish'. This option only works when the single job submission is enforced ('high-availability' is enabled). Please note that this is an experimental option and may be changed in the future.
execution.target	(none)	The deployment target for the execution. This can take one of the following values when calling bin/flink run:remotelocalyarn-per-job (deprecated)yarn-sessionkubernetes-sessionAnd one of the following values when calling bin/flink run-application:yarn-applicationkubernetes-application
state.backend.rocksdb.checkpoint.transfer.thread.num	4	The number of threads (per stateful operator) used to transfer (download and upload) files in RocksDBStateBackend.
state.backend.rocksdb.localdir	(none)	The local directory (on the TaskManager) where RocksDB puts its files. Per default, it will be <WORKING_DIR>/tmp. See process.taskmanager.working-dir for more details.
state.backend.rocksdb.memory.fixed-per-slot	(none)	The fixed total amount of memory, shared among all RocksDB instances per slot. This option overrides the 'state.backend.rocksdb.memory.managed' option when configured. If neither this option, nor the 'state.backend.rocksdb.memory.managed' optionare set, then each RocksDB column family state has its own memory caches (as controlled by the column family options).
state.backend.rocksdb.memory.high-prio-pool-ratio	0.1	The fraction of cache memory that is reserved for high-priority data like index, filter, and compression dictionary blocks. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.
state.backend.rocksdb.memory.managed	true	If set, the RocksDB state backend will automatically configure itself to use the managed memory budget of the task slot, and divide the memory over write buffers, indexes, block caches, etc. That way, the three major uses of memory of RocksDB will be capped.
state.backend.rocksdb.memory.partitioned-index-filters	false	With partitioning, the index/filter block of an SST file is partitioned into smaller blocks with an additional top-level index on them. When reading an index/filter, only top-level index is loaded into memory. The partitioned index/filter then uses the top-level index to load on demand into the block cache the partitions that are required to perform the index/filter query. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.
state.backend.rocksdb.memory.write-buffer-ratio	0.5	The maximum amount of memory that write buffers may take, as a fraction of the total shared memory. This option only has an effect when 'state.backend.rocksdb.memory.managed' or 'state.backend.rocksdb.memory.fixed-per-slot' are configured.
state.backend.rocksdb.options-factory	(none)	The options factory class for users to add customized options in DBOptions and ColumnFamilyOptions for RocksDB. If set, the RocksDB state backend will load the class and apply configs to DBOptions and ColumnFamilyOptions after loading ones from 'RocksDBConfigurableOptions' and pre-defined options.
state.backend.rocksdb.predefined-options	"DEFAULT"	The predefined settings for RocksDB DBOptions and ColumnFamilyOptions by Flink community. Current supported candidate predefined-options are DEFAULT, SPINNING_DISK_OPTIMIZED, SPINNING_DISK_OPTIMIZED_HIGH_MEM or FLASH_SSD_OPTIMIZED. Note that user customized options and options from the RocksDBOptionsFactory are applied on top of these predefined ones.
state.backend.rocksdb.timer-service.factory	ROCKSDB	This determines the factory for timer service state implementation.Possible values:"HEAP": Heap-based"ROCKSDB": Implementation based on RocksDB
taskmanager.runtime.hashjoin-bloom-filters	false	Flag to activate/deactivate bloom filters in the hybrid hash join implementation. In cases where the hash join needs to spill to disk (datasets larger than the reserved fraction of memory), these bloom filters can greatly reduce the number of spilled records, at the cost some CPU cycles.
taskmanager.runtime.large-record-handler	false	Whether to use the LargeRecordHandler when spilling. If a record will not fit into the sorting buffer. The record will be spilled on disk and the sorting will continue with only the key. The record itself will be read afterwards when merging.
taskmanager.runtime.max-fan	128	The maximal fan-in for external merge joins and fan-out for spilling hash tables. Limits the number of file handles per operator, but may cause intermediate merging/partitioning, if set too small.
taskmanager.runtime.sort-spilling-threshold	0.8	A sort operation starts spilling when this fraction of its memory budget is full.
cluster.evenly-spread-out-slots	false	Enable the slot spread out allocation strategy. This strategy tries to spread out the slots evenly across all available TaskExecutors.
cluster.fine-grained-resource-management.enabled	false	Defines whether the cluster uses fine-grained resource management.
fine-grained.shuffle-mode.all-blocking	false	Whether to convert all PIPELINE edges to BLOCKING when apply fine-grained resource management in batch jobs.
jobmanager.adaptive-batch-scheduler.avg-data-volume-per-task	1 gb	The average size of data volume to expect each task instance to process if jobmanager.scheduler has been set to AdaptiveBatch. Note that since the parallelism of the vertices is adjusted to a power of 2, the actual average size will be 0.75~1.5 times this value. It is also important to note that when data skew occurs or the decided parallelism reaches the jobmanager.adaptive-batch-scheduler.max-parallelism (due to too much data), the data actually processed by some tasks may far exceed this value.
jobmanager.adaptive-batch-scheduler.default-source-parallelism	1	The default parallelism of source vertices if jobmanager.scheduler has been set to AdaptiveBatch
jobmanager.adaptive-batch-scheduler.max-parallelism	128	The upper bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded down to a power of 2 automatically.
jobmanager.adaptive-batch-scheduler.min-parallelism	1	The lower bound of allowed parallelism to set adaptively if jobmanager.scheduler has been set to AdaptiveBatch. Currently, this option should be configured as a power of 2, otherwise it will also be rounded up to a power of 2 automatically.
jobmanager.adaptive-batch-scheduler.speculative.block-slow-node-duration	1 min	Controls how long an detected slow node should be blocked for.
jobmanager.adaptive-batch-scheduler.speculative.enabled	false	Controls whether to enable speculative execution.
jobmanager.adaptive-batch-scheduler.speculative.max-concurrent-executions	2	Controls the maximum number of execution attempts of each operator that can execute concurrently, including the original one and speculative ones.
jobmanager.adaptive-scheduler.min-parallelism-increase	1	Configure the minimum increase in parallelism for a job to scale up.
jobmanager.adaptive-scheduler.resource-stabilization-timeout	10 s	The resource stabilization timeout defines the time the JobManager will wait if fewer than the desired but sufficient resources are available. The timeout starts once sufficient resources for running the job are available. Once this timeout has passed, the job will start executing with the available resources.If scheduler-mode is configured to REACTIVE, this configuration value will default to 0, so that jobs are starting immediately with the available resources.
jobmanager.adaptive-scheduler.resource-wait-timeout	5 min	The maximum time the JobManager will wait to acquire all required resources after a job submission or restart. Once elapsed it will try to run the job with a lower parallelism, or fail if the minimum amount of resources could not be acquired.Increasing this value will make the cluster more resilient against temporary resources shortages (e.g., there is more time for a failed TaskManager to be restarted).Setting a negative duration will disable the resource timeout: The JobManager will wait indefinitely for resources to appear.If scheduler-mode is configured to REACTIVE, this configuration value will default to a negative value to disable the resource timeout.
jobmanager.scheduler	Default	Determines which scheduler implementation is used to schedule tasks. Accepted values are:'Default': Default scheduler'Adaptive': Adaptive scheduler. More details can be found here.'AdaptiveBatch': Adaptive batch scheduler. More details can be found here.Possible values:"Default""Adaptive""AdaptiveBatch"
scheduler-mode	(none)	Determines the mode of the scheduler. Note that scheduler-mode=REACTIVE is only supported by standalone application deployments, not by active resource managers (YARN, Kubernetes) or session clusters.Possible values:"REACTIVE"
slot.idle.timeout	50000	The timeout in milliseconds for a idle slot in Slot Pool.
slot.request.timeout	300000	The timeout in milliseconds for requesting a slot from Slot Pool.
slotmanager.max-total-resource.cpu	(none)	Maximum cpu cores the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'.
slotmanager.max-total-resource.memory	(none)	Maximum memory size the Flink cluster allocates for slots. Resources for JobManager and TaskManager framework are excluded. If not configured, it will be derived from 'slotmanager.number-of-slots.max'.
slotmanager.number-of-slots.max	2147483647	Defines the maximum number of slots that the Flink cluster allocates. This configuration option is meant for limiting the resource consumption for batch workloads. It is not recommended to configure this option for streaming workloads, which may fail if there are not enough slots. Note that this configuration option does not take effect for standalone clusters, where how many slots are allocated is not controlled by Flink.
slow-task-detector.check-interval	1 s	The interval to check slow tasks.
slow-task-detector.execution-time.baseline-lower-bound	1 min	The lower bound of slow task detection baseline.
slow-task-detector.execution-time.baseline-multiplier	1.5	The multiplier to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline.
slow-task-detector.execution-time.baseline-ratio	0.75	The finished execution ratio threshold to calculate the slow tasks detection baseline. Given that the parallelism is N and the ratio is R, define T as the median of the first N*R finished tasks' execution time. The baseline will be T*M, where M is the multiplier of the baseline.
metrics.fetcher.update-interval	10000	Update interval for the metric fetcher used by the web UI in milliseconds. Decrease this value for faster updating metrics. Increase this value if the metric fetcher causes too much load. Setting this value to 0 disables the metric fetching completely.
metrics.internal.query-service.port	"0"	The port range used for Flink's internal metric query service. Accepts a list of ports (“50100,50101”), ranges(“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Flink components are running on the same machine. Per default Flink will pick a random port.
metrics.internal.query-service.thread-priority	1	The thread priority used for Flink's internal metric query service. The thread is created by Akka's thread pool executor. The range of the priority is from 1 (MIN_PRIORITY) to 10 (MAX_PRIORITY). Warning, increasing this value may bring the main Flink components down.
metrics.job.status.enable	CURRENT_TIME	The selection of job status metrics that should be reported.Possible values:"STATE": For a given state, return 1 if the job is currently in that state, otherwise return 0."CURRENT_TIME": For a given state, if the job is currently in that state, return the time since the job transitioned into that state, otherwise return 0."TOTAL_TIME": For a given state, return how much time the job has spent in that state in total.
metrics.latency.granularity	"operator"	Defines the granularity of latency metrics. Accepted values are:single - Track latency without differentiating between sources and subtasks.operator - Track latency while differentiating between sources, but not subtasks.subtask - Track latency while differentiating between sources and subtasks.
metrics.latency.history-size	128	Defines the number of measured latencies to maintain at each operator.
metrics.latency.interval	0	Defines the interval at which latency tracking marks are emitted from the sources. Disables latency tracking if set to 0 or a negative value. Enabling this feature can significantly impact the performance of the cluster.
metrics.reporter.<name>.<parameter>	(none)	Configures the parameter <parameter> for the reporter named <name>.
metrics.reporter.<name>.factory.class	(none)	The reporter factory class to use for the reporter named <name>.
metrics.reporter.<name>.filter.excludes		The metrics that should be excluded for the reporter named <name>. The format is identical to filter.includes
metrics.reporter.<name>.filter.includes	"*:*:*"	The metrics that should be included for the reporter named <name>. Filters are specified as a list, with each filter following this format:<scope>[:<name>[,<name>][:<type>[,<type>]]]A metric matches a filter if the scope pattern and at least one of the name patterns and at least one of the types match.scope: Filters based on the logical scope.Specified as a pattern where * matches any sequence of characters and . separates scope components.For example: "jobmanager.job" matches any job-related metrics on the JobManager, "*.job" matches all job-related metrics and "*.job.*" matches all metrics below the job-level (i.e., task/operator metrics etc.).name: Filters based on the metric name.Specified as a comma-separate list of patterns where * matches any sequence of characters.For example, "*Records*,*Bytes*" matches any metrics where the name contains "Records" or "Bytes".type: Filters based on the metric type. Specified as a comma-separated list of metric types: [counter, meter, gauge, histogram]Examples:"*:numRecords*" Matches metrics like numRecordsIn."*.job.task.operator:numRecords*" Matches metrics like numRecordsIn on the operator level."*.job.task.operator:numRecords*:meter" Matches meter metrics like numRecordsInPerSecond on the operator level."*:numRecords*,numBytes*:counter,meter" Matches all counter/meter metrics like or numRecordsInPerSecond.
metrics.reporter.<name>.interval	10 s	The reporter interval to use for the reporter named <name>. Only applicable to push-based reporters.
metrics.reporter.<name>.scope.delimiter	"."	The delimiter used to assemble the metric identifier for the reporter named <name>.
metrics.reporter.<name>.scope.variables.additional		The map of additional variables that should be included for the reporter named <name>. Only applicable to tag-based reporters.
metrics.reporter.<name>.scope.variables.excludes	"."	The set of variables that should be excluded for the reporter named <name>. Only applicable to tag-based reporters.
metrics.reporters	(none)	An optional list of reporter names. If configured, only reporters whose name matches any of the names in the list will be started. Otherwise, all reporters that could be found in the configuration will be started.
metrics.scope.delimiter	"."	Delimiter used to assemble the metric identifier.
metrics.scope.jm	"<host>.jobmanager"	Defines the scope format string that is applied to all metrics scoped to a JobManager. Only effective when a identifier-based reporter is configured.
metrics.scope.jm.job	"<host>.jobmanager.<job_name>"	Defines the scope format string that is applied to all metrics scoped to a job on a JobManager. Only effective when a identifier-based reporter is configured
metrics.scope.operator	"<host>.taskmanager.<tm_id>.<job_name>.<operator_name>.<subtask_index>"	Defines the scope format string that is applied to all metrics scoped to an operator. Only effective when a identifier-based reporter is configured
metrics.scope.task	"<host>.taskmanager.<tm_id>.<job_name>.<task_name>.<subtask_index>"	Defines the scope format string that is applied to all metrics scoped to a task. Only effective when a identifier-based reporter is configured
metrics.scope.tm	"<host>.taskmanager.<tm_id>"	Defines the scope format string that is applied to all metrics scoped to a TaskManager. Only effective when a identifier-based reporter is configured
metrics.scope.tm.job	"<host>.taskmanager.<tm_id>.<job_name>"	Defines the scope format string that is applied to all metrics scoped to a job on a TaskManager. Only effective when a identifier-based reporter is configured
metrics.system-resource	false	Flag indicating whether Flink should report system resource metrics such as machine's CPU, memory or network usage.
metrics.system-resource-probing-interval	5000	Interval between probing of system resource metrics specified in milliseconds. Has an effect only when 'metrics.system-resource' is enabled.
queryable-state.client.network-threads	0	Number of network (Netty's event loop) Threads for queryable state client.
queryable-state.enable	false	Option whether the queryable state proxy and server should be enabled where possible and configurable.
queryable-state.proxy.network-threads	0	Number of network (Netty's event loop) Threads for queryable state proxy.
queryable-state.proxy.ports	"9069"	The port range of the queryable state proxy. The specified range can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234".
queryable-state.proxy.query-threads	0	Number of query Threads for queryable state proxy. Uses the number of slots if set to 0.
queryable-state.server.network-threads	0	Number of network (Netty's event loop) Threads for queryable state server.
queryable-state.server.ports	"9067"	The port range of the queryable state server. The specified range can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234".
queryable-state.server.query-threads	0	Number of query Threads for queryable state server. Uses the number of slots if set to 0.
state.backend.rocksdb.block.blocksize	4 kb	The approximate size (in bytes) of user data packed per block. The default blocksize is '4KB'.
state.backend.rocksdb.block.cache-size	8 mb	The amount of the cache for data blocks in RocksDB. The default block-cache size is '8MB'.
state.backend.rocksdb.block.metadata-blocksize	4 kb	Approximate size of partitioned metadata packed per block. Currently applied to indexes block when partitioned index/filters option is enabled. The default blocksize is '4KB'.
state.backend.rocksdb.bloom-filter.bits-per-key	10.0	Bits per key that bloom filter will use, this only take effect when bloom filter is used. The default value is 10.0.
state.backend.rocksdb.bloom-filter.block-based-mode	false	If true, RocksDB will use block-based filter instead of full filter, this only take effect when bloom filter is used. The default value is 'false'.
state.backend.rocksdb.compaction.level.max-size-level-base	256 mb	The upper-bound of the total size of level base files in bytes. The default value is '256MB'.
state.backend.rocksdb.compaction.level.target-file-size-base	64 mb	The target file size for compaction, which determines a level-1 file size. The default value is '64MB'.
state.backend.rocksdb.compaction.level.use-dynamic-size	false	If true, RocksDB will pick target size of each level dynamically. From an empty DB, RocksDB would make last level the base level, which means merging L0 data into the last level, until it exceeds max_bytes_for_level_base. And then repeat this process for second last level and so on. The default value is 'false'. For more information, please refer to RocksDB's doc.
state.backend.rocksdb.compaction.style	LEVEL	The specified compaction style for DB. Candidate compaction style is LEVEL, FIFO, UNIVERSAL or NONE, and Flink chooses 'LEVEL' as default style.Possible values:"LEVEL""UNIVERSAL""FIFO""NONE"
state.backend.rocksdb.files.open	-1	The maximum number of open files (per stateful operator) that can be used by the DB, '-1' means no limit. The default value is '-1'.
state.backend.rocksdb.log.dir	(none)	The directory for RocksDB's information logging files. If empty (Flink default setting), log files will be in the same directory as the Flink log. If non-empty, this directory will be used and the data directory's absolute path will be used as the prefix of the log file name. If setting this option as a non-existing location, e.g '/dev/null', RocksDB will then create the log under its own database folder as before.
state.backend.rocksdb.log.file-num	4	The maximum number of files RocksDB should keep for information logging (Default setting: 4).
state.backend.rocksdb.log.level	INFO_LEVEL	The specified information logging level for RocksDB. If unset, Flink will use INFO_LEVEL.Note: RocksDB info logs will not be written to the TaskManager logs and there is no rolling strategy, unless you configure state.backend.rocksdb.log.dir, state.backend.rocksdb.log.max-file-size, and state.backend.rocksdb.log.file-num accordingly. Without a rolling strategy, long-running tasks may lead to uncontrolled disk space usage if configured with increased log levels!There is no need to modify the RocksDB log level, unless for troubleshooting RocksDB.Possible values:"DEBUG_LEVEL""INFO_LEVEL""WARN_LEVEL""ERROR_LEVEL""FATAL_LEVEL""HEADER_LEVEL""NUM_INFO_LOG_LEVELS"
state.backend.rocksdb.log.max-file-size	25 mb	The maximum size of RocksDB's file used for information logging. If the log files becomes larger than this, a new file will be created. If 0, all logs will be written to one log file. The default maximum file size is '25MB'. 
state.backend.rocksdb.restore-overlap-fraction-threshold	0.0	The threshold of overlap fraction between the handle's key-group range and target key-group range. When restore base DB, only the handle which overlap fraction greater than or equal to threshold has a chance to be an initial handle. The default value is 0.0, there is always a handle will be selected for initialization. 
state.backend.rocksdb.thread.num	2	The maximum number of concurrent background flush and compaction jobs (per stateful operator). The default value is '2'.
state.backend.rocksdb.use-bloom-filter	false	If true, every newly created SST file will contain a Bloom filter. It is disabled by default.
state.backend.rocksdb.write-batch-size	2 mb	The max size of the consumed memory for RocksDB batch write, will flush just based on item count if this config set to 0.
state.backend.rocksdb.writebuffer.count	2	The maximum number of write buffers that are built up in memory. The default value is '2'.
state.backend.rocksdb.writebuffer.number-to-merge	1	The minimum number of write buffers that will be merged together before writing to storage. The default value is '1'.
state.backend.rocksdb.writebuffer.size	64 mb	The amount of data built up in memory (backed by an unsorted log on disk) before converting to a sorted on-disk files. The default writebuffer size is '64MB'.
zookeeper.sasl.disable	false	
zookeeper.sasl.login-context-name	"Client"	
zookeeper.sasl.service-name	"zookeeper"	
pulsar.sink.deliveryGuarantee	none	Optional delivery guarantee when committing.Possible values:"exactly-once": Records are only delivered exactly-once also under failover scenarios. To build a complete exactly-once pipeline is required that the source and sink support exactly-once and are properly configured."at-least-once": Records are ensured to be delivered but it may happen that the same record is delivered multiple times. Usually, this guarantee is faster than the exactly-once delivery."none": Records are delivered on a best effort basis. It is often the fastest way to process records but it may happen that records are lost or duplicated.
pulsar.sink.enableSchemaEvolution	false	If you enable this option and use PulsarSerializationSchema.pulsarSchema(), we would consume and deserialize the message by using Pulsar's Schema.
pulsar.sink.maxPendingMessages	1000	The maximum number of pending messages in one sink parallelism.
pulsar.sink.maxRecommitTimes	5	The allowed transaction recommit times if we meet some retryable exception. This is used in Pulsar Transaction.
pulsar.sink.messageKeyHash	murmur-3-32-hash	The hash policy for routing message by calculating the hash code of message key.Possible values:"java-hash": This hash would use String.hashCode() to calculate the message key string's hash code."murmur-3-32-hash": This hash would calculate message key's hash code by using Murmur3 algorithm.
pulsar.sink.topicMetadataRefreshInterval	1800000	Auto update the topic metadata in a fixed interval (in ms). The default value is 30 minutes.
pulsar.sink.transactionTimeoutMillis	10800000	This option is used when the user require the DeliveryGuarantee.EXACTLY_ONCE semantic.We would use transaction for making sure the message could be write only once.
jobmanager.future-pool.size	(none)	The size of the future thread pool to execute future callbacks for all spawned JobMasters. If no value is specified, then Flink defaults to the number of available CPU cores.
jobmanager.io-pool.size	(none)	The size of the IO thread pool to run blocking operations for all spawned JobMasters. This includes recovery and completion of checkpoints. Increase this value if you experience slow checkpoint operations when running many jobs. If no value is specified, then Flink defaults to the number of available CPU cores.
jmx.server.port	(none)	The port range for the JMX server to start the registry. The port config can be a single port: "9123", a range of ports: "50100-50200", or a list of ranges and ports: "50100-50200,50300-50400,51234". This option overrides metrics.reporter.*.port option.
cluster.intercept-user-system-exit	DISABLED	Flag to check user code exiting system by terminating JVM (e.g., System.exit()). Note that this configuration option can interfere with cluster.processes.halt-on-fatal-error: In intercepted user-code, a call to System.exit() will not cause the JVM to halt, when THROW is configured.Possible values:"DISABLED": Flink is not monitoring or intercepting calls to System.exit()"LOG": Log exit attempt with stack trace but still allowing exit to be performed"THROW": Throw exception when exit is attempted disallowing JVM termination
cluster.processes.halt-on-fatal-error	false	Whether processes should halt on fatal errors instead of performing a graceful shutdown. In some environments (e.g. Java 8 with the G1 garbage collector), a regular graceful shutdown can lead to a JVM deadlock. See FLINK-16510 for details.
cluster.thread-dump.stacktrace-max-depth	8	The maximum stacktrace depth of TaskManager and JobManager's thread dump web-frontend displayed.
cluster.uncaught-exception-handling	LOG	Defines whether cluster will handle any uncaught exceptions by just logging them (LOG mode), or by failing job (FAIL mode)Possible values:"LOG""FAIL"
process.jobmanager.working-dir	(none)	Working directory for Flink JobManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to process.working-dir.
process.taskmanager.working-dir	(none)	Working directory for Flink TaskManager processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to process.working-dir.
process.working-dir	io.tmp.dirs	Local working directory for Flink processes. The working directory can be used to store information that can be used upon process recovery. If not configured, then it will default to a randomly picked temporary directory defined via io.tmp.dirs.
blob.client.connect.timeout	0	The connection timeout in milliseconds for the blob client.
blob.client.socket.timeout	300000	The socket timeout in milliseconds for the blob client.
blob.fetch.backlog	1000	The config parameter defining the desired backlog of BLOB fetches on the JobManager.Note that the operating system usually enforces an upper limit on the backlog size based on the SOMAXCONN setting.
blob.fetch.num-concurrent	50	The config parameter defining the maximum number of concurrent BLOB fetches that the JobManager serves.
blob.fetch.retries	5	The config parameter defining number of retires for failed BLOB fetches.
blob.offload.minsize	1048576	The minimum size for messages to be offloaded to the BlobServer.
blob.server.port	"0"	The config parameter defining the server port of the blob service.
blob.service.cleanup.interval	3600	Cleanup interval of the blob caches at the task managers (in seconds).
blob.service.ssl.enabled	true	Flag to override ssl support for the blob service transport.
blob.storage.directory	(none)	The config parameter defining the local storage directory to be used by the blob server. If not configured, then it will default to <WORKING_DIR>/blobStorage.
state.backend.rocksdb.metrics.actual-delayed-write-rate	false	Monitor the current actual delayed write rate. 0 means no delay.
state.backend.rocksdb.metrics.background-errors	false	Monitor the number of background errors in RocksDB.
state.backend.rocksdb.metrics.block-cache-capacity	false	Monitor block cache capacity.
state.backend.rocksdb.metrics.block-cache-hit	false	Monitor the total count of block cache hit in RocksDB (BLOCK_CACHE_HIT == BLOCK_CACHE_INDEX_HIT + BLOCK_CACHE_FILTER_HIT + BLOCK_CACHE_DATA_HIT).
state.backend.rocksdb.metrics.block-cache-miss	false	Monitor the total count of block cache misses in RocksDB (BLOCK_CACHE_MISS == BLOCK_CACHE_INDEX_MISS + BLOCK_CACHE_FILTER_MISS + BLOCK_CACHE_DATA_MISS).
state.backend.rocksdb.metrics.block-cache-pinned-usage	false	Monitor the memory size for the entries being pinned in block cache.
state.backend.rocksdb.metrics.block-cache-usage	false	Monitor the memory size for the entries residing in block cache.
state.backend.rocksdb.metrics.bytes-read	false	Monitor the number of uncompressed bytes read (from memtables/cache/sst) from Get() operation in RocksDB.
state.backend.rocksdb.metrics.bytes-written	false	Monitor the number of uncompressed bytes written by DB::{Put(), Delete(), Merge(), Write()} operations, which does not include the compaction written bytes, in RocksDB.
state.backend.rocksdb.metrics.column-family-as-variable	false	Whether to expose the column family as a variable for RocksDB property based metrics.
state.backend.rocksdb.metrics.compaction-pending	false	Track pending compactions in RocksDB. Returns 1 if a compaction is pending, 0 otherwise.
state.backend.rocksdb.metrics.compaction-read-bytes	false	Monitor the bytes read during compaction in RocksDB.
state.backend.rocksdb.metrics.compaction-write-bytes	false	Monitor the bytes written during compaction in RocksDB.
state.backend.rocksdb.metrics.cur-size-active-mem-table	false	Monitor the approximate size of the active memtable in bytes.
state.backend.rocksdb.metrics.cur-size-all-mem-tables	false	Monitor the approximate size of the active and unflushed immutable memtables in bytes.
state.backend.rocksdb.metrics.estimate-live-data-size	false	Estimate of the amount of live data in bytes (usually smaller than sst files size due to space amplification).
state.backend.rocksdb.metrics.estimate-num-keys	false	Estimate the number of keys in RocksDB.
state.backend.rocksdb.metrics.estimate-pending-compaction-bytes	false	Estimated total number of bytes compaction needs to rewrite to get all levels down to under target size. Not valid for other compactions than level-based.
state.backend.rocksdb.metrics.estimate-table-readers-mem	false	Estimate the memory used for reading SST tables, excluding memory used in block cache (e.g.,filter and index blocks) in bytes.
state.backend.rocksdb.metrics.is-write-stopped	false	Track whether write has been stopped in RocksDB. Returns 1 if write has been stopped, 0 otherwise.
state.backend.rocksdb.metrics.iter-bytes-read	false	Monitor the number of uncompressed bytes read (from memtables/cache/sst) from an iterator operation in RocksDB.
state.backend.rocksdb.metrics.live-sst-files-size	false	Monitor the total size (bytes) of all SST files belonging to the latest version.WARNING: may slow down online queries if there are too many files.
state.backend.rocksdb.metrics.mem-table-flush-pending	false	Monitor the number of pending memtable flushes in RocksDB.
state.backend.rocksdb.metrics.num-deletes-active-mem-table	false	Monitor the total number of delete entries in the active memtable.
state.backend.rocksdb.metrics.num-deletes-imm-mem-tables	false	Monitor the total number of delete entries in the unflushed immutable memtables.
state.backend.rocksdb.metrics.num-entries-active-mem-table	false	Monitor the total number of entries in the active memtable.
state.backend.rocksdb.metrics.num-entries-imm-mem-tables	false	Monitor the total number of entries in the unflushed immutable memtables.
state.backend.rocksdb.metrics.num-immutable-mem-table	false	Monitor the number of immutable memtables in RocksDB.
state.backend.rocksdb.metrics.num-live-versions	false	Monitor number of live versions. Version is an internal data structure. See RocksDB file version_set.h for details. More live versions often mean more SST files are held from being deleted, by iterators or unfinished compactions.
state.backend.rocksdb.metrics.num-running-compactions	false	Monitor the number of currently running compactions.
state.backend.rocksdb.metrics.num-running-flushes	false	Monitor the number of currently running flushes.
state.backend.rocksdb.metrics.num-snapshots	false	Monitor the number of unreleased snapshots of the database.
state.backend.rocksdb.metrics.size-all-mem-tables	false	Monitor the approximate size of the active, unflushed immutable, and pinned immutable memtables in bytes.
state.backend.rocksdb.metrics.stall-micros	false	Monitor the duration of writer requiring to wait for compaction or flush to finish in RocksDB.
state.backend.rocksdb.metrics.total-sst-files-size	false	Monitor the total size (bytes) of all SST files of all versions.WARNING: may slow down online queries if there are too many files.
external-resource.<resource_name>.kubernetes.config-key	(none)	If configured, Flink will add "resources.limits.<config-key>" and "resources.requests.<config-key>" to the main container of TaskExecutor and set the value to the value of external-resource.<resource_name>.amount.
kubernetes.client.io-pool.size	4	The size of the IO executor pool used by the Kubernetes client to execute blocking IO operations (e.g. start/stop TaskManager pods, update leader related ConfigMaps, etc.). Increasing the pool size allows to run more IO operations concurrently.
kubernetes.client.user-agent	"flink"	The user agent to be used for contacting with Kubernetes APIServer.
kubernetes.cluster-id	(none)	The cluster-id, which should be no more than 45 characters, is used for identifying a unique Flink cluster. The id must only contain lowercase alphanumeric characters and "-". The required format is [a-z]([-a-z0-9]*[a-z0-9]). If not set, the client will automatically generate it with a random ID.
kubernetes.config.file	(none)	The kubernetes config file will be used to create the client. The default is located at ~/.kube/config
kubernetes.container.image	The default value depends on the actually running version. In general it looks like "flink:<FLINK_VERSION>-scala_<SCALA_VERSION>"	Image to use for Flink containers. The specified image must be based upon the same Apache Flink and Scala versions as used by the application. Visit here for the official docker images provided by the Flink project. The Flink project also publishes docker images to apache/flink DockerHub repository.
kubernetes.container.image.pull-policy	IfNotPresent	The Kubernetes container image pull policy. The default policy is IfNotPresent to avoid putting pressure to image repository.Possible values:"IfNotPresent""Always""Never"
kubernetes.container.image.pull-secrets	(none)	A semicolon-separated list of the Kubernetes secrets used to access private image registries.
kubernetes.context	(none)	The desired context from your Kubernetes config file used to configure the Kubernetes client for interacting with the cluster. This could be helpful if one has multiple contexts configured and wants to administrate different Flink clusters on different Kubernetes clusters/contexts.
kubernetes.entry.path	"/docker-entrypoint.sh"	The entrypoint script of kubernetes in the image. It will be used as command for jobmanager and taskmanager container.
kubernetes.env.secretKeyRef	(none)	The user-specified secrets to set env variables in Flink container. The value should be in the form of env:FOO_ENV,secret:foo_secret,key:foo_key;env:BAR_ENV,secret:bar_secret,key:bar_key.
kubernetes.flink.conf.dir	"/opt/flink/conf"	The flink conf directory that will be mounted in pod. The flink-conf.yaml, log4j.properties, logback.xml in this path will be overwritten from config map.
kubernetes.flink.log.dir	(none)	The directory that logs of jobmanager and taskmanager be saved in the pod. The default value is $FLINK_HOME/log.
kubernetes.hadoop.conf.config-map.name	(none)	Specify the name of an existing ConfigMap that contains custom Hadoop configuration to be mounted on the JobManager(s) and TaskManagers.
kubernetes.hostnetwork.enabled	false	Whether to enable HostNetwork mode. The HostNetwork allows the pod could use the node network namespace instead of the individual pod network namespace. Please note that the JobManager service account should have the permission to update Kubernetes service.
kubernetes.jobmanager.annotations	(none)	The user-specified annotations that are set to the JobManager pod. The value could be in the form of a1:v1,a2:v2
kubernetes.jobmanager.cpu	1.0	The number of cpu used by job manager
kubernetes.jobmanager.cpu.limit-factor	1.0	The limit factor of cpu used by job manager. The resources limit cpu will be set to cpu * limit-factor.
kubernetes.jobmanager.entrypoint.args	(none)	Extra arguments used when starting the job manager.
kubernetes.jobmanager.labels	(none)	The labels to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test.
kubernetes.jobmanager.memory.limit-factor	1.0	The limit factor of memory used by job manager. The resources limit memory will be set to memory * limit-factor.
kubernetes.jobmanager.node-selector	(none)	The node selector to be set for JobManager pod. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd.
kubernetes.jobmanager.owner.reference	(none)	The user-specified Owner References to be set to the JobManager Deployment. When all the owner resources are deleted, the JobManager Deployment will be deleted automatically, which also deletes all the resources created by this Flink cluster. The value should be formatted as a semicolon-separated list of owner references, where each owner reference is a comma-separated list of `key:value` pairs. E.g., apiVersion:v1,blockOwnerDeletion:true,controller:true,kind:FlinkApplication,name:flink-app-name,uid:flink-app-uid;apiVersion:v1,kind:Deployment,name:deploy-name,uid:deploy-uid
kubernetes.jobmanager.replicas	1	Specify how many JobManager pods will be started simultaneously. Configure the value to greater than 1 to start standby JobManagers. It will help to achieve faster recovery. Notice that high availability should be enabled when starting standby JobManagers.
kubernetes.jobmanager.service-account	"default"	Service account that is used by jobmanager within kubernetes cluster. The job manager uses this service account when requesting taskmanager pods from the API server. If not explicitly configured, config option 'kubernetes.service-account' will be used.
kubernetes.jobmanager.tolerations	(none)	The user-specified tolerations to be set to the JobManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000
kubernetes.namespace	"default"	The namespace that will be used for running the jobmanager and taskmanager pods.
kubernetes.pod-template-file	(none)	Specify a local file that contains the pod template definition. It will be used to initialize the jobmanager and taskmanager pod. The main container should be defined with name 'flink-main-container'. Notice that this can be overwritten by config options 'kubernetes.pod-template-file.jobmanager' and 'kubernetes.pod-template-file.taskmanager' for jobmanager and taskmanager respectively.
kubernetes.pod-template-file.jobmanager	(none)	Specify a local file that contains the jobmanager pod template definition. It will be used to initialize the jobmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file' will be used.
kubernetes.pod-template-file.taskmanager	(none)	Specify a local file that contains the taskmanager pod template definition. It will be used to initialize the taskmanager pod. The main container should be defined with name 'flink-main-container'. If not explicitly configured, config option 'kubernetes.pod-template-file' will be used.
kubernetes.rest-service.annotations	(none)	The user-specified annotations that are set to the rest Service. The value should be in the form of a1:v1,a2:v2
kubernetes.rest-service.exposed.node-port-address-type	InternalIP	The user-specified address type that is used for filtering node IPs when constructing a node port connection string. This option is only considered when 'kubernetes.rest-service.exposed.type' is set to 'NodePort'.Possible values:"InternalIP""ExternalIP"
kubernetes.rest-service.exposed.type	ClusterIP	The exposed type of the rest service. The exposed rest service could be used to access the Flink’s Web UI and REST endpoint.Possible values:"ClusterIP""NodePort""LoadBalancer""Headless_ClusterIP"
kubernetes.secrets	(none)	The user-specified secrets that will be mounted into Flink container. The value should be in the form of foo:/opt/secrets-foo,bar:/opt/secrets-bar.
kubernetes.service-account	"default"	Service account that is used by jobmanager and taskmanager within kubernetes cluster. Notice that this can be overwritten by config options 'kubernetes.jobmanager.service-account' and 'kubernetes.taskmanager.service-account' for jobmanager and taskmanager respectively.
kubernetes.taskmanager.annotations	(none)	The user-specified annotations that are set to the TaskManager pod. The value could be in the form of a1:v1,a2:v2
kubernetes.taskmanager.cpu	-1.0	The number of cpu used by task manager. By default, the cpu is set to the number of slots per TaskManager
kubernetes.taskmanager.cpu.limit-factor	1.0	The limit factor of cpu used by task manager. The resources limit cpu will be set to cpu * limit-factor.
kubernetes.taskmanager.entrypoint.args	(none)	Extra arguments used when starting the task manager.
kubernetes.taskmanager.labels	(none)	The labels to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, version:alphav1,deploy:test.
kubernetes.taskmanager.memory.limit-factor	1.0	The limit factor of memory used by task manager. The resources limit memory will be set to memory * limit-factor.
kubernetes.taskmanager.node-selector	(none)	The node selector to be set for TaskManager pods. Specified as key:value pairs separated by commas. For example, environment:production,disk:ssd.
kubernetes.taskmanager.service-account	"default"	Service account that is used by taskmanager within kubernetes cluster. The task manager uses this service account when watching config maps on the API server to retrieve leader address of jobmanager and resourcemanager. If not explicitly configured, config option 'kubernetes.service-account' will be used.
kubernetes.taskmanager.tolerations	(none)	The user-specified tolerations to be set to the TaskManager pod. The value should be in the form of key:key1,operator:Equal,value:value1,effect:NoSchedule;key:key2,operator:Exists,effect:NoExecute,tolerationSeconds:6000
kubernetes.transactional-operation.max-retries	5	Defines the number of Kubernetes transactional operation retries before the client gives up. For example, FlinkKubeClient#checkAndUpdateConfigMap.
job-result-store.delete-on-commit	true	Determines whether job results should be automatically removed from the underlying job result store when the corresponding entity transitions into a clean state. If false, the cleaned job results are, instead, marked as clean to indicate their state. In this case, Flink no longer has ownership and the resources need to be cleaned up by the user.
job-result-store.storage-path	(none)	Defines where job results should be stored. This should be an underlying file-system that provides read-after-write consistency. By default, this is {high-availability.storageDir}/job-result-store/{high-availability.cluster-id}.
state.backend	(none)	The state backend to be used to store state.The implementation can be specified either via their shortcut  name, or via the class name of a StateBackendFactory. If a factory is specified it is instantiated via its zero argument constructor and its StateBackendFactory#createFromConfig(ReadableConfig, ClassLoader) method is called.Recognized shortcut names are 'hashmap' and 'rocksdb'.
state.checkpoint-storage	(none)	The checkpoint storage implementation to be used to checkpoint state.The implementation can be specified either via their shortcut  name, or via the class name of a CheckpointStorageFactory. If a factory is specified it is instantiated via its zero argument constructor and its CheckpointStorageFactory#createFromConfig(ReadableConfig, ClassLoader)  method is called.Recognized shortcut names are 'jobmanager' and 'filesystem'.
state.checkpoints.dir	(none)	The default directory used for storing the data files and meta data of checkpoints in a Flink supported filesystem. The storage path must be accessible from all participating processes/nodes(i.e. all TaskManagers and JobManagers).
state.savepoints.dir	(none)	The default directory for savepoints. Used by the state backends that write savepoints to file systems (HashMapStateBackend, EmbeddedRocksDBStateBackend).
state.backend.incremental	false	Option whether the state backend should create incremental checkpoints, if possible. For an incremental checkpoint, only a diff from the previous checkpoint is stored, rather than the complete checkpoint state. Once enabled, the state size shown in web UI or fetched from rest API only represents the delta checkpoint size instead of full checkpoint size. Some state backends may not support incremental checkpoints and ignore this option.
state.backend.local-recovery	false	This option configures local recovery for this state backend. By default, local recovery is deactivated. Local recovery currently only covers keyed state backends (including both the EmbeddedRocksDBStateBackend and the HashMapStateBackend).
state.checkpoints.num-retained	1	The maximum number of completed checkpoints to retain.
taskmanager.state.local.root-dirs	(none)	The config parameter defining the root directories for storing file-based state for local recovery. Local recovery currently only covers keyed state backends. If not configured it will default to <WORKING_DIR>/localState. The <WORKING_DIR> can be configured via process.taskmanager.working-dir
high-availability	"NONE"	Defines high-availability mode used for cluster execution. To enable high-availability, set this mode to "ZOOKEEPER", "KUBERNETES", or specify the fully qualified name of the factory class.
high-availability.cluster-id	"/default"	The ID of the Flink cluster, used to separate multiple Flink clusters from each other. Needs to be set for standalone clusters but is automatically inferred in YARN.
high-availability.jobmanager.port	"0"	The port (range) used by the Flink Master for its RPC connections in highly-available setups. In highly-available setups, this value is used instead of 'jobmanager.rpc.port'.A value of '0' means that a random free port is chosen. TaskManagers discover this port through the high-availability services (leader election), so a random port or a port range works without requiring any additional means of service discovery.
high-availability.storageDir	(none)	File system path (URI) where Flink persists metadata in high-availability setups.
high-availability.zookeeper.client.acl	"open"	Defines the ACL (open|creator) to be configured on ZK node. The configuration value can be set to “creator” if the ZooKeeper server configuration has the “authProvider” property mapped to use SASLAuthenticationProvider and the cluster is configured to run in secure mode (Kerberos).
high-availability.zookeeper.client.connection-timeout	15000	Defines the connection timeout for ZooKeeper in ms.
high-availability.zookeeper.client.max-retry-attempts	3	Defines the number of connection retries before the client gives up.
high-availability.zookeeper.client.retry-wait	5000	Defines the pause between consecutive retries in ms.
high-availability.zookeeper.client.session-timeout	60000	Defines the session timeout for the ZooKeeper session in ms.
high-availability.zookeeper.client.tolerate-suspended-connections	false	Defines whether a suspended ZooKeeper connection will be treated as an error that causes the leader information to be invalidated or not. In case you set this option to true, Flink will wait until a ZooKeeper connection is marked as lost before it revokes the leadership of components. This has the effect that Flink is more resilient against temporary connection instabilities at the cost of running more likely into timing issues with ZooKeeper.
high-availability.zookeeper.path.jobgraphs	"/jobgraphs"	ZooKeeper root path (ZNode) for job graphs
high-availability.zookeeper.path.root	"/flink"	The root path under which Flink stores its entries in ZooKeeper.
high-availability.zookeeper.path.running-registry	"/running_job_registry/"	
high-availability.zookeeper.quorum	(none)	The ZooKeeper quorum to use, when running Flink in a high-availability mode with ZooKeeper.
python.archives	(none)	Add python archive files for job. The archive files will be extracted to the working directory of python UDF worker. For each archive file, a target directory is specified. If the target directory name is specified, the archive file will be extracted to a directory with the specified name. Otherwise, the archive file will be extracted to a directory with the same name of the archive file. The files uploaded via this option are accessible via relative path. '#' could be used as the separator of the archive file path and the target directory name. Comma (',') could be used as the separator to specify multiple archive files. This option can be used to upload the virtual environment, the data files used in Python UDF. The data files could be accessed in Python UDF, e.g.: f = open('data/data.txt', 'r'). The option is equivalent to the command line option "-pyarch".
python.client.executable	"python"	The path of the Python interpreter used to launch the Python process when submitting the Python jobs via "flink run" or compiling the Java/Scala jobs containing Python UDFs. Equivalent to the command line option "-pyclientexec" or the environment variable PYFLINK_CLIENT_EXECUTABLE. The priority is as following: 1. the configuration 'python.client.executable' defined in the source code(Only used in Flink Java SQL/Table API job call Python UDF);2. the command line option "-pyclientexec";3. the configuration 'python.client.executable' defined in flink-conf.yaml4. the environment variable PYFLINK_CLIENT_EXECUTABLE;
python.executable	"python"	Specify the path of the python interpreter used to execute the python UDF worker. The python UDF worker depends on Python 3.6+, Apache Beam (version == 2.38.0), Pip (version >= 20.3) and SetupTools (version >= 37.0.0). Please ensure that the specified environment meets the above requirements. The option is equivalent to the command line option "-pyexec".
python.execution-mode	"process"	Specify the python runtime execution mode. The optional values are `process` and `thread`. The `process` mode means that the Python user-defined functions will be executed in separate Python process. The `thread` mode means that the Python user-defined functions will be executed in the same process of the Java operator. Note that currently it still doesn't support to execute Python user-defined functions in `thread` mode in all places. It will fall back to `process` mode in these cases.
python.files	(none)	Attach custom files for job. The standard resource file suffixes such as .py/.egg/.zip/.whl or directory are all supported. These files will be added to the PYTHONPATH of both the local client and the remote python UDF worker. Files suffixed with .zip will be extracted and added to PYTHONPATH. Comma (',') could be used as the separator to specify multiple files. The option is equivalent to the command line option "-pyfs". 
python.fn-execution.arrow.batch.size	10000	The maximum number of elements to include in an arrow batch for Python user-defined function execution. The arrow batch size should not exceed the bundle size. Otherwise, the bundle size will be used as the arrow batch size.
python.fn-execution.bundle.size	100000	The maximum number of elements to include in a bundle for Python user-defined function execution. The elements are processed asynchronously. One bundle of elements are processed before processing the next bundle of elements. A larger value can improve the throughput, but at the cost of more memory usage and higher latency.
python.fn-execution.bundle.time	1000	Sets the waiting timeout(in milliseconds) before processing a bundle for Python user-defined function execution. The timeout defines how long the elements of a bundle will be buffered before being processed. Lower timeouts lead to lower tail latencies, but may affect throughput.
python.fn-execution.memory.managed	true	If set, the Python worker will configure itself to use the managed memory budget of the task slot. Otherwise, it will use the Off-Heap Memory of the task slot. In this case, users should set the Task Off-Heap Memory using the configuration key taskmanager.memory.task.off-heap.size.
python.map-state.iterate-response-batch-size	1000	The maximum number of the MapState keys/entries sent to Python UDF worker in each batch when iterating a Python MapState. Note that this is an experimental flag and might not be available in future releases.
python.map-state.read-cache-size	1000	The maximum number of cached entries for a single Python MapState. Note that this is an experimental flag and might not be available in future releases.
python.map-state.write-cache-size	1000	The maximum number of cached write requests for a single Python MapState. The write requests will be flushed to the state backend (managed in the Java operator) when the number of cached write requests exceed this limit. Note that this is an experimental flag and might not be available in future releases.
python.metric.enabled	true	When it is false, metric for Python will be disabled. You can disable the metric to achieve better performance at some circumstance.
python.operator-chaining.enabled	true	Python operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization.
python.profile.enabled	false	Specifies whether to enable Python worker profiling. The profile result will be displayed in the log file of the TaskManager periodically. The interval between each profiling is determined by the config options python.fn-execution.bundle.size and python.fn-execution.bundle.time.
python.requirements	(none)	Specify a requirements.txt file which defines the third-party dependencies. These dependencies will be installed and added to the PYTHONPATH of the python UDF worker. A directory which contains the installation packages of these dependencies could be specified optionally. Use '#' as the separator if the optional parameter exists. The option is equivalent to the command line option "-pyreq".
python.state.cache-size	1000	The maximum number of states cached in a Python UDF worker. Note that this is an experimental flag and might not be available in future releases.
heartbeat.interval	10000	Time interval between heartbeat RPC requests from the sender to the receiver side.
heartbeat.rpc-failure-threshold	2	The number of consecutive failed heartbeat RPCs until a heartbeat target is marked as unreachable. Failed heartbeat RPCs can be used to detect dead targets faster because they no longer receive the RPCs. The detection time is heartbeat.interval * heartbeat.rpc-failure-threshold. In environments with a flaky network, setting this value too low can produce false positives. In this case, we recommend to increase this value, but not higher than heartbeat.timeout / heartbeat.interval. The mechanism can be disabled by setting this option to -1
heartbeat.timeout	50000	Timeout for requesting and receiving heartbeats for both sender and receiver sides.
cluster.io-pool.size	(none)	The size of the IO executor pool used by the cluster to execute blocking IO operations (Master as well as TaskManager processes). By default it will use 4 * the number of CPU cores (hardware contexts) that the cluster process has access to. Increasing the pool size allows to run more IO operations concurrently.
cluster.registration.error-delay	10000	The pause made after an registration attempt caused an exception (other than timeout) in milliseconds.
cluster.registration.initial-timeout	100	Initial registration timeout between cluster components in milliseconds.
cluster.registration.max-timeout	30000	Maximum registration timeout between cluster components in milliseconds.
cluster.registration.refused-registration-delay	30000	The pause made after the registration attempt was refused in milliseconds.
cluster.services.shutdown-timeout	30000	The shutdown timeout for cluster services like executors in milliseconds.
resourcemanager.job.timeout	"5 minutes"	Timeout for jobs which don't have a job manager as leader assigned.
resourcemanager.previous-worker.recovery.timeout	0 ms	Timeout for resource manager to recover all the previous attempts workers. If exceeded, resource manager will handle new resource requests by requesting new workers. If you would like to reuse the previous workers as much as possible, you should configure a longer timeout time to wait for previous workers to register.
resourcemanager.rpc.port	0	Defines the network port to connect to for communication with the resource manager. By default, the port of the JobManager, because the same ActorSystem is used. Its not possible to use this configuration key to define port ranges.
resourcemanager.standalone.start-up-time	-1	Time in milliseconds of the start-up period of a standalone cluster. During this time, resource manager of the standalone cluster expects new task executors to be registered, and will not fail slot requests that can not be satisfied by any current registered slots. After this time, it will fail pending and new coming requests immediately that can not be satisfied by registered slots. If not set, slot.request.timeout will be used by default.
resourcemanager.start-worker.max-failure-rate	10.0	The maximum number of start worker failures (Native Kubernetes / Yarn) per minute before pausing requesting new workers. Once the threshold is reached, subsequent worker requests will be postponed to after a configured retry interval ('resourcemanager.start-worker.retry-interval').
resourcemanager.start-worker.retry-interval	3 s	The time to wait before requesting new workers (Native Kubernetes / Yarn) once the max failure rate of starting workers ('resourcemanager.start-worker.max-failure-rate') is reached.
resourcemanager.taskmanager-registration.timeout	5 min	Timeout for TaskManagers to register at the active resource managers. If exceeded, active resource manager will release and try to re-request the resource for the worker. If not configured, fallback to 'taskmanager.registration.timeout'.
resourcemanager.taskmanager-timeout	30000	The timeout for an idle task manager to be released.
slotmanager.redundant-taskmanager-num	0	The number of redundant task managers. Redundant task managers are extra task managers started by Flink, in order to speed up job recovery in case of failures due to task manager lost. Note that this feature is available only to the active deployments (native K8s, Yarn).
restart-strategy.failure-rate.delay	1 s	Delay between two consecutive restart attempts if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s"
restart-strategy.failure-rate.failure-rate-interval	1 min	Time interval for measuring failure rate if restart-strategy has been set to failure-rate. It can be specified using notation: "1 min", "20 s"
restart-strategy.failure-rate.max-failures-per-interval	1	Maximum number of restarts in given time interval before failing a job if restart-strategy has been set to failure-rate.
security.ssl.internal.close-notify-flush-timeout	-1	The timeout (in ms) for flushing the `close_notify` that was triggered by closing a channel. If the `close_notify` was not flushed in the given timeout the channel will be closed forcibly. (-1 = use system default)
security.ssl.internal.handshake-timeout	-1	The timeout (in ms) during SSL handshake. (-1 = use system default)
security.ssl.internal.session-cache-size	-1	The size of the cache used for storing SSL session objects. According to here, you should always set this to an appropriate number to not run into a bug with stalling IO threads during garbage collection. (-1 = use system default).
security.ssl.internal.session-timeout	-1	The timeout (in ms) for the cached SSL session objects. (-1 = use system default)
security.ssl.provider	"JDK"	The SSL engine provider to use for the ssl transport:JDK: default Java-based SSL engineOPENSSL: openSSL-based SSL engine using system librariesOPENSSL is based on netty-tcnative and comes in two flavours:dynamically linked: This will use your system's openSSL libraries (if compatible) and requires opt/flink-shaded-netty-tcnative-dynamic-*.jar to be copied to lib/statically linked: Due to potential licensing issues with openSSL (see LEGAL-393), we cannot ship pre-built libraries. However, you can build the required library yourself and put it into lib/:git clone https://github.com/apache/flink-shaded.git && cd flink-shaded && mvn clean package -Pinclude-netty-tcnative-static -pl flink-shaded-netty-tcnative-static
fs.allowed-fallback-filesystems	(none)	A (semicolon-separated) list of file schemes, for which Hadoop can be used instead of an appropriate Flink plugin. (example: s3;wasb)
fs.default-scheme	(none)	The default filesystem scheme, used for paths that do not declare a scheme explicitly. May contain an authority, e.g. host:port in case of an HDFS NameNode.
io.tmp.dirs	'LOCAL_DIRS' on Yarn. System.getProperty("java.io.tmpdir") in standalone.	Directories for temporary files, separated by",", "|", or the system's java.io.File.pathSeparator.
table.optimizer.agg-phase-strategy	"AUTO"	Strategy for aggregate phase. Only AUTO, TWO_PHASE or ONE_PHASE can be set. AUTO: No special enforcer for aggregate stage. Whether to choose two stage aggregate or one stage aggregate depends on cost.  TWO_PHASE: Enforce to use two stage aggregate which has localAggregate and globalAggregate. Note that if aggregate call does not support optimize into two phase, we will still use one stage aggregate. ONE_PHASE: Enforce to use one stage aggregate which only has CompleteGlobalAggregate.
table.optimizer.distinct-agg.split.bucket-num	1024	Configure the number of buckets when splitting distinct aggregation. The number is used in the first level aggregation to calculate a bucket key 'hash_code(distinct_key) % BUCKET_NUM' which is used as an additional group key after splitting.
table.optimizer.distinct-agg.split.enabled	false	Tells the optimizer whether to split distinct aggregation (e.g. COUNT(DISTINCT col), SUM(DISTINCT col)) into two level. The first aggregation is shuffled by an additional key which is calculated using the hashcode of distinct_key and number of buckets. This optimization is very useful when there is data skew in distinct aggregation and gives the ability to scale-up the job. Default is false.
table.optimizer.dynamic-filtering.enabled	true	When it is true, the optimizer will try to push dynamic filtering into scan table source, the irrelevant partitions or input data will be filtered to reduce scan I/O in runtime.
table.optimizer.join-reorder-enabled	false	Enables join reorder in optimizer. Default is disabled.
table.optimizer.join.broadcast-threshold	1048576	Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 to disable broadcasting.
table.optimizer.multiple-input-enabled	true	When it is true, the optimizer will merge the operators with pipelined shuffling into a multiple input operator to reduce shuffling and improve performance. Default value is true.
table.optimizer.non-deterministic-update.strategy	IGNORE	When it is `TRY_RESOLVE`, the optimizer tries to resolve the correctness issue caused by 'Non-Deterministic Updates' (NDU) in a changelog pipeline. Changelog may contain kinds of message types: Insert (I), Delete (D), Update_Before (UB), Update_After (UA). There's no NDU problem in an insert only changelog pipeline. For updates, there are  three main NDU problems:1. Non-deterministic functions, include scalar, table, aggregate functions, both builtin and custom ones.2. LookupJoin on an evolving source3. Cdc-source carries metadata fields which are system columns, not belongs to the entity data itself.For the first step, the optimizer automatically enables the materialization for No.2(LookupJoin) if needed, and gives the detailed error message for No.1(Non-deterministic functions) and No.3(Cdc-source with metadata) which is relatively easier to solve by changing the SQL.Default value is `IGNORE`, the optimizer does no changes.Possible values:"TRY_RESOLVE""IGNORE"
table.optimizer.reuse-source-enabled	true	When it is true, the optimizer will try to find out duplicated table sources and reuse them. This works only when table.optimizer.reuse-sub-plan-enabled is true.
table.optimizer.reuse-sub-plan-enabled	true	When it is true, the optimizer will try to find out duplicated sub-plans and reuse them.
table.optimizer.source.aggregate-pushdown-enabled	true	When it is true, the optimizer will push down the local aggregates into the TableSource which implements SupportsAggregatePushDown.
table.optimizer.source.predicate-pushdown-enabled	true	When it is true, the optimizer will push down predicates into the FilterableTableSource. Default value is true.
table.optimizer.source.report-statistics-enabled	true	When it is true, the optimizer will collect and use the statistics from source connectors if the source extends from SupportsStatisticReport and the statistics from catalog is UNKNOWN.Default value is true.
env.hadoop.conf.dir	(none)	Path to hadoop configuration directory. It is required to read HDFS and/or YARN configuration. You can also set it via environment variable.
env.hbase.conf.dir	(none)	Path to hbase configuration directory. It is required to read HBASE configuration. You can also set it via environment variable.
env.java.opts	(none)	Java options to start the JVM of all Flink processes with.
env.java.opts.client	(none)	Java options to start the JVM of the Flink Client with.
env.java.opts.historyserver	(none)	Java options to start the JVM of the HistoryServer with.
env.java.opts.jobmanager	(none)	Java options to start the JVM of the JobManager with.
env.java.opts.taskmanager	(none)	Java options to start the JVM of the TaskManager with.
env.log.dir	(none)	Defines the directory where the Flink logs are saved. It has to be an absolute path. (Defaults to the log directory under Flink’s home)
env.log.max	5	The maximum number of old log files to keep.
env.pid.dir	"/tmp"	Defines the directory where the flink-<host>-<process>.pid files are saved.
env.ssh.opts	(none)	Additional command line options passed to SSH clients when starting or stopping JobManager, TaskManager, and Zookeeper services (start-cluster.sh, stop-cluster.sh, start-zookeeper-quorum.sh, stop-zookeeper-quorum.sh).
env.yarn.conf.dir	(none)	Path to yarn configuration directory. It is required to run flink on YARN. You can also set it via environment variable.
pulsar.client.authParamMap	(none)	Parameters for the authentication plugin.
pulsar.client.authParams	(none)	Parameters for the authentication plugin.Example:key1:val1,key2:val2
pulsar.client.authPluginClassName	(none)	Name of the authentication plugin.
pulsar.client.concurrentLookupRequest	5000	The number of concurrent lookup requests allowed to send on each broker connection to prevent overload on the broker. It should be configured with a higher value only in case of it requires to produce or subscribe on thousands of topic using a created PulsarClient
pulsar.client.connectionTimeoutMs	10000	Duration (in ms) of waiting for a connection to a broker to be established.If the duration passes without a response from a broker, the connection attempt is dropped.
pulsar.client.connectionsPerBroker	1	The maximum number of connections that the client library will open to a single broker. By default, the connection pool will use a single connection for all the producers and consumers. Increasing this parameter may improve throughput when using many producers over a high latency connection.
pulsar.client.enableBusyWait	false	Option to enable busy-wait settings.This option will enable spin-waiting on executors and IO threads in order to reduce latency during context switches. The spinning will consume 100% CPU even when the broker is not doing any work. It is recommended to reduce the number of IO threads and BookKeeper client threads to only have fewer CPU cores busy.
pulsar.client.enableTransaction	false	If transaction is enabled, start the transactionCoordinatorClient with PulsarClient.
pulsar.client.initialBackoffIntervalNanos	100000000	Default duration (in nanoseconds) for a backoff interval.
pulsar.client.keepAliveIntervalSeconds	30	Interval (in seconds) for keeping connection between the Pulsar client and broker alive.
pulsar.client.listenerName	(none)	Configure the listenerName that the broker will return the corresponding advertisedListener.
pulsar.client.maxBackoffIntervalNanos	60000000000	The maximum duration (in nanoseconds) for a backoff interval.
pulsar.client.maxLookupRedirects	20	The maximum number of times a lookup-request redirections to a broker.
pulsar.client.maxLookupRequest	50000	The maximum number of lookup requests allowed on each broker connection to prevent overload on the broker. It should be greater than pulsar.client.concurrentLookupRequest. Requests that inside pulsar.client.concurrentLookupRequest are already sent to broker, and requests beyond pulsar.client.concurrentLookupRequest and under maxLookupRequests will wait in each client cnx.
pulsar.client.maxNumberOfRejectedRequestPerConnection	50	The maximum number of rejected requests of a broker in a certain period (30s) after the current connection is closed and the client creates a new connection to connect to a different broker.
pulsar.client.memoryLimitBytes	67108864	The limit (in bytes) on the amount of direct memory that will be allocated by this client instance.Note: at this moment this is only limiting the memory for producers. Setting this to 0 will disable the limit.
pulsar.client.numIoThreads	1	The number of threads used for handling connections to brokers.
pulsar.client.numListenerThreads	1	The number of threads used for handling message listeners. The listener thread pool is shared across all the consumers and readers that are using a listener model to get messages. For a given consumer, the listener is always invoked from the same thread to ensure ordering.
pulsar.client.operationTimeoutMs	30000	Operation timeout (in ms). Operations such as creating producers, subscribing or unsubscribing topics are retried during this interval. If the operation is not completed during this interval, the operation will be marked as failed.
pulsar.client.proxyProtocol	SNI	Protocol type to determine the type of proxy routing when a client connects to the proxy using pulsar.client.proxyServiceUrl.Possible values:"SNI"
pulsar.client.proxyServiceUrl	(none)	Proxy-service URL when a client connects to the broker via the proxy. The client can choose the type of proxy-routing.
pulsar.client.requestTimeoutMs	60000	Maximum duration (in ms) for completing a request. This config option is not supported before Pulsar 2.8.1
pulsar.client.serviceUrl	(none)	Service URL provider for Pulsar service.To connect to Pulsar using client libraries, you need to specify a Pulsar protocol URL.You can assign Pulsar protocol URLs to specific clusters and use the Pulsar scheme.This is an example of localhost: pulsar://localhost:6650.If you have multiple brokers, the URL is as: pulsar://localhost:6550,localhost:6651,localhost:6652A URL for a production Pulsar cluster is as: pulsar://pulsar.us-west.example.com:6650If you use TLS authentication, the URL is as pulsar+ssl://pulsar.us-west.example.com:6651
pulsar.client.sslProvider	(none)	The name of the security provider used for SSL connections. The default value is the default security provider of the JVM.
pulsar.client.statsIntervalSeconds	60	Interval between each stats info.Stats is activated with positive statsIntervalSet statsIntervalSeconds to 1 second at least.
pulsar.client.tlsAllowInsecureConnection	false	Whether the Pulsar client accepts untrusted TLS certificate from the broker.
pulsar.client.tlsCiphers		A list of cipher suites. This is a named combination of authentication, encryption, MAC and the key exchange algorithm used to negotiate the security settings for a network connection using the TLS or SSL network protocol. By default all the available cipher suites are supported.
pulsar.client.tlsHostnameVerificationEnable	false	Whether to enable TLS hostname verification. It allows to validate hostname verification when a client connects to the broker over TLS. It validates incoming x509 certificate and matches provided hostname (CN/SAN) with the expected broker's host name. It follows RFC 2818, 3.1. Server Identity hostname verification.
pulsar.client.tlsProtocols		The SSL protocol used to generate the SSLContext. By default, it is set TLS, which is fine for most cases. Allowed values in recent JVMs are TLS, TLSv1.3, TLSv1.2 and TLSv1.1.
pulsar.client.tlsTrustCertsFilePath	(none)	Path to the trusted TLS certificate file.
pulsar.client.tlsTrustStorePassword	(none)	The store password for the key store file.
pulsar.client.tlsTrustStorePath	(none)	The location of the trust store file.
pulsar.client.tlsTrustStoreType	"JKS"	The file format of the trust store file.
pulsar.client.useKeyStoreTls	false	If TLS is enabled, whether use the KeyStore type as the TLS configuration parameter. If it is set to false, it means to use the default pem type configuration.
pulsar.client.useTcpNoDelay	true	Whether to use the TCP no-delay flag on the connection to disable Nagle algorithm.No-delay features ensures that packets are sent out on the network as soon as possible, and it is critical to achieve low latency publishes. On the other hand, sending out a huge number of small packets might limit the overall throughput. Therefore, if latency is not a concern, it is recommended to set this option to false.By default, it is set to true.
jobmanager.memory.enable-jvm-direct-memory-limit	false	Whether to enable the JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize). The limit will be set to the value of 'jobmanager.memory.off-heap.size' option. 
jobmanager.memory.flink.size	(none)	Total Flink Memory size for the JobManager. This includes all the memory that a JobManager consumes, except for JVM Metaspace and JVM Overhead. It consists of JVM Heap Memory and Off-heap Memory. See also 'jobmanager.memory.process.size' for total process memory size configuration.
jobmanager.memory.heap.size	(none)	JVM Heap Memory size for JobManager. The minimum recommended JVM Heap size is 128.000mb (134217728 bytes).
jobmanager.memory.jvm-metaspace.size	256 mb	JVM Metaspace Size for the JobManager.
jobmanager.memory.jvm-overhead.fraction	0.1	Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.
jobmanager.memory.jvm-overhead.max	1 gb	Max JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.
jobmanager.memory.jvm-overhead.min	192 mb	Min JVM Overhead size for the JobManager. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less or greater than the configured min or max size, the min or max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min and max size to the same value.
jobmanager.memory.off-heap.size	128 mb	Off-heap Memory size for JobManager. This option covers all off-heap memory usage including direct and native memory allocation. The JVM direct memory limit of the JobManager process (-XX:MaxDirectMemorySize) will be set to this value if the limit is enabled by 'jobmanager.memory.enable-jvm-direct-memory-limit'. 
jobmanager.memory.process.size	(none)	Total Process Memory size for the JobManager. This includes all the memory that a JobManager JVM process consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. In containerized setups, this should be set to the container memory. See also 'jobmanager.memory.flink.size' for Total Flink Memory size configuration.
taskmanager.memory.flink.size	(none)	Total Flink Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, except for JVM Metaspace and JVM Overhead. It consists of Framework Heap Memory, Task Heap Memory, Task Off-Heap Memory, Managed Memory, and Network Memory. See also 'taskmanager.memory.process.size' for total process memory size configuration.
taskmanager.memory.framework.heap.size	128 mb	Framework Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for TaskExecutor framework, which will not be allocated to task slots.
taskmanager.memory.framework.off-heap.batch-shuffle.size	64 mb	Size of memory used by blocking shuffle for shuffle data read (currently only used by sort-shuffle and hybrid shuffle). Notes: 1) The memory is cut from 'taskmanager.memory.framework.off-heap.size' so must be smaller than that, which means you may also need to increase 'taskmanager.memory.framework.off-heap.size' after you increase this config value; 2) This memory size can influence the shuffle performance and you can increase this config value for large-scale batch jobs (for example, to 128M or 256M).
taskmanager.memory.framework.off-heap.size	128 mb	Framework Off-Heap Memory size for TaskExecutors. This is the size of off-heap memory (JVM direct memory and native memory) reserved for TaskExecutor framework, which will not be allocated to task slots. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.
taskmanager.memory.jvm-metaspace.size	256 mb	JVM Metaspace Size for the TaskExecutors.
taskmanager.memory.jvm-overhead.fraction	0.1	Fraction of Total Process Memory to be reserved for JVM Overhead. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.
taskmanager.memory.jvm-overhead.max	1 gb	Max JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.
taskmanager.memory.jvm-overhead.min	192 mb	Min JVM Overhead size for the TaskExecutors. This is off-heap memory reserved for JVM overhead, such as thread stack space, compile cache, etc. This includes native memory but not direct memory, and will not be counted when Flink calculates JVM max direct memory size parameter. The size of JVM Overhead is derived to make up the configured fraction of the Total Process Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of JVM Overhead can be explicitly specified by setting the min/max size to the same value.
taskmanager.memory.managed.consumer-weights	OPERATOR:70,STATE_BACKEND:70,PYTHON:30	Managed memory weights for different kinds of consumers. A slot’s managed memory is shared by all kinds of consumers it contains, proportionally to the kinds’ weights and regardless of the number of consumers from each kind. Currently supported kinds of consumers are OPERATOR (for built-in algorithms), STATE_BACKEND (for RocksDB state backend) and PYTHON (for Python processes).
taskmanager.memory.managed.fraction	0.4	Fraction of Total Flink Memory to be used as Managed Memory, if Managed Memory size is not explicitly specified.
taskmanager.memory.managed.size	(none)	Managed Memory size for TaskExecutors. This is the size of off-heap memory managed by the memory manager, reserved for sorting, hash tables, caching of intermediate results and RocksDB state backend. Memory consumers can either allocate memory from the memory manager in the form of MemorySegments, or reserve bytes from the memory manager and keep their memory usage within that boundary. If unspecified, it will be derived to make up the configured fraction of the Total Flink Memory.
taskmanager.memory.network.fraction	0.1	Fraction of Total Flink Memory to be used as Network Memory. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max size to the same value.
taskmanager.memory.network.max	1 gb	Max Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value.
taskmanager.memory.network.min	64 mb	Min Network Memory size for TaskExecutors. Network Memory is off-heap memory reserved for ShuffleEnvironment (e.g., network buffers). Network Memory size is derived to make up the configured fraction of the Total Flink Memory. If the derived size is less/greater than the configured min/max size, the min/max size will be used. The exact size of Network Memory can be explicitly specified by setting the min/max to the same value.
taskmanager.memory.process.size	(none)	Total Process Memory size for the TaskExecutors. This includes all the memory that a TaskExecutor consumes, consisting of Total Flink Memory, JVM Metaspace, and JVM Overhead. On containerized setups, this should be set to the container memory. See also 'taskmanager.memory.flink.size' for total Flink memory size configuration.
taskmanager.memory.task.heap.size	(none)	Task Heap Memory size for TaskExecutors. This is the size of JVM heap memory reserved for tasks. If not specified, it will be derived as Total Flink Memory minus Framework Heap Memory, Framework Off-Heap Memory, Task Off-Heap Memory, Managed Memory and Network Memory.
taskmanager.memory.task.off-heap.size	0 bytes	Task Off-Heap Memory size for TaskExecutors. This is the size of off heap memory (JVM direct memory and native memory) reserved for tasks. The configured value will be fully counted when Flink calculates the JVM max direct memory size parameter.
jobmanager.rpc.address	(none)	The config parameter defining the network address to connect to for communication with the job manager. This value is only interpreted in setups where a single JobManager with static name or address exists (simple standalone setups, or container setups with dynamic service name resolution). It is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.
jobmanager.rpc.port	6123	The config parameter defining the network port to connect to for communication with the job manager. Like jobmanager.rpc.address, this value is only interpreted in setups where a single JobManager with static name/address and port exists (simple standalone setups, or container setups with dynamic service name resolution). This config option is not used in many high-availability setups, when a leader-election service (like ZooKeeper) is used to elect and discover the JobManager leader from potentially multiple standby JobManagers.
rest.address	(none)	The address that should be used by clients to connect to the server. Attention: This option is respected only if the high-availability configuration is NONE.
rest.bind-address	(none)	The address that the server binds itself.
rest.bind-port	"8081"	The port that the server binds itself. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple Rest servers are running on the same machine.
rest.port	8081	The port that the client connects to. If rest.bind-port has not been specified, then the REST server will bind to this port. Attention: This option is respected only if the high-availability configuration is NONE.
taskmanager.data.port	0	The task manager’s external port used for data exchange operations.
taskmanager.host	(none)	The external address of the network interface where the TaskManager is exposed. Because different TaskManagers need different values for this option, usually it is specified in an additional non-shared TaskManager-specific config file.
taskmanager.rpc.port	"0"	The external RPC port where the TaskManager is exposed. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple TaskManagers are running on the same machine.
historyserver.archive.clean-expired-jobs	false	Whether HistoryServer should cleanup jobs that are no longer present `historyserver.archive.fs.dir`.
historyserver.archive.fs.dir	(none)	Comma separated list of directories to fetch archived jobs from. The history server will monitor these directories for archived jobs. You can configure the JobManager to archive jobs to a directory via `jobmanager.archive.fs.dir`.
historyserver.archive.fs.refresh-interval	10000	Interval in milliseconds for refreshing the archived job directories.
historyserver.archive.retained-jobs	-1	The maximum number of jobs to retain in each archive directory defined by `historyserver.archive.fs.dir`. If set to `-1`(default), there is no limit to the number of archives. If set to `0` or less than `-1` HistoryServer will throw an IllegalConfigurationException. 
historyserver.log.jobmanager.url-pattern	(none)	Pattern of the log URL of JobManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, `<jobid>`, to the id of job. Only http / https schemes are supported.
historyserver.log.taskmanager.url-pattern	(none)	Pattern of the log URL of TaskManager. The HistoryServer will generate actual URLs from it, with replacing the special placeholders, `<jobid>` and `<tmid>`, to the id of job and TaskManager respectively. Only http / https schemes are supported.
historyserver.web.address	(none)	Address of the HistoryServer's web interface.
historyserver.web.port	8082	Port of the HistoryServers's web interface.
historyserver.web.refresh-interval	10000	The refresh interval for the HistoryServer web-frontend in milliseconds.
historyserver.web.ssl.enabled	false	Enable HTTPs access to the HistoryServer web frontend. This is applicable only when the global SSL flag security.ssl.enabled is set to true.
historyserver.web.tmpdir	(none)	Local directory that is used by the history server REST API for temporary files.
pipeline.time-characteristic	ProcessingTime	The time characteristic for all created streams, e.g., processingtime, event time, or ingestion time.If you set the characteristic to IngestionTime or EventTime this will set a default watermark update interval of 200 ms. If this is not applicable for your application you should change it using pipeline.auto-watermark-interval.Possible values:"ProcessingTime""IngestionTime""EventTime"
cleanup-strategy.fixed-delay.attempts	infinite	The number of times that Flink retries the cleanup before giving up if cleanup-strategy has been set to fixed-delay. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.
cleanup-strategy.fixed-delay.delay	1 min	Amount of time that Flink waits before re-triggering the cleanup after a failed attempt if the cleanup-strategy is set to fixed-delay. It can be specified using the following notation: "1 min", "20 s"
classloader.check-leaked-classloader	true	Fails attempts at loading classes if the user classloader of a job is used after it has terminated. This is usually caused by the classloader being leaked by lingering threads or misbehaving libraries, which may also result in the classloader being used by other jobs. This check should only be disabled if such a leak prevents further jobs from running.
classloader.fail-on-metaspace-oom-error	true	Fail Flink JVM processes if 'OutOfMemoryError: Metaspace' is thrown while trying to load a user code class.
classloader.parent-first-patterns.additional		A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. These patterns are appended to "classloader.parent-first-patterns.default".
classloader.parent-first-patterns.default	"java.";"scala.";"org.apache.flink.";"com.esotericsoftware.kryo";"org.apache.hadoop.";"javax.annotation.";"org.xml";"javax.xml";"org.apache.xerces";"org.w3c";"org.rocksdb.";"org.slf4j";"org.apache.log4j";"org.apache.logging";"org.apache.commons.logging";"ch.qos.logback"	A (semicolon-separated) list of patterns that specifies which classes should always be resolved through the parent ClassLoader first. A pattern is a simple prefix that is checked against the fully qualified class name. This setting should generally not be modified. To add another pattern we recommend to use "classloader.parent-first-patterns.additional" instead.
classloader.resolve-order	"child-first"	Defines the class resolution strategy when loading classes from user code, meaning whether to first check the user code jar ("child-first") or the application classpath ("parent-first"). The default settings indicate to load classes first from the user code jar, which means that user code jars can include and load different dependencies than Flink uses (transitively).
fs.output.always-create-directory	false	File writers running with a parallelism larger than one create a directory for the output file path and put the different result files (one per parallel writer task) into that directory. If this option is set to "true", writers with a parallelism of 1 will also create a directory and place a single result file into it. If the option is set to "false", the writer will directly create the file directly at the output path, without creating a containing directory.
fs.overwrite-files	false	Specifies whether file output writers should overwrite existing files by default. Set to "true" to overwrite by default,"false" otherwise.
parallelism.default	1	Default parallelism for jobs.
state.backend.changelog.enabled	false	Whether to enable state backend to write state changes to StateChangelog. If this config is not set explicitly, it means no preference for enabling the change log, and the value in lower config level will take effect. The default value 'false' here means if no value set (job or cluster), the change log will not be enabled.
state.backend.changelog.max-failures-allowed	3	Max number of consecutive materialization failures allowed.
state.backend.changelog.periodic-materialize.interval	10 min	Defines the interval in milliseconds to perform periodic materialization for state backend. The periodic materialization will be disabled when the value is negative
state.backend.changelog.storage	"memory"	The storage to be used to store state changelog.The implementation can be specified via their shortcut name.The list of recognized shortcut names currently includes 'memory' and 'filesystem'.
table.builtin-catalog-name	"default_catalog"	The name of the initial catalog to be created when instantiating a TableEnvironment.
table.builtin-database-name	"default_database"	The name of the default database in the initial catalog to be created when instantiating TableEnvironment.
table.dml-sync	false	Specifies if the DML job (i.e. the insert operation) is executed asynchronously or synchronously. By default, the execution is async, so you can submit multiple DML jobs at the same time. If set this option to true, the insert operation will wait for the job to finish.
table.dynamic-table-options.enabled	true	Enable or disable the OPTIONS hint used to specify table options dynamically, if disabled, an exception would be thrown if any OPTIONS hint is specified
table.generated-code.max-length	4000	Specifies a threshold where generated code will be split into sub-function calls. Java has a maximum method length of 64 KB. This setting allows for finer granularity if necessary. Default value is 4000 instead of 64KB as by default JIT refuses to work on methods with more than 8K byte code.
table.local-time-zone	"default"	The local time zone defines current session time zone id. It is used when converting to/from <code>TIMESTAMP WITH LOCAL TIME ZONE</code>. Internally, timestamps with local time zone are always represented in the UTC time zone. However, when converting to data types that don't include a time zone (e.g. TIMESTAMP, TIME, or simply STRING), the session time zone is used during conversion. The input of option is either a full name such as "America/Los_Angeles", or a custom timezone id such as "GMT-08:00".
table.plan.compile.catalog-objects	ALL	Strategy how to persist catalog objects such as tables, functions, or data types into a plan during compilation.It influences the need for catalog metadata to be present during a restore operation and affects the plan size.This configuration option does not affect anonymous/inline or temporary objects. Anonymous/inline objects will be persisted entirely (including schema and options) if possible or fail the compilation otherwise. Temporary objects will be persisted only by their identifier and the object needs to be present in the session context during a restore.Possible values:"ALL": All metadata about catalog tables, functions, or data types will be persisted into the plan during compilation. For catalog tables, this includes the table's identifier, schema, and options. For catalog functions, this includes the function's identifier and class. For catalog data types, this includes the identifier and entire type structure. With this strategy, the catalog's metadata doesn't have to be available anymore during a restore operation."SCHEMA": In addition to an identifier, schema information about catalog tables, functions, or data types will be persisted into the plan during compilation. A schema allows for detecting incompatible changes in the catalog during a plan restore operation. However, all other metadata will still be retrieved from the catalog."IDENTIFIER": Only the identifier of catalog tables, functions, or data types will be persisted into the plan during compilation. All metadata will be retrieved from the catalog during a restore operation. With this strategy, plans become less verbose.
table.plan.force-recompile	false	When false COMPILE PLAN statement will fail if the output plan file is already existing, unless the clause IF NOT EXISTS is used. When true COMPILE PLAN will overwrite the existing output plan file. We strongly suggest to enable this flag only for debugging purpose.
table.plan.restore.catalog-objects	ALL	Strategy how to restore catalog objects such as tables, functions, or data types using a given plan and performing catalog lookups if necessary. It influences the need for catalog metadata to bepresent and enables partial enrichment of plan information.Possible values:"ALL": Reads all metadata about catalog tables, functions, or data types that has been persisted in the plan. The strategy performs a catalog lookup by identifier to fill in missing information or enrich mutable options. If the original object is not available in the catalog anymore, pipelines can still be restored if all information necessary is contained in the plan."ALL_ENFORCED": Requires that all metadata about catalog tables, functions, or data types has been persisted in the plan. The strategy will neither perform a catalog lookup by identifier nor enrich mutable options with catalog information. A restore will fail if not all information necessary is contained in the plan."IDENTIFIER": Uses only the identifier of catalog tables, functions, or data types and always performs a catalog lookup. A restore will fail if the original object is not available in the catalog anymore. Additional metadata that might be contained in the plan will be ignored.
table.resources.download-dir	System.getProperty("java.io.tmpdir")	Local directory that is used by planner for storing downloaded resources.
table.sql-dialect	"default"	The SQL dialect defines how to parse a SQL query. A different SQL dialect may support different SQL grammar. Currently supported dialects are: default and hive
pipeline.cep.sharedbuffer.cache.entry-slots	1024	The Config option to set the maximum element number the entryCache of SharedBuffer could hold. And it could accelerate the CEP operate process speed with state.And it could accelerate the CEP operate process speed and limit the capacity of cache in pure memory. Note: It's only effective to limit usage of memory when 'state.backend' was set as 'rocksdb', which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage.
pipeline.cep.sharedbuffer.cache.event-slots	1024	The Config option to set the maximum element number the eventsBufferCache of SharedBuffer could hold. And it could accelerate the CEP operate process speed and limit the capacity of cache in pure memory. Note: It's only effective to limit usage of memory when 'state.backend' was set as 'rocksdb', which would transport the elements exceeded the number of the cache into the rocksdb state storage instead of memory state storage.
pipeline.cep.sharedbuffer.cache.statistics-interval	30 min	The interval to log the information of cache state statistics in CEP operator.
metrics.reporter.influxdb.connectTimeout	10000	（可选的）连接 InfluxDB 时的超时时间。
metrics.reporter.influxdb.consistency	ONE	（可选的）InfluxDB 保存指标时的一致性级别。可选的值："ALL""ANY""ONE""QUORUM"
metrics.reporter.influxdb.db	(none)	存储运行指标的 InfluxDB 数据库名称。
metrics.reporter.influxdb.host	(none)	InfluxDB 服务器的地址。
metrics.reporter.influxdb.password	(none)	（可选的）验证 InfluxDB 时用户名的密码。
metrics.reporter.influxdb.port	8086	InfluxDB 服务器的端口号。
metrics.reporter.influxdb.retentionPolicy	(none)	（可选的）InfluxDB 保存指标时使用的保留策略。
metrics.reporter.influxdb.scheme	http	InfluxDB 使用的协议。可选的值："http""https"
metrics.reporter.influxdb.username	(none)	（可选的）验证 InfluxDB 时的用户名。
metrics.reporter.influxdb.writeTimeout	10000	（可选的）InfluxDB 的写入超时时间。
jobmanager.archive.fs.dir	(none)	Dictionary for JobManager to store the archives of completed jobs.
jobmanager.execution.attempts-history-size	16	The maximum number of historical execution attempts kept in history.
jobmanager.execution.failover-strategy	"region"	This option specifies how the job computation recovers from task failures. Accepted values are:'full': Restarts all tasks to recover the job.'region': Restarts all tasks that could be affected by the task failure. More details can be found here.
jobmanager.resource-id	(none)	The JobManager's ResourceID. If not configured, the ResourceID will be generated randomly.
jobmanager.retrieve-taskmanager-hostname	true	Flag indicating whether JobManager would retrieve canonical host name of TaskManager during registration. If the option is set to "false", TaskManager registration with JobManager could be faster, since no reverse DNS lookup is performed. However, local input split assignment (such as for HDFS files) may be impacted.
jobstore.cache-size	52428800	The job store cache size in bytes which is used to keep completed jobs in memory.
jobstore.expiration-time	3600	The time in seconds after which a completed job expires and is purged from the job store.
jobstore.max-capacity	2147483647	The max number of completed jobs that can be kept in the job store. NOTICE: if memory store keeps too many jobs in session cluster, it may cause FullGC or OOM in jm.
jobstore.type	File	Determines which job store implementation is used in session cluster. Accepted values are:'File': the file job store keeps the archived execution graphs in files'Memory': the memory job store keeps the archived execution graphs in memory. You may need to limit the jobstore.max-capacity to mitigate FullGC or OOM when there are too many graphsPossible values:"File""Memory"
web.exception-history-size	16	The maximum number of failures collected by the exception history per job.
web.access-control-allow-origin	"*"	Access-Control-Allow-Origin header for all responses from the web-frontend.
web.cancel.enable	true	Flag indicating whether jobs can be canceled from the web-frontend.
web.checkpoints.history	10	Number of checkpoints to remember for recent history.
web.history	5	Number of archived jobs for the JobManager.
web.log.path	(none)	Path to the log file (may be in /log for standalone but under log directory when using YARN).
web.refresh-interval	3000	Refresh interval for the web-frontend in milliseconds.
web.submit.enable	true	Flag indicating whether jobs can be uploaded and run from the web-frontend.
web.timeout	600000	Timeout for asynchronous operations by the web monitor in milliseconds.
web.tmpdir	System.getProperty("java.io.tmpdir")	Local directory that is used by the REST API for temporary files.
web.upload.dir	(none)	Local directory that is used by the REST API for storing uploaded jars. If not specified a dynamic directory will be created under web.tmpdir.
metrics.reporter.prometheus.deleteOnShutdown	true	Specifies whether to delete metrics from the PushGateway on shutdown. Flink will try its best to delete the metrics but this is not guaranteed. See here for more details.
metrics.reporter.prometheus.filterLabelValueCharacters	true	Specifies whether to filter label value characters. If enabled, all characters not matching [a-zA-Z0-9:_] will be removed, otherwise no characters will be removed. Before disabling this option please ensure that your label values meet the Prometheus requirements.
metrics.reporter.prometheus.groupingKey	(none)	Specifies the grouping key which is the group and global labels of all metrics. The label name and value are separated by '=', and labels are separated by ';', e.g., k1=v1;k2=v2. Please ensure that your grouping key meets the Prometheus requirements.
metrics.reporter.prometheus.hostUrl	(none)	The PushGateway server host URL including scheme, host name, and port.
metrics.reporter.prometheus.jobName	(none)	The job name under which metrics will be pushed
metrics.reporter.prometheus.randomJobNameSuffix	true	Specifies whether a random suffix should be appended to the job name.
pipeline.auto-generate-uids	true	When auto-generated UIDs are disabled, users are forced to manually specify UIDs on DataStream applications.It is highly recommended that users specify UIDs before deploying to production since they are used to match state in savepoints to operators in a job. Because auto-generated ID's are likely to change when modifying a job, specifying custom IDs allow an application to evolve over time without discarding state.
pipeline.auto-type-registration	true	Controls whether Flink is automatically registering all types in the user programs with Kryo.
pipeline.auto-watermark-interval	0 ms	The interval of the automatic watermark emission. Watermarks are used throughout the streaming system to keep track of the progress of time. They are used, for example, for time based windowing.
pipeline.cached-files	(none)	Files to be registered at the distributed cache under the given name. The files will be accessible from any user-defined function in the (distributed) runtime under a local path. Files may be local files (which will be distributed via BlobServer), or files in a distributed file system. The runtime will copy the files temporarily to a local cache, if needed.Example:name:file1,path:`file:///tmp/file1`;name:file2,path:`hdfs:///tmp/file2`
pipeline.classpaths	(none)	A semicolon-separated list of the classpaths to package with the job jars to be sent to the cluster. These have to be valid URLs.
pipeline.closure-cleaner-level	RECURSIVE	Configures the mode in which the closure cleaner works.Possible values:"NONE": Disables the closure cleaner completely."TOP_LEVEL": Cleans only the top-level class without recursing into fields."RECURSIVE": Cleans all fields recursively.
pipeline.default-kryo-serializers	(none)	Semicolon separated list of pairs of class names and Kryo serializers class names to be used as Kryo default serializersExample:class:org.example.ExampleClass,serializer:org.example.ExampleSerializer1; class:org.example.ExampleClass2,serializer:org.example.ExampleSerializer2
pipeline.force-avro	false	Forces Flink to use the Apache Avro serializer for POJOs.Important: Make sure to include the flink-avro module.
pipeline.force-kryo	false	If enabled, forces TypeExtractor to use Kryo serializer for POJOS even though we could analyze as POJO. In some cases this might be preferable. For example, when using interfaces with subclasses that cannot be analyzed as POJO.
pipeline.generic-types	true	If the use of generic types is disabled, Flink will throw an UnsupportedOperationException whenever it encounters a data type that would go through Kryo for serialization.Disabling generic types can be helpful to eagerly find and eliminate the use of types that would go through Kryo serialization during runtime. Rather than checking types individually, using this option will throw exceptions eagerly in the places where generic types are used.We recommend to use this option only during development and pre-production phases, not during actual production use. The application program and/or the input data may be such that new, previously unseen, types occur at some point. In that case, setting this option would cause the program to fail.
pipeline.global-job-parameters	(none)	Register a custom, serializable user configuration object. The configuration can be  accessed in operators
pipeline.jars	(none)	A semicolon-separated list of the jars to package with the job jars to be sent to the cluster. These have to be valid paths.
pipeline.max-parallelism	-1	The program-wide maximum parallelism used for operators which haven't specified a maximum parallelism. The maximum parallelism specifies the upper limit for dynamic scaling and the number of key groups used for partitioned state.
pipeline.name	(none)	The job name used for printing and logging.
pipeline.object-reuse	false	When enabled objects that Flink internally uses for deserialization and passing data to user-code functions will be reused. Keep in mind that this can lead to bugs when the user-code function of an operation is not aware of this behaviour.
pipeline.operator-chaining	true	Operator chaining allows non-shuffle operations to be co-located in the same thread fully avoiding serialization and de-serialization.
pipeline.registered-kryo-types	(none)	Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written.
pipeline.registered-pojo-types	(none)	Semicolon separated list of types to be registered with the serialization stack. If the type is eventually serialized as a POJO, then the type is registered with the POJO serializer. If the type ends up being serialized with Kryo, then it will be registered at Kryo to make sure that only tags are written.
pipeline.vertex-description-mode	TREE	The mode how we organize description of a job vertex.Possible values:"TREE""CASCADING"
pipeline.vertex-name-include-index-prefix	false	Whether name of vertex includes topological index or not. When it is true, the name will have a prefix of index of the vertex, like '[vertex-0]Source: source'. It is false by default
security.context.factory.classes	"org.apache.flink.runtime.security.contexts.HadoopSecurityContextFactory";"org.apache.flink.runtime.security.contexts.NoOpSecurityContextFactory"	List of factories that should be used to instantiate a security context. If multiple are configured, Flink will use the first compatible factory. You should have a NoOpSecurityContextFactory in this list as a fallback.
security.kerberos.access.hadoopFileSystems	(none)	A comma-separated list of Kerberos-secured Hadoop filesystems Flink is going to access. For example, security.kerberos.access.hadoopFileSystems=hdfs://namenode2:9002,hdfs://namenode3:9003. The JobManager needs to have access to these filesystems to retrieve the security tokens.
security.kerberos.fetch.delegation-token	true	Indicates whether to fetch the delegation tokens for external services the Flink job needs to contact. Only HDFS and HBase are supported. It is used in Yarn deployments. If true, Flink will fetch HDFS and HBase delegation tokens and inject them into Yarn AM containers. If false, Flink will assume that the delegation tokens are managed outside of Flink. As a consequence, it will not fetch delegation tokens for HDFS and HBase. You may need to disable this option, if you rely on submission mechanisms, e.g. Apache Oozie, to handle delegation tokens.
security.kerberos.krb5-conf.path	(none)	Specify the local location of the krb5.conf file. If defined, this conf would be mounted on the JobManager and TaskManager containers/pods for Kubernetes and Yarn. Note: The KDC defined needs to be visible from inside the containers.
security.kerberos.login.contexts	(none)	A comma-separated list of login contexts to provide the Kerberos credentials to (for example, `Client,KafkaClient` to use the credentials for ZooKeeper authentication and for Kafka authentication)
security.kerberos.login.keytab	(none)	Absolute path to a Kerberos keytab file that contains the user credentials.
security.kerberos.login.principal	(none)	Kerberos principal name associated with the keytab.
security.kerberos.login.use-ticket-cache	true	Indicates whether to read from your Kerberos ticket cache.
security.kerberos.relogin.period	1 min	The time period when keytab login happens automatically in order to always have a valid TGT.
security.kerberos.tokens.renewal.retry.backoff	1 h	The time period how long to wait before retrying to obtain new delegation tokens after a failure.
security.kerberos.tokens.renewal.time-ratio	0.75	Ratio of the tokens's expiration time when new credentials should be re-obtained.
security.module.factory.classes	"org.apache.flink.runtime.security.modules.HadoopModuleFactory";"org.apache.flink.runtime.security.modules.JaasModuleFactory";"org.apache.flink.runtime.security.modules.ZookeeperModuleFactory"	List of factories that should be used to instantiate security modules. All listed modules will be installed. Keep in mind that the configured security context might rely on some modules being present.
security.ssl.algorithms	"TLS_RSA_WITH_AES_128_CBC_SHA"	The comma separated list of standard SSL algorithms to be supported. Read more here
security.ssl.internal.cert.fingerprint	(none)	The sha1 fingerprint of the internal certificate. This further protects the internal communication to present the exact certificate used by Flink.This is necessary where one cannot use private CA(self signed) or there is internal firm wide CA is required
security.ssl.internal.enabled	false	Turns on SSL for internal network communication. Optionally, specific components may override this through their own settings (rpc, data transport, REST, etc).
security.ssl.internal.key-password	(none)	The secret to decrypt the key in the keystore for Flink's internal endpoints (rpc, data transport, blob server).
security.ssl.internal.keystore	(none)	The Java keystore file with SSL Key and Certificate, to be used Flink's internal endpoints (rpc, data transport, blob server).
security.ssl.internal.keystore-password	(none)	The secret to decrypt the keystore file for Flink's for Flink's internal endpoints (rpc, data transport, blob server).
security.ssl.internal.truststore	(none)	The truststore file containing the public CA certificates to verify the peer for Flink's internal endpoints (rpc, data transport, blob server).
security.ssl.internal.truststore-password	(none)	The password to decrypt the truststore for Flink's internal endpoints (rpc, data transport, blob server).
security.ssl.protocol	"TLSv1.2"	The SSL protocol version to be supported for the ssl transport. Note that it doesn’t support comma separated list.
security.ssl.rest.authentication-enabled	false	Turns on mutual SSL authentication for external communication via the REST endpoints.
security.ssl.rest.cert.fingerprint	(none)	The sha1 fingerprint of the rest certificate. This further protects the rest REST endpoints to present certificate which is only used by proxy serverThis is necessary where once uses public CA or internal firm wide CA
security.ssl.rest.enabled	false	Turns on SSL for external communication via the REST endpoints.
security.ssl.rest.key-password	(none)	The secret to decrypt the key in the keystore for Flink's external REST endpoints.
security.ssl.rest.keystore	(none)	The Java keystore file with SSL Key and Certificate, to be used Flink's external REST endpoints.
security.ssl.rest.keystore-password	(none)	The secret to decrypt the keystore file for Flink's for Flink's external REST endpoints.
security.ssl.rest.truststore	(none)	The truststore file containing the public CA certificates to verify the peer for Flink's external REST endpoints.
security.ssl.rest.truststore-password	(none)	The password to decrypt the truststore for Flink's external REST endpoints.
security.ssl.verify-hostname	true	Flag to enable peer’s hostname verification during ssl handshake.
high-availability.kubernetes.leader-election.lease-duration	15 s	Define the lease duration for the Kubernetes leader election. The leader will continuously renew its lease time to indicate its existence. And the followers will do a lease checking against the current time. "renewTime + leaseDuration > now" means the leader is alive.
high-availability.kubernetes.leader-election.renew-deadline	15 s	Defines the deadline duration when the leader tries to renew the lease. The leader will give up its leadership if it cannot successfully renew the lease in the given time.
high-availability.kubernetes.leader-election.retry-period	5 s	Defines the pause duration between consecutive retries. All the contenders, including the current leader and all other followers, periodically try to acquire/renew the leadership if possible at this interval.
taskmanager.network.batch-shuffle.compression.enabled	true	Boolean flag indicating whether the shuffle data will be compressed for batch shuffle mode. Note that data is compressed per buffer and compression can incur extra CPU overhead, so it is more effective for IO bounded scenario when compression ratio is high.
taskmanager.network.blocking-shuffle.type	"file"	The blocking shuffle type, either "mmap" or "file". The "auto" means selecting the property type automatically based on system memory architecture (64 bit for mmap and 32 bit for file). Note that the memory usage of mmap is not accounted by configured memory limits, but some resource frameworks like yarn would track this memory usage and kill the container once memory exceeding some threshold. Also note that this option is experimental and might be changed future.
taskmanager.network.compression.codec	"LZ4"	The codec to be used when compressing shuffle data, only "LZ4", "LZO" and "ZSTD" are supported now. Through tpc-ds test of these three algorithms, the results show that "LZ4" algorithm has the highest compression and decompression speed, but the compression ratio is the lowest. "ZSTD" has the highest compression ratio, but the compression and decompression speed is the slowest, and LZO is between the two. Also note that this option is experimental and might be changed in the future.
taskmanager.network.detailed-metrics	false	Boolean flag to enable/disable more detailed metrics about inbound/outbound network queue lengths.
taskmanager.network.max-num-tcp-connections	1	The maximum number of tpc connections between taskmanagers for data communication.
taskmanager.network.memory.buffer-debloat.enabled	false	The switch of the automatic buffered debloating feature. If enabled the amount of in-flight data will be adjusted automatically accordingly to the measured throughput.
taskmanager.network.memory.buffer-debloat.period	200 ms	The minimum period of time after which the buffer size will be debloated if required. The low value provides a fast reaction to the load fluctuation but can influence the performance.
taskmanager.network.memory.buffer-debloat.samples	20	The number of the last buffer size values that will be taken for the correct calculation of the new one.
taskmanager.network.memory.buffer-debloat.target	1 s	The target total time after which buffered in-flight data should be fully consumed. This configuration option will be used, in combination with the measured throughput, to adjust the amount of in-flight data.
taskmanager.network.memory.buffer-debloat.threshold-percentages	25	The minimum difference in percentage between the newly calculated buffer size and the old one to announce the new value. Can be used to avoid constant back and forth small adjustments.
taskmanager.network.memory.buffers-per-channel	2	Number of exclusive network buffers to use for each outgoing/incoming channel (subpartition/input channel) in the credit-based flow control model. It should be configured at least 2 for good performance. 1 buffer is for receiving in-flight data in the subpartition and 1 buffer is for parallel serialization. The minimum valid value that can be configured is 0. When 0 buffers-per-channel is configured, the exclusive network buffers used per downstream incoming channel will be 0, but for each upstream outgoing channel, max(1, configured value) will be used. In other words we ensure that, for performance reasons, there is at least one buffer per outgoing channel regardless of the configuration.
taskmanager.network.memory.floating-buffers-per-gate	8	Number of extra network buffers to use for each outgoing/incoming gate (result partition/input gate). In credit-based flow control mode, this indicates how many floating credits are shared among all the input channels. The floating buffers are distributed based on backlog (real-time output buffers in the subpartition) feedback, and can help relieve back-pressure caused by unbalanced data distribution among the subpartitions. This value should be increased in case of higher round trip times between nodes and/or larger number of machines in the cluster.
taskmanager.network.memory.max-buffers-per-channel	10	Number of max buffers that can be used for each channel. If a channel exceeds the number of max buffers, it will make the task become unavailable, cause the back pressure and block the data processing. This might speed up checkpoint alignment by preventing excessive growth of the buffered in-flight data in case of data skew and high number of configured floating buffers. This limit is not strictly guaranteed, and can be ignored by things like flatMap operators, records spanning multiple buffers or single timer producing large amount of data.
taskmanager.network.memory.max-overdraft-buffers-per-gate	5	Number of max overdraft network buffers to use for each ResultPartition. The overdraft buffers will be used when the subtask cannot apply to the normal buffers  due to back pressure, while subtask is performing an action that can not be interrupted in the middle,  like serializing a large record, flatMap operator producing multiple records for one single input record or processing time timer producing large output. In situations like that system will allow subtask to request overdraft buffers, so that the subtask can finish such uninterruptible action, without blocking unaligned checkpoints for long period of time. Overdraft buffers are provided on best effort basis only if the system has some unused buffers available. Subtask that has used overdraft buffers won't be allowed to process any more records until the overdraft buffers are returned to the pool.
taskmanager.network.netty.client.connectTimeoutSec	120	The Netty client connection timeout.
taskmanager.network.netty.client.numThreads	-1	The number of Netty client threads.
taskmanager.network.netty.num-arenas	-1	The number of Netty arenas.
taskmanager.network.netty.sendReceiveBufferSize	0	The Netty send and receive buffer size. This defaults to the system buffer size (cat /proc/sys/net/ipv4/tcp_[rw]mem) and is 4 MiB in modern Linux.
taskmanager.network.netty.server.backlog	0	The netty server connection backlog.
taskmanager.network.netty.server.numThreads	-1	The number of Netty server threads.
taskmanager.network.netty.transport	"auto"	The Netty transport type, either "nio" or "epoll". The "auto" means selecting the property mode automatically based on the platform. Note that the "epoll" mode can get better performance, less GC and have more advanced features which are only available on modern Linux.
taskmanager.network.request-backoff.initial	100	Minimum backoff in milliseconds for partition requests of input channels.
taskmanager.network.request-backoff.max	10000	Maximum backoff in milliseconds for partition requests of input channels.
taskmanager.network.retries	0	The number of retry attempts for network communication. Currently it's only used for establishing input/output channel connections
taskmanager.network.sort-shuffle.min-buffers	512	Minimum number of network buffers required per blocking result partition for sort-shuffle. For production usage, it is suggested to increase this config value to at least 2048 (64M memory if the default 32K memory segment size is used) to improve the data compression ratio and reduce the small network packets. Usually, several hundreds of megabytes memory is enough for large scale batch jobs. Note: you may also need to increase the size of total network memory to avoid the 'insufficient number of network buffers' error if you are increasing this config value.
taskmanager.network.sort-shuffle.min-parallelism	1	Parallelism threshold to switch between sort-based blocking shuffle and hash-based blocking shuffle, which means for batch jobs of smaller parallelism, hash-shuffle will be used and for batch jobs of larger or equal parallelism, sort-shuffle will be used. The value 1 means that sort-shuffle is the default option. Note: For production usage, you may also need to tune 'taskmanager.network.sort-shuffle.min-buffers' and 'taskmanager.memory.framework.off-heap.batch-shuffle.size' for better performance.
taskmanager.network.tcp-connection.enable-reuse-across-jobs	true	Whether to reuse tcp connections across multi jobs. If set to true, tcp connections will not be released after job finishes. The subsequent jobs will be free from the overhead of the connection re-establish. However, this may lead to an increase in the total number of connections on your machine. When it reaches the upper limit, you can set it to false to release idle connections. Note that to avoid connection leak, you must set taskmanager.network.max-num-tcp-connections to a smaller value before you enable tcp connection reuse.
shuffle-service-factory.class	"org.apache.flink.runtime.io.network.NettyShuffleServiceFactory"	The full class name of the shuffle service factory implementation to be used by the cluster. The default implementation uses Netty for network communication and local memory as well disk space to store results on a TaskExecutor.
restart-strategy	(none)	Defines the restart strategy to use in case of job failures.Accepted values are:none, off, disable: No restart strategy.fixeddelay, fixed-delay: Fixed delay restart strategy. More details can be found here.failurerate, failure-rate: Failure rate restart strategy. More details can be found here.exponentialdelay, exponential-delay: Exponential delay restart strategy. More details can be found here.If checkpointing is disabled, the default value is none. If checkpointing is enabled, the default value is fixed-delay with Integer.MAX_VALUE restart attempts and '1 s' delay.
execution.savepoint-restore-mode	NO_CLAIM	Describes the mode how Flink should restore from the given savepoint or retained checkpoint.Possible values:"CLAIM": Flink will take ownership of the given snapshot. It will clean the snapshot once it is subsumed by newer ones."NO_CLAIM": Flink will not claim ownership of the snapshot files. However it will make sure it does not depend on any artefacts from the restored snapshot. In order to do that, Flink will take the first checkpoint as a full one, which means it might reupload/duplicate files that are part of the restored checkpoint."LEGACY": This is the mode in which Flink worked so far. It will not claim ownership of the snapshot and will not delete the files. However, it can directly depend on the existence of the files of the restored checkpoint. It might not be safe to delete checkpoints that were restored in legacy mode 
execution.savepoint.ignore-unclaimed-state	false	Allow to skip savepoint state that cannot be restored. Allow this if you removed an operator from your pipeline after the savepoint was triggered.
execution.savepoint.path	(none)	Path to a savepoint to restore the job from (for example hdfs:///flink/savepoint-1537).
taskmanager.data.bind-port	(none)	The task manager's bind port used for data exchange operations. If not configured, 'taskmanager.data.port' will be used.
taskmanager.data.ssl.enabled	true	Enable SSL support for the taskmanager data transport. This is applicable only when the global flag for internal SSL (security.ssl.internal.enabled) is set to true
task.cancellation.interval	30000	Time interval between two successive task cancellation attempts in milliseconds.
task.cancellation.timeout	180000	Timeout in milliseconds after which a task cancellation times out and leads to a fatal TaskManager error. A value of 0 deactivates the watch dog. Notice that a task cancellation is different from both a task failure and a clean shutdown.  Task cancellation timeout only applies to task cancellation and does not apply to task closing/clean-up caused by a task failure or a clean shutdown.
task.cancellation.timers.timeout	7500	Time we wait for the timers in milliseconds to finish all pending timer threads when the stream task is cancelled.
taskmanager.debug.memory.log	false	Flag indicating whether to start a thread, which repeatedly logs the memory usage of the JVM.
taskmanager.debug.memory.log-interval	5000	The interval (in ms) for the log thread to log the current memory usage.
taskmanager.jvm-exit-on-oom	false	Whether to kill the TaskManager when the task thread throws an OutOfMemoryError.
taskmanager.memory.min-segment-size	256 bytes	Minimum possible size of memory buffers used by the network stack and the memory manager. ex. can be used for automatic buffer size adjustment.
taskmanager.memory.segment-size	32 kb	Size of memory buffers used by the network stack and the memory manager.
taskmanager.network.bind-policy	"ip"	The automatic address binding policy used by the TaskManager if "taskmanager.host" is not set. The value should be one of the following: "name" - uses hostname as binding address"ip" - uses host's ip address as binding address
taskmanager.numberOfTaskSlots	1	The number of parallel operator or user function instances that a single TaskManager can run. If this value is larger than 1, a single TaskManager takes multiple instances of a function or operator. That way, the TaskManager can utilize multiple CPU cores, but at the same time, the available memory is divided between the different operator or function instances. This value is typically proportional to the number of physical CPU cores that the TaskManager's machine has (e.g., equal to the number of cores, or half the number of cores).
taskmanager.registration.timeout	5 min	Defines the timeout for the TaskManager registration. If the duration is exceeded without a successful registration, then the TaskManager terminates.
taskmanager.resource-id	(none)	The TaskManager's ResourceID. If not configured, the ResourceID will be generated with the "RpcAddress:RpcPort" and a 6-character random string. Notice that this option is not valid in Yarn and Native Kubernetes mode.
taskmanager.slot.timeout	10 s	Timeout used for identifying inactive slots. The TaskManager will free the slot if it does not become active within the given amount of time. Inactive slots can be caused by an out-dated slot request. If no value is configured, then it will fall back to akka.ask.timeout.
state.backend.latency-track.history-size	128	Defines the number of measured latencies to maintain at each state access operation.
state.backend.latency-track.keyed-state-enabled	false	Whether to track latency of keyed state operations, e.g value state put/get/clear.
state.backend.latency-track.sample-interval	100	The sample interval of latency track once 'state.backend.latency-track.keyed-state-enabled' is enabled. The default value is 100, which means we would track the latency every 100 access requests.
state.backend.latency-track.state-name-as-variable	true	Whether to expose state name as a variable if tracking latency.
execution.batch-shuffle-mode	ALL_EXCHANGES_BLOCKING	Defines how data is exchanged between tasks in batch 'execution.runtime-mode' if the shuffling behavior has not been set explicitly for an individual exchange.With pipelined exchanges, upstream and downstream tasks run simultaneously. In order to achieve lower latency, a result record is immediately sent to and processed by the downstream task. Thus, the receiver back-pressures the sender. The streaming mode always uses this exchange.With blocking exchanges, upstream and downstream tasks run in stages. Records are persisted to some storage between stages. Downstream tasks then fetch these records after the upstream tasks finished. Such an exchange reduces the resources required to execute the job as it does not need to run upstream and downstream tasks simultaneously.With hybrid exchanges (experimental), downstream tasks can run anytime as long as upstream tasks start running. When given sufficient resources, it can reduce the overall job execution time by running tasks simultaneously. Otherwise, it also allows jobs to be executed with very little resources. It adapts to custom preferences between persisting less data and restarting less tasks on failures, by providing different spilling strategies.Possible values:"ALL_EXCHANGES_PIPELINED": Upstream and downstream tasks run simultaneously. This leads to lower latency and more evenly distributed (but higher) resource usage across tasks."ALL_EXCHANGES_BLOCKING": Upstream and downstream tasks run subsequently. This reduces the resource usage as downstream tasks are started after upstream tasks finished."ALL_EXCHANGES_HYBRID_FULL": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will spill all data to disk to support re-consume."ALL_EXCHANGES_HYBRID_SELECTIVE": Downstream can start running anytime, as long as the upstream has started. This adapts the resource usage to whatever is available. This type will selective spilling data to reduce disk writes as much as possible.
execution.buffer-timeout	100 ms	The maximum time frequency (milliseconds) for the flushing of the output buffers. By default the output buffers flush frequently to provide low latency and to aid smooth developer experience. Setting the parameter can result in three logical modes:A positive value triggers flushing periodically by that interval0 triggers flushing after every record thus minimizing latency-1 ms triggers flushing only when the output buffer is full thus maximizing throughput
execution.checkpointing.snapshot-compression	false	Tells if we should use compression for the state snapshot data or not
execution.runtime-mode	STREAMING	Runtime execution mode of DataStream programs. Among other things, this controls task scheduling, network shuffle behavior, and time semantics.Possible values:"STREAMING""BATCH""AUTOMATIC"
pulsar.consumer.ackReceiptEnabled	false	Acknowledgement will return a receipt but this does not mean that the message will not be resent after getting the receipt.
pulsar.consumer.ackTimeoutMillis	0	The timeout (in ms) for unacknowledged messages, truncated to the nearest millisecond. The timeout needs to be greater than 1 second.By default, the acknowledge timeout is disabled and that means that messages delivered to a consumer will not be re-delivered unless the consumer crashes.When acknowledgement timeout being enabled, if a message is not acknowledged within the specified timeout it will be re-delivered to the consumer (possibly to a different consumer in case of a shared subscription).
pulsar.consumer.acknowledgementsGroupTimeMicros	100000	Group a consumer acknowledgment for a specified time (in μs). By default, a consumer uses 100μs grouping time to send out acknowledgments to a broker. If the group time is set to 0, acknowledgments are sent out immediately. A longer ack group time is more efficient at the expense of a slight increase in message re-deliveries after a failure.
pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull	false	Buffering a large number of outstanding uncompleted chunked messages can bring memory pressure and it can be guarded by providing this pulsar.consumer.maxPendingChunkedMessage threshold. Once a consumer reaches this threshold, it drops the outstanding unchunked-messages by silently acknowledging if pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull is true. Otherwise, it marks them for redelivery.
pulsar.consumer.autoUpdatePartitionsIntervalSeconds	60	The interval (in seconds) of updating partitions. This only works if autoUpdatePartitions is enabled.
pulsar.consumer.consumerName	(none)	The consumer name is informative and it can be used to identify a particular consumer instance from the topic stats.
pulsar.consumer.cryptoFailureAction	FAIL	The consumer should take action when it receives a message that can not be decrypted.FAIL: this is the default option to fail messages until crypto succeeds.DISCARD: silently acknowledge but do not deliver messages to an application.CONSUME: deliver encrypted messages to applications. It is the application's responsibility to decrypt the message.Fail to decompress the messages.If messages contain batch messages, a client is not be able to retrieve individual messages in batch.The delivered encrypted message contains EncryptionContext which contains encryption and compression information in. You can use an application to decrypt the consumed message payload.Possible values:"FAIL""DISCARD""CONSUME"
pulsar.consumer.deadLetterPolicy.deadLetterTopic	(none)	Name of the dead topic where the failed messages are sent.
pulsar.consumer.deadLetterPolicy.maxRedeliverCount	0	The maximum number of times that a message are redelivered before being sent to the dead letter queue.
pulsar.consumer.deadLetterPolicy.retryLetterTopic	(none)	Name of the retry topic where the failed messages are sent.
pulsar.consumer.expireTimeOfIncompleteChunkedMessageMillis	60000	If a producer fails to publish all the chunks of a message, the consumer can expire incomplete chunks if the consumer cannot receive all chunks in expire times (default 1 hour, in ms).
pulsar.consumer.maxPendingChunkedMessage	10	The consumer buffers chunk messages into memory until it receives all the chunks of the original message. While consuming chunk-messages, chunks from the same message might not be contiguous in the stream and they might be mixed with other messages' chunks. So, consumer has to maintain multiple buffers to manage chunks coming from different messages. This mainly happens when multiple publishers are publishing messages on the topic concurrently or publishers failed to publish all chunks of the messages.For example, there are M1-C1, M2-C1, M1-C2, M2-C2 messages.Messages M1-C1 and M1-C2 belong to the M1 original message while M2-C1 and M2-C2 belong to the M2 message.Buffering a large number of outstanding uncompleted chunked messages can bring memory pressure and it can be guarded by providing this pulsar.consumer.maxPendingChunkedMessage threshold. Once, a consumer reaches this threshold, it drops the outstanding unchunked messages by silently acknowledging or asking the broker to redeliver messages later by marking it unacknowledged. This behavior can be controlled by the pulsar.consumer.autoAckOldestChunkedMessageOnQueueFull option.
pulsar.consumer.maxTotalReceiverQueueSizeAcrossPartitions	50000	The maximum total receiver queue size across partitions.This setting reduces the receiver queue size for individual partitions if the total receiver queue size exceeds this value.
pulsar.consumer.negativeAckRedeliveryDelayMicros	60000000	Delay (in μs) to wait before redelivering messages that failed to be processed.When an application uses Consumer.negativeAcknowledge(Message), failed messages are redelivered after a fixed timeout.
pulsar.consumer.poolMessages	false	Enable pooling of messages and the underlying data buffers.
pulsar.consumer.priorityLevel	0	Priority level for a consumer to which a broker gives more priorities while dispatching messages in the shared subscription type.The broker follows descending priorities. For example, 0=max-priority, 1, 2,...In shared subscription mode, the broker first dispatches messages to the consumers on the highest priority level if they have permits. Otherwise, the broker considers consumers on the next priority level.Example 1If a subscription has consumer A with priorityLevel 0 and consumer B with priorityLevel 1, then the broker only dispatches messages to consumer A until it runs out permits and then starts dispatching messages to consumer B.Example 2Consumer Priority, Level, Permits C1, 0, 2 C2, 0, 1 C3, 0, 1 C4, 1, 2 C5, 1, 1 The order in which a broker dispatches messages to consumers is: C1, C2, C3, C1, C4, C5, C4.
pulsar.consumer.properties		A name or value property of this consumer. properties is application defined metadata attached to a consumer. When getting a topic stats, associate this metadata with the consumer stats for easier identification.
pulsar.consumer.readCompacted	false	If enabling readCompacted, a consumer reads messages from a compacted topic rather than reading a full message backlog of a topic.A consumer only sees the latest value for each key in the compacted topic, up until reaching the point in the topic message when compacting backlog. Beyond that point, send messages as normal.Only enabling readCompacted on subscriptions to persistent topics, which have a single active consumer (like failure or exclusive subscriptions).Attempting to enable it on subscriptions to non-persistent topics or on shared subscriptions leads to a subscription call throwing a PulsarClientException.
pulsar.consumer.receiverQueueSize	1000	Size of a consumer's receiver queue.For example, the number of messages accumulated by a consumer before an application calls Receive.A value higher than the default value increases consumer throughput, though at the expense of more memory utilization.
pulsar.consumer.replicateSubscriptionState	false	If replicateSubscriptionState is enabled, a subscription state is replicated to geo-replicated clusters.
pulsar.consumer.retryEnable	false	If enabled, the consumer will automatically retry messages.
pulsar.consumer.subscriptionMode	Durable	Select the subscription mode to be used when subscribing to the topic.Durable: Make the subscription to be backed by a durable cursor that will retain messages and persist the current position.NonDurable: Lightweight subscription mode that doesn't have a durable cursor associatedPossible values:"Durable""NonDurable"
pulsar.consumer.subscriptionName	(none)	Specify the subscription name for this consumer. This argument is required when constructing the consumer.
pulsar.consumer.subscriptionType	Shared	Subscription type.Four subscription types are available:ExclusiveFailoverSharedKey_SharedPossible values:"Exclusive""Shared""Failover""Key_Shared"
pulsar.consumer.tickDurationMillis	1000	Granularity (in ms) of the ack-timeout redelivery.A greater (for example, 1 hour) tickDurationMillis reduces the memory overhead to track messages.
akka.ask.callstack	true	If true, call stack for asynchronous asks are captured. That way, when an ask fails (for example times out), you get a proper exception, describing to the original method call and call site. Note that in case of having millions of concurrent RPC calls, this may add to the memory footprint.
akka.ask.timeout	10 s	Timeout used for all futures and blocking Akka calls. If Flink fails due to timeouts then you should try to increase this value. Timeouts can be caused by slow machines or a congested network. The timeout value requires a time-unit specifier (ms/s/min/h/d).
akka.client-socket-worker-pool.pool-size-factor	1.0	The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.
akka.client-socket-worker-pool.pool-size-max	2	Max number of threads to cap factor-based number to.
akka.client-socket-worker-pool.pool-size-min	1	Min number of threads to cap factor-based number to.
akka.fork-join-executor.parallelism-factor	2.0	The parallelism factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the parallelism-min and parallelism-max values.
akka.fork-join-executor.parallelism-max	64	Max number of threads to cap factor-based parallelism number to.
akka.fork-join-executor.parallelism-min	8	Min number of threads to cap factor-based parallelism number to.
akka.framesize	"10485760b"	Maximum size of messages which are sent between the JobManager and the TaskManagers. If Flink fails because messages exceed this limit, then you should increase it. The message size requires a size-unit specifier.
akka.jvm-exit-on-fatal-error	true	Exit JVM on fatal Akka errors.
akka.log.lifecycle.events	false	Turns on the Akka’s remote logging of events. Set this value to 'true' in case of debugging.
akka.lookup.timeout	10 s	Timeout used for the lookup of the JobManager. The timeout value has to contain a time-unit specifier (ms/s/min/h/d).
akka.retry-gate-closed-for	50	Milliseconds a gate should be closed for after a remote connection was disconnected.
akka.server-socket-worker-pool.pool-size-factor	1.0	The pool size factor is used to determine thread pool size using the following formula: ceil(available processors * factor). Resulting size is then bounded by the pool-size-min and pool-size-max values.
akka.server-socket-worker-pool.pool-size-max	2	Max number of threads to cap factor-based number to.
akka.server-socket-worker-pool.pool-size-min	1	Min number of threads to cap factor-based number to.
akka.ssl.enabled	true	Turns on SSL for Akka’s remote communication. This is applicable only when the global ssl flag security.ssl.enabled is set to true.
akka.startup-timeout	(none)	Timeout after which the startup of a remote component is considered being failed.
akka.tcp.timeout	"20 s"	Timeout for all outbound connections. If you should experience problems with connecting to a TaskManager due to a slow network, you should increase this value.
akka.throughput	15	Number of messages that are processed in a batch before returning the thread to the pool. Low values denote a fair scheduling whereas high values can increase the performance at the cost of unfairness.
external-resource.<resource_name>.yarn.config-key	(none)	If configured, Flink will add this key to the resource profile of container request to Yarn. The value will be set to the value of external-resource.<resource_name>.amount.
flink.hadoop.<key>	(none)	A general option to probe Hadoop configuration through prefix 'flink.hadoop.'. Flink will remove the prefix to get <key> (from core-default.xml and hdfs-default.xml) then set the <key> and value to Hadoop configuration. For example, flink.hadoop.dfs.replication=5 in Flink configuration and convert to dfs.replication=5 in Hadoop configuration.
flink.yarn.<key>	(none)	A general option to probe Yarn configuration through prefix 'flink.yarn.'. Flink will remove the prefix 'flink.' to get yarn.<key> (from yarn-default.xml) then set the yarn.<key> and value to Yarn configuration. For example, flink.yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Flink configuration and convert to yarn.resourcemanager.container.liveness-monitor.interval-ms=300000 in Yarn configuration.
yarn.application-attempt-failures-validity-interval	10000	Time window in milliseconds which defines the number of application attempt failures when restarting the AM. Failures which fall outside of this window are not being considered. Set this value to -1 in order to count globally. See here for more information.
yarn.application-attempts	(none)	Number of ApplicationMaster restarts. By default, the value will be set to 1. If high availability is enabled, then the default value will be 2. The restart number is also limited by YARN (configured via yarn.resourcemanager.am.max-attempts). Note that that the entire Flink cluster will restart and the YARN Client will lose the connection.
yarn.application-master.port	"0"	With this configuration option, users can specify a port, a range of ports or a list of ports for the Application Master (and JobManager) RPC port. By default we recommend using the default value (0) to let the operating system choose an appropriate port. In particular when multiple AMs are running on the same physical host, fixed port assignments prevent the AM from starting. For example when running Flink on YARN on an environment with a restrictive firewall, this option allows specifying a range of allowed ports.
yarn.application.id	(none)	The YARN application id of the running yarn cluster. This is the YARN cluster where the pipeline is going to be executed.
yarn.application.name	(none)	A custom name for your YARN application.
yarn.application.node-label	(none)	Specify YARN node label for the YARN application.
yarn.application.priority	-1	A non-negative integer indicating the priority for submitting a Flink YARN application. It will only take effect if YARN priority scheduling setting is enabled. Larger integer corresponds with higher priority. If priority is negative or set to '-1'(default), Flink will unset yarn priority setting and use cluster default priority. Please refer to YARN's official documentation for specific settings required to enable priority scheduling for the targeted YARN version.
yarn.application.queue	(none)	The YARN queue on which to put the current pipeline.
yarn.application.type	(none)	A custom type for your YARN application..
yarn.appmaster.vcores	1	The number of virtual cores (vcores) used by YARN application master.
yarn.classpath.include-user-jar	ORDER	Defines whether user-jars are included in the system class path as well as their positioning in the path.Possible values:"DISABLED": Exclude user jars from the system class path"FIRST": Position at the beginning"LAST": Position at the end"ORDER": Position based on the name of the jar
yarn.containers.vcores	-1	The number of virtual cores (vcores) per YARN container. By default, the number of vcores is set to the number of slots per TaskManager, if set, or to 1, otherwise. In order for this parameter to be used your cluster must have CPU scheduling enabled. You can do this by setting the org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.
yarn.file-replication	-1	Number of file replication of each local resource file. If it is not configured, Flink will use the default replication value in hadoop configuration.
yarn.flink-dist-jar	(none)	The location of the Flink dist jar.
yarn.heartbeat.container-request-interval	500	Time between heartbeats with the ResourceManager in milliseconds if Flink requests containers:The lower this value is, the faster Flink will get notified about container allocations since requests and allocations are transmitted via heartbeats.The lower this value is, the more excessive containers might get allocated which will eventually be released but put pressure on Yarn.If you observe too many container allocations on the ResourceManager, then it is recommended to increase this value. See this link for more information.
yarn.heartbeat.interval	5	Time between heartbeats with the ResourceManager in seconds.
yarn.properties-file.location	(none)	When a Flink job is submitted to YARN, the JobManager’s host and the number of available processing slots is written into a properties file, so that the Flink client is able to pick those details up. This configuration parameter allows changing the default location of that file (for example for environments sharing a Flink installation between users).
yarn.provided.lib.dirs	(none)	A semicolon-separated list of provided lib directories. They should be pre-uploaded and world-readable. Flink will use them to exclude the local Flink jars(e.g. flink-dist, lib/, plugins/)uploading to accelerate the job submission process. Also YARN will cache them on the nodes so that they doesn't need to be downloaded every time for each application. An example could be hdfs://$namenode_address/path/of/flink/lib
yarn.provided.usrlib.dir	(none)	The provided usrlib directory in remote. It should be pre-uploaded and world-readable. Flink will use it to exclude the local usrlib directory(i.e. usrlib/ under the parent directory of FLINK_LIB_DIR). Unlike yarn.provided.lib.dirs, YARN will not cache it on the nodes as it is for each application. An example could be hdfs://$namenode_address/path/of/flink/usrlib
yarn.security.kerberos.localized-keytab-path	"krb5.keytab"	Local (on NodeManager) path where kerberos keytab file will be localized to. If yarn.security.kerberos.ship-local-keytab set to true, Flink willl ship the keytab file as a YARN local resource. In this case, the path is relative to the local resource directory. If set to false, Flink will try to directly locate the keytab from the path itself.
yarn.security.kerberos.ship-local-keytab	true	When this is true Flink will ship the keytab file configured via security.kerberos.login.keytab as a localized YARN resource.
yarn.ship-archives	(none)	A semicolon-separated list of archives to be shipped to the YARN cluster. These archives will be un-packed when localizing and they can be any of the following types: ".tar.gz", ".tar", ".tgz", ".dst", ".jar", ".zip".
yarn.ship-files	(none)	A semicolon-separated list of files and/or directories to be shipped to the YARN cluster.
yarn.staging-directory	(none)	Staging directory used to store YARN files while submitting applications. Per default, it uses the home directory of the configured file system.
yarn.tags	(none)	A comma-separated list of tags to apply to the Flink YARN application.
yarn.taskmanager.node-label	(none)	Specify YARN node label for the Flink TaskManagers, it will override the yarn.application.node-label for TaskManagers if both are set.
restart-strategy.exponential-delay.backoff-multiplier	2.0	Backoff value is multiplied by this value after every failure,until max backoff is reached if restart-strategy has been set to exponential-delay.
restart-strategy.exponential-delay.initial-backoff	1 s	Starting duration between restarts if restart-strategy has been set to exponential-delay. It can be specified using notation: "1 min", "20 s"
restart-strategy.exponential-delay.jitter-factor	0.1	Jitter specified as a portion of the backoff if restart-strategy has been set to exponential-delay. It represents how large random value will be added or subtracted to the backoff. Useful when you want to avoid restarting multiple jobs at the same time.
restart-strategy.exponential-delay.max-backoff	5 min	The highest possible duration between restarts if restart-strategy has been set to exponential-delay. It can be specified using notation: "1 min", "20 s"
restart-strategy.exponential-delay.reset-backoff-threshold	1 h	Threshold when the backoff is reset to its initial value if restart-strategy has been set to exponential-delay. It specifies how long the job must be running without failure to reset the exponentially increasing backoff to its initial value. It can be specified using notation: "1 min", "20 s"
sql-client.display.max-column-width	30	When printing the query results, this parameter determines the number of characters shown on screen before truncating.This only applies to columns with variable-length types (e.g. STRING) in streaming mode.Fixed-length types and all types in batch mode are printed using a deterministic column width
sql-client.execution.max-table-result.rows	1000000	The number of rows to cache when in the table mode. If the number of rows exceeds the specified value, it retries the row in the FIFO style.
sql-client.execution.result-mode	TABLE	Determines how the query result should be displayed.Possible values:"TABLE": Materializes results in memory and visualizes them in a regular, paginated table representation."CHANGELOG": Visualizes the result stream that is produced by a continuous query."TABLEAU": Display results in the screen directly in a tableau format.
sql-client.verbose	false	Determine whether to output the verbose output to the console. If set the option true, it will print the exception stack. Otherwise, it only output the cause.
rest.async.store-duration	5 min	Maximum duration that the result of an async operation is stored. Once elapsed the result of the operation can no longer be retrieved.
rest.await-leader-timeout	30000	The time in ms that the client waits for the leader address, e.g., Dispatcher or WebMonitorEndpoint
rest.client.max-content-length	104857600	The maximum content length in bytes that the client will handle.
rest.connection-timeout	15000	The maximum time in ms for the client to establish a TCP connection.
rest.flamegraph.cleanup-interval	10 min	Time after which cached stats are cleaned up if not accessed. It can be specified using notation: "100 s", "10 m".
rest.flamegraph.delay-between-samples	50 ms	Delay between individual stack trace samples taken for building a FlameGraph. It can be specified using notation: "100 ms", "1 s".
rest.flamegraph.enabled	false	Enables the experimental flame graph feature.
rest.flamegraph.num-samples	100	Number of samples to take to build a FlameGraph.
rest.flamegraph.refresh-interval	1 min	Time after which available stats are deprecated and need to be refreshed (by resampling).  It can be specified using notation: "30 s", "1 m".
rest.flamegraph.stack-depth	100	Maximum depth of stack traces used to create FlameGraphs.
rest.idleness-timeout	300000	The maximum time in ms for a connection to stay idle before failing.
rest.retry.delay	3000	The time in ms that the client waits between retries (See also `rest.retry.max-attempts`).
rest.retry.max-attempts	20	The number of retries the client will attempt if a retryable operations fails.
rest.server.max-content-length	104857600	The maximum content length in bytes that the server will handle.
rest.server.numThreads	4	The number of threads for the asynchronous processing of requests.
rest.server.thread-priority	5	Thread priority of the REST server's executor for processing asynchronous requests. Lowering the thread priority will give Flink's main components more CPU time whereas increasing will allocate more time for the REST server's processing.
external-resource.<resource_name>.amount	(none)	The amount for the external resource specified by <resource_name> per TaskExecutor.
external-resource.<resource_name>.driver-factory.class	(none)	Defines the factory class name for the external resource identified by <resource_name>. The factory will be used to instantiated the ExternalResourceDriver at the TaskExecutor side. For example, org.apache.flink.externalresource.gpu.GPUDriverFactory
external-resource.<resource_name>.param.<param>	(none)	The naming pattern of custom config options for the external resource specified by <resource_name>. Only the configurations that follow this pattern would be passed into the driver factory of that external resource.
external-resources		List of the <resource_name> of all external resources with delimiter ";", e.g. "gpu;fpga" for two external resource gpu and fpga. The <resource_name> will be used to splice related config options for external resource. Only the <resource_name> defined here will go into effect by external resource framework. Do not set the <resource_name> to 'none', which is preserved internally
state.storage.fs.memory-threshold	20 kb	The minimum size of state data files. All state chunks smaller than that are stored inline in the root checkpoint metadata file. The max memory threshold for this configuration is 1MB.
state.storage.fs.write-buffer-size	4096	The default size of the write buffer for the checkpoint streams that write to file systems. The actual write buffer size is determined to be the maximum of the value of this option and option 'state.storage.fs.memory-threshold'.
taskmanager.bind-host	(none)	The local address of the network interface that the task manager binds to. If not configured, '0.0.0.0' will be used.
taskmanager.rpc.bind-port	(none)	The local RPC port that the TaskManager binds to. If not configured, the external port (configured by 'taskmanager.rpc.port') will be used.
cleanup-strategy	"exponential-delay"	Defines the cleanup strategy to use in case of cleanup failures.Accepted values are:none, disable, off: Cleanup is only performed once. No retry will be initiated in case of failure. The job artifacts (and the job's JobResultStore entry) have to be cleaned up manually in case of a failure.fixed-delay, fixeddelay: Cleanup attempts will be separated by a fixed interval up to the point where the cleanup is considered successful or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.exponential-delay, exponentialdelay: Exponential delay restart strategy triggers the cleanup with an exponentially increasing delay up to the point where the cleanup succeeded or a set amount of retries is reached. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.The default configuration relies on an exponentially delayed retry strategy with the given default values.
compiler.delimited-informat.max-line-samples	10	The maximum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters.
compiler.delimited-informat.max-sample-len	2097152	The maximal length of a line sample that the compiler takes for delimited inputs. If the length of a single sample exceeds this value (possible because of misconfiguration of the parser), the sampling aborts. This value can be overridden for a specific input with the input format’s parameters.
compiler.delimited-informat.min-line-samples	2	The minimum number of line samples taken by the compiler for delimited inputs. The samples are used to estimate the number of records. This value can be overridden for a specific input with the input format’s parameters
dstl.dfs.base-path	(none)	Base path to store changelog files.
dstl.dfs.batch.persist-delay	10 ms	Delay before persisting changelog after receiving persist request (on checkpoint). Minimizes the number of files and requests if multiple operators (backends) or sub-tasks are using the same store. Correspondingly increases checkpoint time (async phase).
dstl.dfs.batch.persist-size-threshold	10 mb	Size threshold for state changes that were requested to be persisted but are waiting for dstl.dfs.batch.persist-delay (from all operators). . Once reached, accumulated changes are persisted immediately. This is different from dstl.dfs.preemptive-persist-threshold as it happens AFTER the checkpoint and potentially for state changes of multiple operators. Must not exceed in-flight data limit (see below)
dstl.dfs.compression.enabled	false	Whether to enable compression when serializing changelog.
dstl.dfs.discard.num-threads	1	Number of threads to use to discard changelog (e.g. pre-emptively uploaded unused state).
dstl.dfs.download.local-cache.idle-timeout-ms	10 min	Maximum idle time for cache files of distributed changelog file, after which the cache files will be deleted.
dstl.dfs.preemptive-persist-threshold	5 mb	Size threshold for state changes of a single operator beyond which they are persisted pre-emptively without waiting for a checkpoint.  Improves checkpointing time by allowing quasi-continuous uploading of state changes (as opposed to uploading all accumulated changes on checkpoint).
dstl.dfs.upload.buffer-size	1 mb	Buffer size used when uploading change sets
dstl.dfs.upload.max-attempts	3	Maximum number of attempts (including the initial one) to perform a particular upload. Only takes effect if dstl.dfs.upload.retry-policy is fixed.
dstl.dfs.upload.max-in-flight	100 mb	Max amount of data allowed to be in-flight. Upon reaching this limit the task will be back-pressured.  I.e., snapshotting will block; normal processing will block if dstl.dfs.preemptive-persist-threshold is set and reached. The limit is applied to the total size of in-flight changes if multiple operators/backends are using the same changelog storage. Must be greater than or equal to dstl.dfs.batch.persist-size-threshold
dstl.dfs.upload.next-attempt-delay	500 ms	Delay before the next attempt (if the failure was not caused by a timeout).
dstl.dfs.upload.num-threads	5	Number of threads to use for upload.
dstl.dfs.upload.retry-policy	"fixed"	Retry policy for the failed uploads (in particular, timed out). Valid values: none, fixed.
dstl.dfs.upload.timeout	1 s	Time threshold beyond which an upload is considered timed out. If a new attempt is made but this upload succeeds earlier then this upload result will be used. May improve upload times if tail latencies of upload requests are significantly high. Only takes effect if dstl.dfs.upload.retry-policy is fixed. Please note that timeout * max_attempts should be less than execution.checkpointing.timeout
restart-strategy.fixed-delay.attempts	1	The number of times that Flink retries the execution before the job is declared as failed if restart-strategy has been set to fixed-delay.
restart-strategy.fixed-delay.delay	1 s	Delay between two consecutive restart attempts if restart-strategy has been set to fixed-delay. Delaying the retries can be helpful when the program interacts with external systems where for example connections or pending transactions should reach a timeout before re-execution is attempted. It can be specified using notation: "1 min", "20 s"
jobmanager.bind-host	(none)	The local address of the network interface that the job manager binds to. If not configured, '0.0.0.0' will be used.
jobmanager.rpc.bind-port	(none)	The local RPC port that the JobManager binds to. If not configured, the external port (configured by 'jobmanager.rpc.port') will be used.
pulsar.admin.adminUrl	(none)	The Pulsar service HTTP URL for the admin endpoint. For example, http://my-broker.example.com:8080, or https://my-broker.example.com:8443 for TLS.
pulsar.admin.autoCertRefreshTime	300000	The auto cert refresh time (in ms) if Pulsar admin supports TLS authentication.
pulsar.admin.connectTimeout	60000	The connection time out (in ms) for the PulsarAdmin client.
pulsar.admin.readTimeout	60000	The server response read timeout (in ms) for the PulsarAdmin client for any request.
pulsar.admin.requestTimeout	300000	The server request timeout (in ms) for the PulsarAdmin client for any request.
pulsar.producer.batchingEnabled	true	Enable batch send ability, it was enabled by default.
pulsar.producer.batchingMaxBytes	131072	The maximum size of messages permitted in a batch. Keep the maximum consistent as previous versions.
pulsar.producer.batchingMaxMessages	1000	The maximum number of messages permitted in a batch.
pulsar.producer.batchingMaxPublishDelayMicros	1000	Batching time period of sending messages.
pulsar.producer.batchingPartitionSwitchFrequencyByPublishDelay	10	The maximum wait time for switching topic partitions.
pulsar.producer.chunkingEnabled	false	
pulsar.producer.compressionType	NONE	Message data compression type used by a producer.Available options:https://github.com/lz4/lz4https://zlib.net/https://facebook.github.io/zstd/https://google.github.io/snappy/Possible values:"NONE""LZ4""ZLIB""ZSTD""SNAPPY"
pulsar.producer.initialSequenceId	(none)	The sequence id for avoiding the duplication, it's used when Pulsar doesn't have transaction.
pulsar.producer.producerName	(none)	A producer name which would be displayed in the Pulsar's dashboard. If no producer name was provided, we would use a Pulsar generated name instead.
pulsar.producer.properties		A name or value property of this consumer. properties is application defined metadata attached to a consumer. When getting a topic stats, associate this metadata with the consumer stats for easier identification.
pulsar.producer.sendTimeoutMs	30000	Message send timeout in ms.If a message is not acknowledged by a server before the sendTimeout expires, an error occurs.
cleanup-strategy.exponential-delay.attempts	infinite	The number of times a failed cleanup is retried if cleanup-strategy has been set to exponential-delay. Reaching the configured limit means that the job artifacts (and the job's JobResultStore entry) might need to be cleaned up manually.
cleanup-strategy.exponential-delay.initial-backoff	1 s	Starting duration between cleanup retries if cleanup-strategy has been set to exponential-delay. It can be specified using the following notation: "1 min", "20 s"
cleanup-strategy.exponential-delay.max-backoff	1 h	The highest possible duration between cleanup retries if cleanup-strategy has been set to exponential-delay. It can be specified using the following notation: "1 min", "20 s"
client.retry-period	2 s	The interval (in ms) between consecutive retries of failed attempts to execute commands through the CLI or Flink's clients, wherever retry is supported (default 2sec).
client.timeout	1 min	Timeout on the client side.
execution.checkpointing.aligned-checkpoint-timeout	0 ms	Only relevant if execution.checkpointing.unaligned is enabled.If timeout is 0, checkpoints will always start unaligned.If timeout has a positive value, checkpoints will start aligned. If during checkpointing, checkpoint start delay exceeds this timeout, alignment will timeout and checkpoint barrier will start working as unaligned checkpoint.
execution.checkpointing.checkpoints-after-tasks-finish.enabled	true	Feature toggle for enabling checkpointing even if some of tasks have finished. Before you enable it, please take a look at the important considerations 
execution.checkpointing.externalized-checkpoint-retention	NO_EXTERNALIZED_CHECKPOINTS	Externalized checkpoints write their meta data out to persistent storage and are not automatically cleaned up when the owning job fails or is suspended (terminating with job status JobStatus#FAILED or JobStatus#SUSPENDED). In this case, you have to manually clean up the checkpoint state, both the meta data and actual program state.The mode defines how an externalized checkpoint should be cleaned up on job cancellation. If you choose to retain externalized checkpoints on cancellation you have to handle checkpoint clean up manually when you cancel the job as well (terminating with job status JobStatus#CANCELED).The target directory for externalized checkpoints is configured via state.checkpoints.dir.Possible values:"DELETE_ON_CANCELLATION": Checkpoint state is only kept when the owning job fails. It is deleted if the job is cancelled."RETAIN_ON_CANCELLATION": Checkpoint state is kept when the owning job is cancelled or fails."NO_EXTERNALIZED_CHECKPOINTS": Externalized checkpoints are disabled.
execution.checkpointing.interval	(none)	Gets the interval in which checkpoints are periodically scheduled.This setting defines the base interval. Checkpoint triggering may be delayed by the settings execution.checkpointing.max-concurrent-checkpoints and execution.checkpointing.min-pause
execution.checkpointing.max-concurrent-checkpoints	1	The maximum number of checkpoint attempts that may be in progress at the same time. If this value is n, then no checkpoints will be triggered while n checkpoint attempts are currently in flight. For the next checkpoint to be triggered, one checkpoint attempt would need to finish or expire.
execution.checkpointing.min-pause	0 ms	The minimal pause between checkpointing attempts. This setting defines how soon thecheckpoint coordinator may trigger another checkpoint after it becomes possible to triggeranother checkpoint with respect to the maximum number of concurrent checkpoints(see execution.checkpointing.max-concurrent-checkpoints).If the maximum number of concurrent checkpoints is set to one, this setting makes effectively sure that a minimum amount of time passes where no checkpoint is in progress at all.
execution.checkpointing.mode	EXACTLY_ONCE	The checkpointing mode (exactly-once vs. at-least-once).Possible values:"EXACTLY_ONCE""AT_LEAST_ONCE"
execution.checkpointing.recover-without-channel-state.checkpoint-id	-1	Checkpoint id for which in-flight data should be ignored in case of the recovery from this checkpoint.It is better to keep this value empty until there is explicit needs to restore from the specific checkpoint without in-flight data.
execution.checkpointing.timeout	10 min	The maximum time that a checkpoint may take before being discarded.
execution.checkpointing.tolerable-failed-checkpoints	(none)	The tolerable checkpoint consecutive failure number. If set to 0, that means we do not tolerance any checkpoint failure. This only applies to the following failure reasons: IOException on the Job Manager, failures in the async phase on the Task Managers and checkpoint expiration due to a timeout. Failures originating from the sync phase on the Task Managers are always forcing failover of an affected task. Other types of checkpoint failures (such as checkpoint being subsumed) are being ignored.
execution.checkpointing.unaligned	false	Enables unaligned checkpoints, which greatly reduce checkpointing times under backpressure.Unaligned checkpoints contain data stored in buffers as part of the checkpoint state, which allows checkpoint barriers to overtake these buffers. Thus, the checkpoint duration becomes independent of the current throughput as checkpoint barriers are effectively not embedded into the stream of data anymore.Unaligned checkpoints can only be enabled if execution.checkpointing.mode is EXACTLY_ONCE and if execution.checkpointing.max-concurrent-checkpoints is 1
execution.checkpointing.unaligned.forced	false	Forces unaligned checkpoints, particularly allowing them for iterative jobs.
address	(none)	The address that should be used by clients to connect to the sql gateway server.
bind-address	(none)	The address that the sql gateway server binds itself.
bind-port	"8083"	The port that the sql gateway server binds itself. Accepts a list of ports (“50100,50101”), ranges (“50100-50200”) or a combination of both. It is recommended to set a range of ports to avoid collisions when multiple sql gateway servers are running on the same machine.
port	8083	The port that the client connects to. If bind-port has not been specified, then the sql gateway server will bind to this port.
table.exec.async-lookup.buffer-capacity	100	The max number of async i/o operation that the async lookup join can trigger.
table.exec.async-lookup.output-mode	ORDERED	Output mode for asynchronous operations which will convert to {@see AsyncDataStream.OutputMode}, ORDERED by default. If set to ALLOW_UNORDERED, will attempt to use {@see AsyncDataStream.OutputMode.UNORDERED} when it does not affect the correctness of the result, otherwise ORDERED will be still used.Possible values:"ORDERED""ALLOW_UNORDERED"
table.exec.async-lookup.timeout	3 min	The async timeout for the asynchronous operation to complete.
table.exec.deduplicate.insert-update-after-sensitive-enabled	true	Set whether the job (especially the sinks) is sensitive to INSERT messages and UPDATE_AFTER messages. If false, Flink may, sometimes (e.g. deduplication for last row), send UPDATE_AFTER instead of INSERT for the first row. If true, Flink will guarantee to send INSERT for the first row, in that case there will be additional overhead. Default is true.
table.exec.deduplicate.mini-batch.compact-changes-enabled	false	Set whether to compact the changes sent downstream in row-time mini-batch. If true, Flink will compact changes and send only the latest change downstream. Note that if the downstream needs the details of versioned data, this optimization cannot be applied. If false, Flink will send all changes to downstream just like when the mini-batch is not enabled.
table.exec.disabled-operators	(none)	Mainly for testing. A comma-separated list of operator names, each name represents a kind of disabled operator. Operators that can be disabled include "NestedLoopJoin", "ShuffleHashJoin", "BroadcastHashJoin", "SortMergeJoin", "HashAgg", "SortAgg". By default no operator is disabled.
table.exec.legacy-cast-behaviour	DISABLED	Determines whether CAST will operate following the legacy behaviour or the new one that introduces various fixes and improvements.Possible values:"ENABLED": CAST will operate following the legacy behaviour."DISABLED": CAST will operate following the new correct behaviour.
table.exec.mini-batch.allow-latency	0 ms	The maximum latency can be used for MiniBatch to buffer input records. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: If table.exec.mini-batch.enabled is set true, its value must be greater than zero.
table.exec.mini-batch.enabled	false	Specifies whether to enable MiniBatch optimization. MiniBatch is an optimization to buffer input records to reduce state access. This is disabled by default. To enable this, users should set this config to true. NOTE: If mini-batch is enabled, 'table.exec.mini-batch.allow-latency' and 'table.exec.mini-batch.size' must be set.
table.exec.mini-batch.size	-1	The maximum number of input records can be buffered for MiniBatch. MiniBatch is an optimization to buffer input records to reduce state access. MiniBatch is triggered with the allowed latency interval and when the maximum number of buffered records reached. NOTE: MiniBatch only works for non-windowed aggregations currently. If table.exec.mini-batch.enabled is set true, its value must be positive.
table.exec.rank.topn-cache-size	10000	Rank operators have a cache which caches partial state contents to reduce state access. Cache size is the number of records in each ranking task.
table.exec.resource.default-parallelism	-1	Sets default parallelism for all operators (such as aggregate, join, filter) to run with parallel instances. This config has a higher priority than parallelism of StreamExecutionEnvironment (actually, this config overrides the parallelism of StreamExecutionEnvironment). A value of -1 indicates that no default parallelism is set, then it will fallback to use the parallelism of StreamExecutionEnvironment.
table.exec.simplify-operator-name-enabled	true	When it is true, the optimizer will simplify the operator name with id and type of ExecNode and keep detail in description. Default value is true.
table.exec.sink.keyed-shuffle	AUTO	In order to minimize the distributed disorder problem when writing data into table with primary keys that many users suffers. FLINK will auto add a keyed shuffle by default when the sink's parallelism differs from upstream operator and upstream is append only. This works only when the upstream ensures the multi-records' order on the primary key, if not, the added shuffle can not solve the problem (In this situation, a more proper way is to consider the deduplicate operation for the source firstly or use an upsert source with primary key definition which truly reflect the records evolution).By default, the keyed shuffle will be added when the sink's parallelism differs from upstream operator. You can set to no shuffle(NONE) or force shuffle(FORCE).Possible values:"NONE""AUTO""FORCE"
table.exec.sink.not-null-enforcer	ERROR	Determines how Flink enforces NOT NULL column constraints when inserting null values.Possible values:"ERROR": Throw a runtime exception when writing null values into NOT NULL column."DROP": Drop records silently if a null value would have to be inserted into a NOT NULL column.
table.exec.sink.type-length-enforcer	IGNORE	Determines whether values for columns with CHAR(<length>)/VARCHAR(<length>)/BINARY(<length>)/VARBINARY(<length>) types will be trimmed or padded (only for CHAR(<length>)/BINARY(<length>)), so that their length will match the one defined by the length of their respective CHAR/VARCHAR/BINARY/VARBINARY column type.Possible values:"IGNORE": Don't apply any trimming and padding, and instead ignore the CHAR/VARCHAR/BINARY/VARBINARY length directive."TRIM_PAD": Trim and pad string and binary values to match the length defined by the CHAR/VARCHAR/BINARY/VARBINARY length.
table.exec.sink.upsert-materialize	AUTO	Because of the disorder of ChangeLog data caused by Shuffle in distributed system, the data received by Sink may not be the order of global upsert. So add upsert materialize operator before upsert sink. It receives the upstream changelog records and generate an upsert view for the downstream.By default, the materialize operator will be added when a distributed disorder occurs on unique keys. You can also choose no materialization(NONE) or force materialization(FORCE).Possible values:"NONE""AUTO""FORCE"
table.exec.sort.async-merge-enabled	true	Whether to asynchronously merge sorted spill files.
table.exec.sort.default-limit	-1	Default limit when user don't set a limit after order by. -1 indicates that this configuration is ignored.
table.exec.sort.max-num-file-handles	128	The maximal fan-in for external merge sort. It limits the number of file handles per operator. If it is too small, may cause intermediate merging. But if it is too large, it will cause too many files opened at the same time, consume memory and lead to random reading.
table.exec.source.cdc-events-duplicate	false	Indicates whether the CDC (Change Data Capture) sources in the job will produce duplicate change events that requires the framework to deduplicate and get consistent result. CDC source refers to the source that produces full change events, including INSERT/UPDATE_BEFORE/UPDATE_AFTER/DELETE, for example Kafka source with Debezium format. The value of this configuration is false by default.However, it's a common case that there are duplicate change events. Because usually the CDC tools (e.g. Debezium) work in at-least-once delivery when failover happens. Thus, in the abnormal situations Debezium may deliver duplicate change events to Kafka and Flink will get the duplicate events. This may cause Flink query to get wrong results or unexpected exceptions.Therefore, it is recommended to turn on this configuration if your CDC tool is at-least-once delivery. Enabling this configuration requires to define PRIMARY KEY on the CDC sources. The primary key will be used to deduplicate change events and generate normalized changelog stream at the cost of an additional stateful operator.
table.exec.source.idle-timeout	0 ms	When a source do not receive any elements for the timeout time, it will be marked as temporarily idle. This allows downstream tasks to advance their watermarks without the need to wait for watermarks from this source while it is idle. Default value is 0, which means detecting source idleness is not enabled.
table.exec.spill-compression.block-size	64 kb	The memory size used to do compress when spilling data. The larger the memory, the higher the compression ratio, but more memory resource will be consumed by the job.
table.exec.spill-compression.enabled	true	Whether to compress spilled data. Currently we only support compress spilled data for sort and hash-agg and hash-join operators.
table.exec.state.ttl	0 ms	Specifies a minimum time interval for how long idle state (i.e. state which was not updated), will be retained. State will never be cleared until it was idle for less than the minimum time, and will be cleared at some time after it was idle. Default is never clean-up the state. NOTE: Cleaning up state requires additional overhead for bookkeeping. Default value is 0, which means that it will never clean up state.
table.exec.uid.format	"<id>_<transformation>"	Defines the format pattern for generating the UID of an ExecNode streaming transformation. The pattern can be defined globally or per-ExecNode in the compiled plan. Supported arguments are: <id> (from static counter), <type> (e.g. 'stream-exec-sink'), <version>, and <transformation> (e.g. 'constraint-validator' for a sink). In Flink 1.15.x the pattern was wrongly defined as '<id>_<type>_<version>_<transformation>' which would prevent migrations in the future.
table.exec.uid.generation	PLAN_ONLY	In order to remap state to operators during a restore, it is required that the pipeline's streaming transformations get a UID assigned.The planner can generate and assign explicit UIDs. If no UIDs have been set by the planner, the UIDs will be auto-generated by lower layers that can take the complete topology into account for uniqueness of the IDs. See the DataStream API for more information.This configuration option is for experts only and the default should be sufficient for most use cases. By default, only pipelines created from a persisted compiled plan will get UIDs assigned explicitly. Thus, these pipelines can be arbitrarily moved around within the same topology without affecting the stable UIDs.Possible values:"PLAN_ONLY": Sets UIDs on streaming transformations if and only if the pipeline definition comes from a compiled plan. Pipelines that have been constructed in the API without a compilation step will not set an explicit UID as it might not be stable across multiple translations."ALWAYS": Always sets UIDs on streaming transformations. This strategy is for experts only! Pipelines that have been constructed in the API without a compilation step might not be able to be restored properly. The UID generation depends on previously declared pipelines (potentially across jobs if the same JVM is used). Thus, a stable environment must be ensured. Pipeline definitions that come from a compiled plan are safe to use."DISABLED": No explicit UIDs will be set.
table.exec.window-agg.buffer-size-limit	100000	Sets the window elements buffer size limit used in group window agg operator.
pulsar.source.autoCommitCursorInterval	5000	This option is used only when the user disables the checkpoint and uses Exclusive or Failover subscription. We would automatically commit the cursor using the given period (in ms).
pulsar.source.enableAutoAcknowledgeMessage	false	Flink commits the consuming position with pulsar transactions on checkpoint. However, if you have disabled the Flink checkpoint or disabled transaction for your Pulsar cluster, ensure that you have set this option to true.The source would use pulsar client's internal mechanism and commit cursor in two ways.For Key_Shared and Shared subscription, the cursor would be committed once the message is consumed.For Exclusive and Failover subscription, the cursor would be committed in a given interval.
pulsar.source.maxFetchRecords	100	The maximum number of records to fetch to wait when polling. A longer time increases throughput but also latency. A fetch batch might be finished earlier because of pulsar.source.maxFetchTime.
pulsar.source.maxFetchTime	10000	The maximum time (in ms) to wait when fetching records. A longer time increases throughput but also latency. A fetch batch might be finished earlier because of pulsar.source.maxFetchRecords.
pulsar.source.partitionDiscoveryIntervalMs	30000	The interval (in ms) for the Pulsar source to discover the new partitions. A non-positive value disables the partition discovery.
pulsar.source.transactionTimeoutMillis	10800000	This option is used in Shared or Key_Shared subscription. You should configure this option when you do not enable the pulsar.source.enableAutoAcknowledgeMessage option.The value (in ms) should be greater than the checkpoint interval.
pulsar.source.verifyInitialOffsets	WARN_ON_MISMATCH	Upon (re)starting the source, check whether the expected message can be read. If failure is enabled, the application fails. Otherwise, it logs a warning. A possible solution is to adjust the retention settings in Pulsar or ignoring the check result.Possible values:"FAIL_ON_MISMATCH": Fail the consuming from Pulsar when we don't find the related cursor."WARN_ON_MISMATCH": Print a warn message and start consuming from the valid offset.
